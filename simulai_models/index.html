
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../simulai_residuals/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.7">
    
    
      
        <title>simulai.models - SimulAI Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4b4a2bd9.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#simulaimodels" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="SimulAI Docs" class="md-header__button md-logo" aria-label="SimulAI Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SimulAI Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              simulai.models
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="SimulAI Docs" class="md-nav__button md-logo" aria-label="SimulAI Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    SimulAI Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome to SimulAI
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    simulai.models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    simulai.models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#u-net" class="md-nav__link">
    U-Net
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet" class="md-nav__link">
    UNet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deeponet" class="md-nav__link">
    DeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet" class="md-nav__link">
    DeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flexibledeeponet" class="md-nav__link">
    FlexibleDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet" class="md-nav__link">
    FlexibleDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#improveddeeponet" class="md-nav__link">
    ImprovedDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet" class="md-nav__link">
    ImprovedDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencodermlp" class="md-nav__link">
    AutoencoderMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP" class="md-nav__link">
    AutoencoderMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection" class="md-nav__link">
    eval_projection()
  </a>
  
    <nav class="md-nav" aria-label="eval_projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection" class="md-nav__link">
    projection()
  </a>
  
    <nav class="md-nav" aria-label="projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction" class="md-nav__link">
    reconstruction()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencodercnn" class="md-nav__link">
    AutoencoderCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN" class="md-nav__link">
    AutoencoderCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection" class="md-nav__link">
    projection()
  </a>
  
    <nav class="md-nav" aria-label="projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction" class="md-nav__link">
    reconstruction()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary" class="md-nav__link">
    summary()
  </a>
  
    <nav class="md-nav" aria-label="summary()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoderkoopman" class="md-nav__link">
    AutoencoderKoopman
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman" class="md-nav__link">
    AutoencoderKoopman
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward" class="md-nav__link">
    latent_forward()
  </a>
  
    <nav class="md-nav" aria-label="latent_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m" class="md-nav__link">
    latent_forward_m()
  </a>
  
    <nav class="md-nav" aria-label="latent_forward_m()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward" class="md-nav__link">
    reconstruction_forward()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m" class="md-nav__link">
    reconstruction_forward_m()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward_m()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoencodervariational" class="md-nav__link">
    AutoencoderVariational
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational" class="md-nav__link">
    AutoencoderVariational
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance" class="md-nav__link">
    CoVariance()
  </a>
  
    <nav class="md-nav" aria-label="CoVariance()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu" class="md-nav__link">
    Mu()
  </a>
  
    <nav class="md-nav" aria-label="Mu()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma" class="md-nav__link">
    Sigma()
  </a>
  
    <nav class="md-nav" aria-label="Sigma()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy" class="md-nav__link">
    latent_gaussian_noisy()
  </a>
  
    <nav class="md-nav" aria-label="latent_gaussian_noisy()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval" class="md-nav__link">
    reconstruction_eval()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward" class="md-nav__link">
    reconstruction_forward()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary" class="md-nav__link">
    summary()
  </a>
  
    <nav class="md-nav" aria-label="summary()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../simulai_residuals/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    simulai.residuals
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.Transformer.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.Transformer.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#u-net" class="md-nav__link">
    U-Net
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet" class="md-nav__link">
    UNet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.UNet.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.UNet.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deeponet" class="md-nav__link">
    DeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet" class="md-nav__link">
    DeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.DeepONet.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flexibledeeponet" class="md-nav__link">
    FlexibleDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet" class="md-nav__link">
    FlexibleDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.FlexibleDeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#improveddeeponet" class="md-nav__link">
    ImprovedDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet" class="md-nav__link">
    ImprovedDeepONet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork" class="md-nav__link">
    eval_subnetwork()
  </a>
  
    <nav class="md-nav" aria-label="eval_subnetwork()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.ImprovedDeepONet.eval_subnetwork--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencodermlp" class="md-nav__link">
    AutoencoderMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP" class="md-nav__link">
    AutoencoderMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection" class="md-nav__link">
    eval_projection()
  </a>
  
    <nav class="md-nav" aria-label="eval_projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.eval_projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection" class="md-nav__link">
    projection()
  </a>
  
    <nav class="md-nav" aria-label="projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction" class="md-nav__link">
    reconstruction()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.reconstruction--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderMLP.summary" class="md-nav__link">
    summary()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencodercnn" class="md-nav__link">
    AutoencoderCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN" class="md-nav__link">
    AutoencoderCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection" class="md-nav__link">
    projection()
  </a>
  
    <nav class="md-nav" aria-label="projection()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.projection--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction" class="md-nav__link">
    reconstruction()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.reconstruction--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary" class="md-nav__link">
    summary()
  </a>
  
    <nav class="md-nav" aria-label="summary()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderCNN.summary--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoderkoopman" class="md-nav__link">
    AutoencoderKoopman
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman" class="md-nav__link">
    AutoencoderKoopman
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward" class="md-nav__link">
    latent_forward()
  </a>
  
    <nav class="md-nav" aria-label="latent_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m" class="md-nav__link">
    latent_forward_m()
  </a>
  
    <nav class="md-nav" aria-label="latent_forward_m()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.latent_forward_m--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward" class="md-nav__link">
    reconstruction_forward()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m" class="md-nav__link">
    reconstruction_forward_m()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward_m()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderKoopman.reconstruction_forward_m--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoencodervariational" class="md-nav__link">
    AutoencoderVariational
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational" class="md-nav__link">
    AutoencoderVariational
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance" class="md-nav__link">
    CoVariance()
  </a>
  
    <nav class="md-nav" aria-label="CoVariance()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.CoVariance--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu" class="md-nav__link">
    Mu()
  </a>
  
    <nav class="md-nav" aria-label="Mu()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Mu--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma" class="md-nav__link">
    Sigma()
  </a>
  
    <nav class="md-nav" aria-label="Sigma()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.Sigma--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.__init__" class="md-nav__link">
    __init__()
  </a>
  
    <nav class="md-nav" aria-label="__init__()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.__init__--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval" class="md-nav__link">
    eval()
  </a>
  
    <nav class="md-nav" aria-label="eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.eval--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy" class="md-nav__link">
    latent_gaussian_noisy()
  </a>
  
    <nav class="md-nav" aria-label="latent_gaussian_noisy()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.latent_gaussian_noisy--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project" class="md-nav__link">
    project()
  </a>
  
    <nav class="md-nav" aria-label="project()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.project--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct" class="md-nav__link">
    reconstruct()
  </a>
  
    <nav class="md-nav" aria-label="reconstruct()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruct--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval" class="md-nav__link">
    reconstruction_eval()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_eval()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_eval--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward" class="md-nav__link">
    reconstruction_forward()
  </a>
  
    <nav class="md-nav" aria-label="reconstruction_forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.reconstruction_forward--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary" class="md-nav__link">
    summary()
  </a>
  
    <nav class="md-nav" aria-label="summary()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simulai.models.AutoencoderVariational.summary--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="simulaimodels">simulai.models</h1>
<h2 id="transformer">Transformer</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.Transformer"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>


            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_transformer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads_encoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">num_heads_decoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">embed_dim_encoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span>
                       <span class="n">embed_dim_decoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span>
                       <span class="n">encoder_activation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">decoder_activation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">encoder_mlp_layer_config</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">decoder_mlp_layer_config</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">number_of_encoders</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">number_of_decoders</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A classical encoder-decoder transformer:</span>

<span class="sd">            U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e</span>

<span class="sd">            (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_heads_encoder : int</span>
<span class="sd">            The number of heads for the self-attention layer of the encoder. </span>
<span class="sd">        num_heads_decoder :int</span>
<span class="sd">            The number of heads for the self-attention layer of the decoder.</span>
<span class="sd">        embed_dim_encoder : int</span>
<span class="sd">            The dimension of the embedding for the encoder.</span>
<span class="sd">        embed_dim_decoder : int </span>
<span class="sd">            The dimension of the embedding for the decoder. </span>
<span class="sd">        encoder_activation : Union[str, torch.nn.Module]</span>
<span class="sd">            The activation to be used in all the encoder layers.</span>
<span class="sd">        decoder_activation : Union[str, torch.nn.Module]</span>
<span class="sd">            The activation to be used in all the decoder layers.</span>
<span class="sd">        encoder_mlp_layer_config : dict</span>
<span class="sd">            A configuration dictionary to instantiate the encoder MLP layer.weights</span>
<span class="sd">        decoder_mlp_layer_config : dict</span>
<span class="sd">            A configuration dictionary to instantiate the encoder MLP layer.weights</span>
<span class="sd">        number_of_encoders : int</span>
<span class="sd">            The number of encoders to be used.</span>
<span class="sd">        number_of_decoders : int</span>
<span class="sd">            The number of decoders to be used.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_encoder</span> <span class="o">=</span> <span class="n">num_heads_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_decoder</span> <span class="o">=</span> <span class="n">num_heads_decoder</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_encoder</span> <span class="o">=</span> <span class="n">embed_dim_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_decoder</span> <span class="o">=</span> <span class="n">embed_dim_decoder</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layer_dict</span> <span class="o">=</span> <span class="n">encoder_mlp_layer_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layer_dict</span> <span class="o">=</span> <span class="n">decoder_mlp_layer_config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span> <span class="o">=</span> <span class="n">number_of_encoders</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span> <span class="o">=</span> <span class="n">number_of_encoders</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="n">encoder_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_activation</span> <span class="o">=</span> <span class="n">decoder_activation</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="c1"># Creating independent copies for the MLP layers which will be used </span>
        <span class="c1"># by the multiple encoders/decoders.</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                                    <span class="n">DenseNetwork</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layer_dict</span><span class="p">)</span>
                                               <span class="p">)</span>

        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>

                                                    <span class="n">DenseNetwork</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layer_dict</span><span class="p">)</span>
                                               <span class="p">)</span>

        <span class="c1"># Defining the encoder architecture</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                                <span class="o">*</span><span class="p">[</span><span class="n">BasicEncoder</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads_encoder</span><span class="p">,</span>
                                              <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">,</span>
                                              <span class="n">mlp_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span><span class="p">[</span><span class="n">e</span><span class="p">],</span>
                                              <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_encoder</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span><span class="p">)]</span>
                            <span class="p">)</span>

        <span class="c1"># Defining the decoder architecture</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">BasicDecoder</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads_decoder</span><span class="p">,</span>
                                                               <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_activation</span><span class="p">,</span>
                                                               <span class="n">mlp_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span><span class="p">[</span><span class="n">d</span><span class="p">],</span>
                                                               <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_decoder</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span><span class="p">)</span>
                              <span class="p">])</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">e</span><span class="p">,</span> <span class="n">encoder_e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">encoder_e</span><span class="o">.</span><span class="n">weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;encoder_</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">encoder_e</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">decoder_d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">decoder_d</span><span class="o">.</span><span class="n">weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoder_</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">decoder_d</span><span class="p">)</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[torch.Tensor, np.ndarray] </span>
<span class="sd">            The input dataset.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The transformer output.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

        <span class="n">current_input</span> <span class="o">=</span> <span class="n">input_data</span>
        <span class="k">for</span> <span class="n">decoder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">current_input</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">=</span><span class="n">encoder_output</span><span class="p">)</span>
            <span class="n">current_input</span> <span class="o">=</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         It prints a general view of the architecture.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.Transformer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_heads_encoder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads_decoder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim_encoder</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">embed_dim_decoder</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">encoder_activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">decoder_activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">encoder_mlp_layer_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_mlp_layer_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">number_of_encoders</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_of_decoders</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>A classical encoder-decoder transformer:</p>
<pre><code>U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e

(u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V
</code></pre>
<h4 id="simulai.models.Transformer.__init__--parameters">Parameters</h4>
<p>num_heads_encoder : int
    The number of heads for the self-attention layer of the encoder. 
num_heads_decoder :int
    The number of heads for the self-attention layer of the decoder.
embed_dim_encoder : int
    The dimension of the embedding for the encoder.
embed_dim_decoder : int 
    The dimension of the embedding for the decoder. 
encoder_activation : Union[str, torch.nn.Module]
    The activation to be used in all the encoder layers.
decoder_activation : Union[str, torch.nn.Module]
    The activation to be used in all the decoder layers.
encoder_mlp_layer_config : dict
    A configuration dictionary to instantiate the encoder MLP layer.weights
decoder_mlp_layer_config : dict
    A configuration dictionary to instantiate the encoder MLP layer.weights
number_of_encoders : int
    The number of encoders to be used.
number_of_decoders : int
    The number of decoders to be used.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_transformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads_encoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">num_heads_decoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">embed_dim_encoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span>
                   <span class="n">embed_dim_decoder</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span>
                   <span class="n">encoder_activation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                   <span class="n">decoder_activation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                   <span class="n">encoder_mlp_layer_config</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">decoder_mlp_layer_config</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">number_of_encoders</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">number_of_decoders</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A classical encoder-decoder transformer:</span>

<span class="sd">        U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e</span>

<span class="sd">        (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num_heads_encoder : int</span>
<span class="sd">        The number of heads for the self-attention layer of the encoder. </span>
<span class="sd">    num_heads_decoder :int</span>
<span class="sd">        The number of heads for the self-attention layer of the decoder.</span>
<span class="sd">    embed_dim_encoder : int</span>
<span class="sd">        The dimension of the embedding for the encoder.</span>
<span class="sd">    embed_dim_decoder : int </span>
<span class="sd">        The dimension of the embedding for the decoder. </span>
<span class="sd">    encoder_activation : Union[str, torch.nn.Module]</span>
<span class="sd">        The activation to be used in all the encoder layers.</span>
<span class="sd">    decoder_activation : Union[str, torch.nn.Module]</span>
<span class="sd">        The activation to be used in all the decoder layers.</span>
<span class="sd">    encoder_mlp_layer_config : dict</span>
<span class="sd">        A configuration dictionary to instantiate the encoder MLP layer.weights</span>
<span class="sd">    decoder_mlp_layer_config : dict</span>
<span class="sd">        A configuration dictionary to instantiate the encoder MLP layer.weights</span>
<span class="sd">    number_of_encoders : int</span>
<span class="sd">        The number of encoders to be used.</span>
<span class="sd">    number_of_decoders : int</span>
<span class="sd">        The number of decoders to be used.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_encoder</span> <span class="o">=</span> <span class="n">num_heads_encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_decoder</span> <span class="o">=</span> <span class="n">num_heads_decoder</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_encoder</span> <span class="o">=</span> <span class="n">embed_dim_encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_decoder</span> <span class="o">=</span> <span class="n">embed_dim_decoder</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layer_dict</span> <span class="o">=</span> <span class="n">encoder_mlp_layer_config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layer_dict</span> <span class="o">=</span> <span class="n">decoder_mlp_layer_config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span> <span class="o">=</span> <span class="n">number_of_encoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span> <span class="o">=</span> <span class="n">number_of_encoders</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="n">encoder_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_activation</span> <span class="o">=</span> <span class="n">decoder_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># Creating independent copies for the MLP layers which will be used </span>
    <span class="c1"># by the multiple encoders/decoders.</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                                <span class="n">DenseNetwork</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layer_dict</span><span class="p">)</span>
                                           <span class="p">)</span>

    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>

                                                <span class="n">DenseNetwork</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layer_dict</span><span class="p">)</span>
                                           <span class="p">)</span>

    <span class="c1"># Defining the encoder architecture</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                            <span class="o">*</span><span class="p">[</span><span class="n">BasicEncoder</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads_encoder</span><span class="p">,</span>
                                          <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">,</span>
                                          <span class="n">mlp_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_mlp_layers_list</span><span class="p">[</span><span class="n">e</span><span class="p">],</span>
                                          <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_encoder</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_encoders</span><span class="p">)]</span>
                        <span class="p">)</span>

    <span class="c1"># Defining the decoder architecture</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">BasicDecoder</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads_decoder</span><span class="p">,</span>
                                                           <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_activation</span><span class="p">,</span>
                                                           <span class="n">mlp_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_mlp_layers_list</span><span class="p">[</span><span class="n">d</span><span class="p">],</span>
                                                           <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim_decoder</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_decoders</span><span class="p">)</span>
                          <span class="p">])</span>


    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">e</span><span class="p">,</span> <span class="n">encoder_e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">encoder_e</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;encoder_</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">encoder_e</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">decoder_d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">decoder_d</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoder_</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">decoder_d</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.Transformer.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <h4 id="simulai.models.Transformer.forward--parameters">Parameters</h4>
<p>input_data : Union[torch.Tensor, np.ndarray] 
    The input dataset.</p>
<h4 id="simulai.models.Transformer.forward--returns">Returns</h4>
<p>torch.Tensor
    The transformer output.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_transformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@as_tensor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[torch.Tensor, np.ndarray] </span>
<span class="sd">        The input dataset.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The transformer output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">EncoderStage</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

    <span class="n">current_input</span> <span class="o">=</span> <span class="n">input_data</span>
    <span class="k">for</span> <span class="n">decoder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">DecoderStage</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">current_input</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">=</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">current_input</span> <span class="o">=</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.Transformer.summary" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">summary</span><span class="p">()</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It prints a general view of the architecture.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_transformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     It prints a general view of the architecture.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="u-net">U-Net</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.UNet"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>


            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_unet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>


    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers_config</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">intermediary_outputs_indices</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">intermediary_inputs_indices</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">encoder_extra_args</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
                 <span class="n">decoder_extra_args</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        U-Net. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        layers_config : Dict</span>
<span class="sd">            A dictionary containing the complete configuration for the </span>
<span class="sd">            U-Net encoder and decoder.</span>
<span class="sd">        intermediary_outputs_indices : List[int]</span>
<span class="sd">            A list of indices for indicating the encoder outputs.</span>
<span class="sd">        intermediary_inputs_indices : List[int]</span>
<span class="sd">            A list of indices for indicating the decoder inputs.</span>
<span class="sd">        encoder_extra_args : Dict</span>
<span class="sd">            A dictionary containing extra arguments for the encoder.</span>
<span class="sd">        decoder_extra_args : Dict</span>
<span class="sd">            A dictionary containing extra arguments for the decoder. </span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">UNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span> <span class="o">=</span> <span class="n">layers_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediary_outputs_indices</span> <span class="o">=</span> <span class="n">intermediary_outputs_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediary_inputs_indices</span> <span class="o">=</span> <span class="n">intermediary_inputs_indices</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;encoder&quot;</span><span class="p">]</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;decoder&quot;</span><span class="p">]</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;encoder_activations&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;decoder_activations&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_horizontal_outputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1"># Configuring the encoder</span>
        <span class="n">encoder_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
        <span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;architecture&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_type</span> <span class="o">==</span> <span class="s2">&quot;cnn&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CNNUnetEncoder</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="p">[</span><span class="s2">&quot;architecture&quot;</span><span class="p">],</span>
                                          <span class="n">activations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_activations</span><span class="p">,</span>
                                          <span class="n">intermediary_outputs_indices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediary_outputs_indices</span><span class="p">,</span>
                                          <span class="n">case</span><span class="o">=</span><span class="s2">&quot;2d&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">encoder_extra_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Option </span><span class="si">{</span><span class="n">encoder_type</span><span class="si">}</span><span class="s2"> is not available.&quot;</span><span class="p">)</span>

         <span class="c1"># Configuring the decoder</span>
        <span class="n">decoder_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
        <span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;architecture&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_type</span> <span class="o">==</span> <span class="s2">&quot;cnn&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">CNNUnetDecoder</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span><span class="p">[</span><span class="s2">&quot;architecture&quot;</span><span class="p">],</span>
                                          <span class="n">activations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_activations</span><span class="p">,</span>
                                          <span class="n">intermediary_inputs_indices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediary_inputs_indices</span><span class="p">,</span>
                                          <span class="n">case</span><span class="o">=</span><span class="s2">&quot;2d&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">decoder_extra_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Option </span><span class="si">{</span><span class="n">encoder_type</span><span class="si">}</span><span class="s2"> is not available.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The U-Net forward method. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[torch.Tensor, np.ndarray],</span>
<span class="sd">            A dataset to be inputted in the CNN U-Net encoder.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The U-Net output. </span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">encoder_main_output</span><span class="p">,</span> <span class="n">encoder_intermediary_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">input_data</span> <span class="o">=</span> <span class="n">encoder_main_output</span><span class="p">,</span>
                              <span class="n">intermediary_encoder_outputs</span><span class="o">=</span><span class="n">encoder_intermediary_outputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It shows a general view of the architecture.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.UNet.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">layers_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intermediary_outputs_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intermediary_inputs_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_extra_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span> <span class="n">decoder_extra_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>U-Net. </p>
<h4 id="simulai.models.UNet.__init__--parameters">Parameters</h4>
<p>layers_config : Dict
    A dictionary containing the complete configuration for the 
    U-Net encoder and decoder.
intermediary_outputs_indices : List[int]
    A list of indices for indicating the encoder outputs.
intermediary_inputs_indices : List[int]
    A list of indices for indicating the decoder inputs.
encoder_extra_args : Dict
    A dictionary containing extra arguments for the encoder.
decoder_extra_args : Dict
    A dictionary containing extra arguments for the decoder.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_unet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers_config</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intermediary_outputs_indices</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intermediary_inputs_indices</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">encoder_extra_args</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
             <span class="n">decoder_extra_args</span><span class="p">:</span><span class="n">Dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    U-Net. </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    layers_config : Dict</span>
<span class="sd">        A dictionary containing the complete configuration for the </span>
<span class="sd">        U-Net encoder and decoder.</span>
<span class="sd">    intermediary_outputs_indices : List[int]</span>
<span class="sd">        A list of indices for indicating the encoder outputs.</span>
<span class="sd">    intermediary_inputs_indices : List[int]</span>
<span class="sd">        A list of indices for indicating the decoder inputs.</span>
<span class="sd">    encoder_extra_args : Dict</span>
<span class="sd">        A dictionary containing extra arguments for the encoder.</span>
<span class="sd">    decoder_extra_args : Dict</span>
<span class="sd">        A dictionary containing extra arguments for the decoder. </span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">UNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span> <span class="o">=</span> <span class="n">layers_config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">intermediary_outputs_indices</span> <span class="o">=</span> <span class="n">intermediary_outputs_indices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">intermediary_inputs_indices</span> <span class="o">=</span> <span class="n">intermediary_inputs_indices</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;encoder&quot;</span><span class="p">]</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;decoder&quot;</span><span class="p">]</span> 

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;encoder_activations&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config</span><span class="p">[</span><span class="s2">&quot;decoder_activations&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_horizontal_outputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="c1"># Configuring the encoder</span>
    <span class="n">encoder_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
    <span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;architecture&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">encoder_type</span> <span class="o">==</span> <span class="s2">&quot;cnn&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CNNUnetEncoder</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="p">[</span><span class="s2">&quot;architecture&quot;</span><span class="p">],</span>
                                      <span class="n">activations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_activations</span><span class="p">,</span>
                                      <span class="n">intermediary_outputs_indices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediary_outputs_indices</span><span class="p">,</span>
                                      <span class="n">case</span><span class="o">=</span><span class="s2">&quot;2d&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span>
                                      <span class="o">**</span><span class="n">encoder_extra_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Option </span><span class="si">{</span><span class="n">encoder_type</span><span class="si">}</span><span class="s2"> is not available.&quot;</span><span class="p">)</span>

     <span class="c1"># Configuring the decoder</span>
    <span class="n">decoder_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
    <span class="n">layers_config_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_config_encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;architecture&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">encoder_type</span> <span class="o">==</span> <span class="s2">&quot;cnn&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">CNNUnetDecoder</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers_config_decoder</span><span class="p">[</span><span class="s2">&quot;architecture&quot;</span><span class="p">],</span>
                                      <span class="n">activations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_activations</span><span class="p">,</span>
                                      <span class="n">intermediary_inputs_indices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediary_inputs_indices</span><span class="p">,</span>
                                      <span class="n">case</span><span class="o">=</span><span class="s2">&quot;2d&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span>
                                      <span class="o">**</span><span class="n">decoder_extra_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Option </span><span class="si">{</span><span class="n">encoder_type</span><span class="si">}</span><span class="s2"> is not available.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.UNet.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>The U-Net forward method. </p>
<h4 id="simulai.models.UNet.forward--parameters">Parameters</h4>
<p>input_data : Union[torch.Tensor, np.ndarray],
    A dataset to be inputted in the CNN U-Net encoder.</p>
<h4 id="simulai.models.UNet.forward--returns">Returns</h4>
<p>torch.Tensor
    The U-Net output.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_unet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@as_tensor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The U-Net forward method. </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[torch.Tensor, np.ndarray],</span>
<span class="sd">        A dataset to be inputted in the CNN U-Net encoder.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The U-Net output. </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">encoder_main_output</span><span class="p">,</span> <span class="n">encoder_intermediary_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">input_data</span> <span class="o">=</span> <span class="n">encoder_main_output</span><span class="p">,</span>
                          <span class="n">intermediary_encoder_outputs</span><span class="o">=</span><span class="n">encoder_intermediary_outputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.UNet.summary" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">summary</span><span class="p">()</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It shows a general view of the architecture.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_unet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It shows a general view of the architecture.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="deeponet">DeepONet</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.DeepONet"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>


            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DeepONet</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;deeponet&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trunk_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># The decoder network is optional and considered</span>
        <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># less effective than the output reshaping alternative</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Classical Deep Operator Network (DeepONet), a deep learning version</span>
<span class="sd">        of the Universal Approximation Theorem.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        trunk_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the coordinates inputs.</span>
<span class="sd">        branch_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">        decoder_network : NetworkTemplate</span>
<span class="sd">            Subnetworks for converting the embedding to the output (optional).</span>
<span class="sd">        var_dim: int</span>
<span class="sd">            Number of output variables.</span>
<span class="sd">        devices: Union[str, list]</span>
<span class="sd">            Devices in which the model will be executed.</span>
<span class="sd">        product_type: str</span>
<span class="sd">            Type of product to execute in the embedding space.</span>
<span class="sd">        rescale_factors: np.ndarray</span>
<span class="sd">            Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">        model_id: str</span>
<span class="sd">            Name for the model</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

        <span class="c1"># Determining the kind of device to be used for allocating the</span>
        <span class="c1"># subnetworks used in the DeepONet model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;trunk_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;branch_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="o">=</span> <span class="n">decoder_network</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">product_type</span> <span class="o">=</span> <span class="n">product_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_id</span> <span class="o">=</span> <span class="n">model_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span> <span class="o">=</span> <span class="n">var_dim</span>

        <span class="c1"># Rescaling factors for the output</span>
        <span class="k">if</span> <span class="n">rescale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">rescale_factors</span><span class="p">)</span> <span class="o">==</span> <span class="n">var_dim</span>
            <span class="p">),</span> <span class="s2">&quot;The number of rescaling factors must be equal to var_dim.&quot;</span>
            <span class="n">rescale_factors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">rescale_factors</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factors</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Checking up whether the output of each subnetwork are in correct shape</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_latent_dimension_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;The trunk network must have&quot;</span>
            <span class="s2">&quot; one-dimensional output , &quot;</span>
            <span class="s2">&quot;but received&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_latent_dimension_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;The branch network must have&quot;</span>
            <span class="s2">&quot; one-dimensional output,&quot;</span>
            <span class="s2">&quot; but received&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># If bias is being used, check whether the network outputs are compatible.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias is being used.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_bias_compatibility_is_correct</span><span class="p">(</span><span class="n">dim_trunk</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
                                                <span class="n">dim_branch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_bias_active</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_bias_inactive</span>

        <span class="c1"># Using a decoder on top of the model or not</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_decoder_active</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_decoder_inactive</span>

        <span class="c1"># Using rescaling factors or not</span>
        <span class="k">if</span> <span class="n">rescale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_rescale_active</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_rescale_inactive</span>

        <span class="c1"># Checking the compatibility of the subnetworks outputs for each kind of product being employed.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">product_type</span> <span class="o">!=</span> <span class="s2">&quot;dense&quot;</span><span class="p">:</span>
            <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span>
            <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span>

            <span class="c1"># It checks if the inner product operation can be performed.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">output_branch</span> <span class="o">==</span> <span class="n">output_trunk</span><span class="p">,</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The output dimensions for the sub-networks&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; trunk and branch must be equal but are&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">output_branch</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; and </span><span class="si">{</span><span class="n">output_trunk</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias compatibility was already verified.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span>

            <span class="k">assert</span> <span class="ow">not</span> <span class="n">output_branch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The number of branch latent outputs must&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; be divisible by the number of variables,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; but received </span><span class="si">{</span><span class="n">output_branch</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">net</span>
            <span class="k">for</span> <span class="n">net</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_trunk</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_branch</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1">#TODO Checking up if the input of the decoder network has the correct dimension</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoder is being used.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Selecting the correct forward approach to be used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_selector_</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;trunk&quot;</span><span class="p">,</span> <span class="s2">&quot;branch&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_latent_dimension_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        It checks if the latent dimension is consistent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        dim : Union[int, tuple]</span>
<span class="sd">            Latent_space_dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        bool</span>
<span class="sd">            The confirmation about the dimensionality correctness.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">tuple</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_bias_compatibility_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span>
                                       <span class="n">dim_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>

        <span class="k">assert</span> <span class="n">dim_branch</span> <span class="o">==</span> <span class="n">dim_trunk</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;When using bias, the dimension&quot;</span><span class="o">+</span>
                                                        <span class="s2">&quot;of the branch output should be&quot;</span> <span class="o">+</span>
                                                        <span class="s2">&quot;trunk output + var_dim.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_forward_dense</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">output_trunk</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">output_branch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Forward method used when the embeddings are multiplied using a matrix-like product, it means, the trunk</span>
<span class="sd">        network outputs serve as &quot;interpolation basis&quot; for the branch outputs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        output_trunk: torch.Tensor</span>
<span class="sd">            The embedding generated by the trunk network.</span>
<span class="sd">        output_branch: torch.Tensor</span>
<span class="sd">            The embedding generated by the branch network.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The product between the two embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">latent_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_branch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">)</span>
        <span class="n">output_branch_reshaped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">output_branch</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output_branch_reshaped</span><span class="p">,</span> <span class="n">output_trunk</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_forward_pointwise</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">output_trunk</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">output_branch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Forward method used when the embeddings are multiplied using a simple point-wise product, after that a</span>
<span class="sd">        reshaping is applied in order to produce multiple outputs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        output_trunk: torch.Tensor</span>
<span class="sd">            The embedding generated by the trunk network.</span>
<span class="sd">        output_branch: torch.Tensor</span>
<span class="sd">            The embedding generated by the branch network.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The product between the two embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">latent_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">)</span>
        <span class="n">output_trunk_reshaped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">output_trunk</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">output_branch_reshaped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">output_branch</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">output_trunk_reshaped</span> <span class="o">*</span> <span class="n">output_branch_reshaped</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_forward_vanilla</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">output_trunk</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">output_branch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Forward method used when the embeddings are multiplied using a simple point-wise product.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        output_trunk: torch.Tensor</span>
<span class="sd">            The embedding generated by the trunk network.</span>
<span class="sd">        output_branch: torch.Tensor</span>
<span class="sd">            The embedding generated by the branch network.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The product between the two embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">output_trunk</span> <span class="o">*</span> <span class="n">output_branch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_forward_selector_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        It selects the forward method to be used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        callable:</span>
<span class="sd">            The callable corresponding to the required forward method.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>

            <span class="c1"># It operates as a typical dense layer</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">product_type</span> <span class="o">==</span> <span class="s2">&quot;dense&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_dense</span>
            <span class="c1"># It executes an inner product by parts between the outputs</span>
            <span class="c1"># of the subnetworks branch and trunk</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pointwise</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_vanilla</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_var_map</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># It checks all the data arrays in self.var_map have the same</span>
        <span class="c1"># batches dimension</span>
        <span class="n">batches_dimensions</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_map</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">batches_dimensions</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;This dataset is not proper to apply shuffling&quot;</span>

        <span class="n">dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">batches_dimensions</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

        <span class="n">var_map_shuffled</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">return</span> <span class="n">var_map_shuffled</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">net</span><span class="o">.</span><span class="n">weights</span> <span class="k">for</span> <span class="n">net</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks</span><span class="p">],</span> <span class="p">[])</span>

    <span class="c1"># Now, a sequence of wrappers</span>
    <span class="k">def</span> <span class="nf">_wrapper_bias_inactive</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">output_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_wrapper_bias_active</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">output_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="n">output_branch_</span> <span class="o">=</span> <span class="n">output_branch</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">]</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">output_branch</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">:]</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch_</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_wrapper_decoder_active</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_wrapper_decoder_inactive</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="k">return</span> <span class="n">input_data</span>

    <span class="k">def</span> <span class="nf">_wrapper_rescale_active</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="k">return</span> <span class="n">input_data</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factors</span>

    <span class="k">def</span> <span class="nf">_wrapper_rescale_inactive</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="k">return</span> <span class="n">input_data</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Wrapper forward method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        input_trunk : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        input_branch : Union[np.ndarray, torch.Tensor]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        torch.Tensor</span>
<span class="sd">            The result of all the hidden operations in the network.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Forward method execution</span>
        <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_trunk</span><span class="p">),</span>
                                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_branch</span><span class="p">),</span>
                                     <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Wrappers are applied to execute user-defined operations.</span>
        <span class="c1"># When those operations are not selected, these wrappers simply</span>
        <span class="c1"># bypass the inputs. </span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">output</span><span class="p">))</span>

    <span class="nd">@guarantee_device</span>
    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        It uses the network to make evaluations.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        trunk_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        branch_data : Union[np.ndarray, torch.Tensor]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        np.ndarray</span>
<span class="sd">            The result of all the hidden operations in the network.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_trunk</span><span class="o">=</span><span class="n">trunk_data</span><span class="p">,</span> <span class="n">input_branch</span><span class="o">=</span><span class="n">branch_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="nd">@guarantee_device</span>
    <span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It evaluates the output of DeepONet subnetworks.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        name : str</span>
<span class="sd">            Name of the subnetwork.</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The data used as input for the subnetwork.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        np.ndarray</span>
<span class="sd">            The evaluation performed by the subnetwork.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="n">network_to_be_used</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">network_to_be_used</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trunk Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Branch Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.DeepONet.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">trunk_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">var_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">product_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rescale_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Classical Deep Operator Network (DeepONet), a deep learning version
of the Universal Approximation Theorem.</p>
<h4 id="simulai.models.DeepONet.__init__--parameters">Parameters</h4>

<details class="trunk_network-" open>
  <summary>NetworkTemplate</summary>
  <p>Subnetwork for processing the coordinates inputs.</p>
</details>      <p>branch_network : NetworkTemplate
    Subnetwork for processing the forcing/conditioning inputs.
decoder_network : NetworkTemplate
    Subnetworks for converting the embedding to the output (optional).
var_dim: int
    Number of output variables.
devices: Union[str, list]
    Devices in which the model will be executed.
product_type: str
    Type of product to execute in the embedding space.
rescale_factors: np.ndarray
    Values used for rescaling the network outputs for a given order of magnitude.
model_id: str
    Name for the model</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">trunk_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># The decoder network is optional and considered</span>
    <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># less effective than the output reshaping alternative</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Classical Deep Operator Network (DeepONet), a deep learning version</span>
<span class="sd">    of the Universal Approximation Theorem.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    trunk_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the coordinates inputs.</span>
<span class="sd">    branch_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">    decoder_network : NetworkTemplate</span>
<span class="sd">        Subnetworks for converting the embedding to the output (optional).</span>
<span class="sd">    var_dim: int</span>
<span class="sd">        Number of output variables.</span>
<span class="sd">    devices: Union[str, list]</span>
<span class="sd">        Devices in which the model will be executed.</span>
<span class="sd">    product_type: str</span>
<span class="sd">        Type of product to execute in the embedding space.</span>
<span class="sd">    rescale_factors: np.ndarray</span>
<span class="sd">        Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">    model_id: str</span>
<span class="sd">        Name for the model</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">DeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

    <span class="c1"># Determining the kind of device to be used for allocating the</span>
    <span class="c1"># subnetworks used in the DeepONet model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;trunk_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;branch_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder_network&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="o">=</span> <span class="n">decoder_network</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">product_type</span> <span class="o">=</span> <span class="n">product_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model_id</span> <span class="o">=</span> <span class="n">model_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span> <span class="o">=</span> <span class="n">var_dim</span>

    <span class="c1"># Rescaling factors for the output</span>
    <span class="k">if</span> <span class="n">rescale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">rescale_factors</span><span class="p">)</span> <span class="o">==</span> <span class="n">var_dim</span>
        <span class="p">),</span> <span class="s2">&quot;The number of rescaling factors must be equal to var_dim.&quot;</span>
        <span class="n">rescale_factors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">rescale_factors</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factors</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Checking up whether the output of each subnetwork are in correct shape</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_latent_dimension_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;The trunk network must have&quot;</span>
        <span class="s2">&quot; one-dimensional output , &quot;</span>
        <span class="s2">&quot;but received&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_latent_dimension_is_correct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;The branch network must have&quot;</span>
        <span class="s2">&quot; one-dimensional output,&quot;</span>
        <span class="s2">&quot; but received&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># If bias is being used, check whether the network outputs are compatible.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias is being used.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_compatibility_is_correct</span><span class="p">(</span><span class="n">dim_trunk</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
                                            <span class="n">dim_branch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_bias_active</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_bias_inactive</span>

    <span class="c1"># Using a decoder on top of the model or not</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_decoder_active</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_decoder_inactive</span>

    <span class="c1"># Using rescaling factors or not</span>
    <span class="k">if</span> <span class="n">rescale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_rescale_active</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapper_rescale_inactive</span>

    <span class="c1"># Checking the compatibility of the subnetworks outputs for each kind of product being employed.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">product_type</span> <span class="o">!=</span> <span class="s2">&quot;dense&quot;</span><span class="p">:</span>
        <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span>
        <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">output_size</span>

        <span class="c1"># It checks if the inner product operation can be performed.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">output_branch</span> <span class="o">==</span> <span class="n">output_trunk</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The output dimensions for the sub-networks&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; trunk and branch must be equal but are&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">output_branch</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and </span><span class="si">{</span><span class="n">output_trunk</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias compatibility was already verified.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">output_size</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="n">output_branch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The number of branch latent outputs must&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; be divisible by the number of variables,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; but received </span><span class="si">{</span><span class="n">output_branch</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">var_dim</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">net</span>
        <span class="k">for</span> <span class="n">net</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_trunk</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_branch</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="c1">#TODO Checking up if the input of the decoder network has the correct dimension</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoder is being used.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="c1"># Selecting the correct forward approach to be used</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_selector_</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;trunk&quot;</span><span class="p">,</span> <span class="s2">&quot;branch&quot;</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.DeepONet.eval" class="doc doc-heading">
          <code class="highlight language-python"><span class="nb">eval</span><span class="p">(</span><span class="n">trunk_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It uses the network to make evaluations.</p>
<h4 id="simulai.models.DeepONet.eval--parameters">Parameters</h4>
<p>trunk_data : Union[np.ndarray, torch.Tensor]
branch_data : Union[np.ndarray, torch.Tensor]</p>
<h4 id="simulai.models.DeepONet.eval--returns">Returns</h4>
<p>np.ndarray
    The result of all the hidden operations in the network.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@guarantee_device</span>
<span class="k">def</span> <span class="nf">eval</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    It uses the network to make evaluations.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    trunk_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">    branch_data : Union[np.ndarray, torch.Tensor]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    np.ndarray</span>
<span class="sd">        The result of all the hidden operations in the network.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">output_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_trunk</span><span class="o">=</span><span class="n">trunk_data</span><span class="p">,</span> <span class="n">input_branch</span><span class="o">=</span><span class="n">branch_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.DeepONet.eval_subnetwork" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">eval_subnetwork</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It evaluates the output of DeepONet subnetworks.</p>
<h4 id="simulai.models.DeepONet.eval_subnetwork--parameters">Parameters</h4>

<details class="name-" open>
  <summary>str</summary>
  <p>Name of the subnetwork.</p>
</details>      <p>input_data : Union[np.ndarray, torch.Tensor]
    The data used as input for the subnetwork.</p>
<h4 id="simulai.models.DeepONet.eval_subnetwork--returns">Returns</h4>
<p>np.ndarray
    The evaluation performed by the subnetwork.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@guarantee_device</span>
<span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It evaluates the output of DeepONet subnetworks.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    name : str</span>
<span class="sd">        Name of the subnetwork.</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The data used as input for the subnetwork.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    np.ndarray</span>
<span class="sd">        The evaluation performed by the subnetwork.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="n">network_to_be_used</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">network_to_be_used</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.DeepONet.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_trunk</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_branch</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Wrapper forward method.</p>
<h4 id="simulai.models.DeepONet.forward--parameters">Parameters</h4>
<p>input_trunk : Union[np.ndarray, torch.Tensor]
input_branch : Union[np.ndarray, torch.Tensor]</p>
<h4 id="simulai.models.DeepONet.forward--returns">Returns</h4>
<p>torch.Tensor
    The result of all the hidden operations in the network.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Wrapper forward method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    input_trunk : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">    input_branch : Union[np.ndarray, torch.Tensor]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    torch.Tensor</span>
<span class="sd">        The result of all the hidden operations in the network.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Forward method execution</span>
    <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_trunk</span><span class="p">),</span>
                                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_branch</span><span class="p">),</span>
                                 <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Wrappers are applied to execute user-defined operations.</span>
    <span class="c1"># When those operations are not selected, these wrappers simply</span>
    <span class="c1"># bypass the inputs. </span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_wrapper</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">output</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="flexibledeeponet">FlexibleDeepONet</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.FlexibleDeepONet"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.models._pytorch_models._deeponet.ResDeepONet">ResDeepONet</span></code></p>


            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlexibleDeepONet</span><span class="p">(</span><span class="n">ResDeepONet</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;flexibledeeponet&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trunk_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pre_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">multiply_by_trunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Flexible DeepONet uses a subnetwork called &#39;pre-network&#39;, which</span>
<span class="sd">                    plays the role of rescaling the trunk input according to the branch input.</span>
<span class="sd">        It is an attempt of reducing the training bias related to the different</span>
<span class="sd">                    orders of magnitude contained in the dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        trunk_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the coordinates inputs.</span>
<span class="sd">        branch_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">        decoder_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for converting the embedding to the output (optional).</span>
<span class="sd">        pre_network : NetworkTemplate</span>
<span class="sd">            Subnework used to predict rescaling parameters for the trunk input</span>
<span class="sd">            accordingly the branch input.</span>
<span class="sd">        var_dim: int</span>
<span class="sd">            Number of output variables.</span>
<span class="sd">        devices: Union[str, list]</span>
<span class="sd">            Devices in which the model will be executed.</span>
<span class="sd">        product_type: str</span>
<span class="sd">            Type of product to execute in the embedding space.</span>
<span class="sd">        rescale_factors: np.ndarray</span>
<span class="sd">            Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">        model_id: str</span>
<span class="sd">            Name for the model</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Guaranteeing the compatibility between the pre and the branch and trunk networks</span>
        <span class="n">t_is</span> <span class="o">=</span> <span class="n">trunk_network</span><span class="o">.</span><span class="n">input_size</span>
        <span class="n">p_is</span> <span class="o">=</span> <span class="n">pre_network</span><span class="o">.</span><span class="n">input_size</span>
        <span class="n">p_os</span> <span class="o">=</span> <span class="n">pre_network</span><span class="o">.</span><span class="n">output_size</span>
        <span class="n">b_is</span> <span class="o">=</span> <span class="n">branch_network</span><span class="o">.</span><span class="n">input_size</span>

        <span class="k">assert</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t_is</span> <span class="o">==</span> <span class="n">p_os</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">b_is</span> <span class="o">==</span> <span class="n">p_is</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;The input of branch and pre networks must have the same dimension&quot;</span>
            <span class="s2">&quot; and the output of pre and the input of trunks, too, but got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="p">(</span><span class="n">b_is</span><span class="p">,</span><span class="w"> </span><span class="n">p_is</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="p">(</span><span class="n">t_is</span><span class="p">,</span><span class="w"> </span><span class="n">p_os</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_is</span> <span class="o">=</span> <span class="n">t_is</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">FlexibleDeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">trunk_network</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span>
            <span class="n">branch_network</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span>
            <span class="n">decoder_network</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span>
            <span class="n">var_dim</span><span class="o">=</span><span class="n">var_dim</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
            <span class="n">product_type</span><span class="o">=</span><span class="n">product_type</span><span class="p">,</span>
            <span class="n">rescale_factors</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span>
            <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
            <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="n">multiply_by_trunk</span><span class="p">,</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">pre_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_flexible</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;pre&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_rescaling_operation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rescaling_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">angular</span> <span class="o">=</span> <span class="n">rescaling_tensor</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_is</span><span class="p">]</span>
        <span class="n">linear</span> <span class="o">=</span> <span class="n">rescaling_tensor</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_is</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">angular</span> <span class="o">*</span> <span class="n">input_data</span> <span class="o">+</span> <span class="n">linear</span>

    <span class="k">def</span> <span class="nf">_forward_flexible</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Flexible forward method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        output_trunk: torch.Tensor</span>
<span class="sd">            The embedding generated by the trunk network.</span>
<span class="sd">        output_branch: torch.Tensor</span>
<span class="sd">            The embedding generated by the branch network.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The product between the two embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Forward method execution</span>
        <span class="n">output_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_branch</span><span class="p">),</span>
                                     <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="n">rescaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_branch</span><span class="p">),</span>
                                 <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">input_trunk_rescaled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rescaling_operation</span><span class="p">(</span>
            <span class="n">input_data</span><span class="o">=</span><span class="n">input_trunk</span><span class="p">,</span> <span class="n">rescaling_tensor</span><span class="o">=</span><span class="n">rescaling</span>
        <span class="p">)</span>

        <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_trunk_rescaled</span><span class="p">),</span> 
                                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

    <span class="nd">@guarantee_device</span>
    <span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It evaluates the output of FlexibleDeepONet subnetworks.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        name : str</span>
<span class="sd">            Name of the subnetwork.</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The data used as input for the subnetwork.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        np.ndarray</span>
<span class="sd">            The evaluation performed by the subnetwork.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="c1"># Pre and branch network has the same input</span>
        <span class="n">pre_data</span> <span class="o">=</span> <span class="n">branch_data</span>

        <span class="n">network_instance</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">network_instance</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trunk Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pre Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Branch Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.FlexibleDeepONet.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">trunk_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pre_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">var_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">product_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rescale_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Flexible DeepONet uses a subnetwork called 'pre-network', which
            plays the role of rescaling the trunk input according to the branch input.
It is an attempt of reducing the training bias related to the different
            orders of magnitude contained in the dataset.</p>
<h4 id="simulai.models.FlexibleDeepONet.__init__--parameters">Parameters</h4>

<details class="trunk_network-" open>
  <summary>NetworkTemplate</summary>
  <p>Subnetwork for processing the coordinates inputs.</p>
</details>      <p>branch_network : NetworkTemplate
    Subnetwork for processing the forcing/conditioning inputs.
decoder_network : NetworkTemplate
    Subnetwork for converting the embedding to the output (optional).
pre_network : NetworkTemplate
    Subnework used to predict rescaling parameters for the trunk input
    accordingly the branch input.
var_dim: int
    Number of output variables.
devices: Union[str, list]
    Devices in which the model will be executed.
product_type: str
    Type of product to execute in the embedding space.
rescale_factors: np.ndarray
    Values used for rescaling the network outputs for a given order of magnitude.
model_id: str
    Name for the model</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">trunk_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pre_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">multiply_by_trunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Flexible DeepONet uses a subnetwork called &#39;pre-network&#39;, which</span>
<span class="sd">                plays the role of rescaling the trunk input according to the branch input.</span>
<span class="sd">    It is an attempt of reducing the training bias related to the different</span>
<span class="sd">                orders of magnitude contained in the dataset.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    trunk_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the coordinates inputs.</span>
<span class="sd">    branch_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">    decoder_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for converting the embedding to the output (optional).</span>
<span class="sd">    pre_network : NetworkTemplate</span>
<span class="sd">        Subnework used to predict rescaling parameters for the trunk input</span>
<span class="sd">        accordingly the branch input.</span>
<span class="sd">    var_dim: int</span>
<span class="sd">        Number of output variables.</span>
<span class="sd">    devices: Union[str, list]</span>
<span class="sd">        Devices in which the model will be executed.</span>
<span class="sd">    product_type: str</span>
<span class="sd">        Type of product to execute in the embedding space.</span>
<span class="sd">    rescale_factors: np.ndarray</span>
<span class="sd">        Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">    model_id: str</span>
<span class="sd">        Name for the model</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Guaranteeing the compatibility between the pre and the branch and trunk networks</span>
    <span class="n">t_is</span> <span class="o">=</span> <span class="n">trunk_network</span><span class="o">.</span><span class="n">input_size</span>
    <span class="n">p_is</span> <span class="o">=</span> <span class="n">pre_network</span><span class="o">.</span><span class="n">input_size</span>
    <span class="n">p_os</span> <span class="o">=</span> <span class="n">pre_network</span><span class="o">.</span><span class="n">output_size</span>
    <span class="n">b_is</span> <span class="o">=</span> <span class="n">branch_network</span><span class="o">.</span><span class="n">input_size</span>

    <span class="k">assert</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t_is</span> <span class="o">==</span> <span class="n">p_os</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">b_is</span> <span class="o">==</span> <span class="n">p_is</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;The input of branch and pre networks must have the same dimension&quot;</span>
        <span class="s2">&quot; and the output of pre and the input of trunks, too, but got&quot;</span>
        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="p">(</span><span class="n">b_is</span><span class="p">,</span><span class="w"> </span><span class="n">p_is</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="p">(</span><span class="n">t_is</span><span class="p">,</span><span class="w"> </span><span class="n">p_os</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">t_is</span> <span class="o">=</span> <span class="n">t_is</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">FlexibleDeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">trunk_network</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span>
        <span class="n">branch_network</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span>
        <span class="n">decoder_network</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span>
        <span class="n">var_dim</span><span class="o">=</span><span class="n">var_dim</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">product_type</span><span class="o">=</span><span class="n">product_type</span><span class="p">,</span>
        <span class="n">rescale_factors</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
        <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="n">multiply_by_trunk</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">pre_network</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">forward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_flexible</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_network</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;pre&quot;</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.FlexibleDeepONet.eval_subnetwork" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">eval_subnetwork</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trunk_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It evaluates the output of FlexibleDeepONet subnetworks.</p>
<h4 id="simulai.models.FlexibleDeepONet.eval_subnetwork--parameters">Parameters</h4>

<details class="name-" open>
  <summary>str</summary>
  <p>Name of the subnetwork.</p>
</details>      <p>input_data : Union[np.ndarray, torch.Tensor]
    The data used as input for the subnetwork.</p>
<h4 id="simulai.models.FlexibleDeepONet.eval_subnetwork--returns">Returns</h4>
<p>np.ndarray
    The evaluation performed by the subnetwork.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@guarantee_device</span>
<span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It evaluates the output of FlexibleDeepONet subnetworks.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    name : str</span>
<span class="sd">        Name of the subnetwork.</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The data used as input for the subnetwork.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    np.ndarray</span>
<span class="sd">        The evaluation performed by the subnetwork.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="c1"># Pre and branch network has the same input</span>
    <span class="n">pre_data</span> <span class="o">=</span> <span class="n">branch_data</span>

    <span class="n">network_instance</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">network_instance</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="improveddeeponet">ImprovedDeepONet</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.ImprovedDeepONet"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.models._pytorch_models._deeponet.ResDeepONet">ResDeepONet</span></code></p>


            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ImprovedDeepONet</span><span class="p">(</span><span class="n">ResDeepONet</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;improveddeeponet&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trunk_network</span><span class="p">:</span> <span class="n">ConvexDenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_network</span><span class="p">:</span> <span class="n">ConvexDenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_trunk</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_branch</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">multiply_by_trunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The so-called Improved DeepONet architecture aims at enhancing the communication</span>
<span class="sd">        between the trunk and branch pipelines during the training process, thus allowing</span>
<span class="sd">                    better generalization capabilities for the composite model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        trunk_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the coordinates inputs.</span>
<span class="sd">        branch_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">        decoder_network : NetworkTemplate</span>
<span class="sd">            Subnetwork for converting the embedding to the output (optional).</span>
<span class="sd">        encoder_trunk : NetworkTemplate</span>
<span class="sd">            Shallow subnework used to map the trunk input to an auxiliary embedding</span>
<span class="sd">            employed in combination with the hidden spaces.</span>
<span class="sd">        encoder_branch : NetworkTemplate</span>
<span class="sd">            Shallow subnework used to map the branch input to an auxiliary embedding</span>
<span class="sd">            employed in combination with the hidden spaces.</span>
<span class="sd">        var_dim: int</span>
<span class="sd">            Number of output variables.</span>
<span class="sd">        devices: Union[str, list]</span>
<span class="sd">            Devices in which the model will be executed.</span>
<span class="sd">        product_type: str</span>
<span class="sd">            Type of product to execute in the embedding space.</span>
<span class="sd">        rescale_factors: np.ndarray</span>
<span class="sd">            Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">        model_id: str</span>
<span class="sd">            Name for the model</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Guaranteeing the compatibility between the encoders and the branch and trunk networks</span>
        <span class="n">t_hs</span> <span class="o">=</span> <span class="n">trunk_network</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">et_os</span> <span class="o">=</span> <span class="n">encoder_trunk</span><span class="o">.</span><span class="n">output_size</span>
        <span class="n">b_hs</span> <span class="o">=</span> <span class="n">branch_network</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">eb_os</span> <span class="o">=</span> <span class="n">encoder_branch</span><span class="o">.</span><span class="n">output_size</span>

        <span class="k">assert</span> <span class="n">t_hs</span> <span class="o">==</span> <span class="n">et_os</span> <span class="o">==</span> <span class="n">b_hs</span> <span class="o">==</span> <span class="n">eb_os</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;The output of the trunk encoder must have the same dimension&quot;</span>
            <span class="s2">&quot; of the trunk network hidden size, but got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ImprovedDeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">trunk_network</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span>
            <span class="n">branch_network</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span>
            <span class="n">decoder_network</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span>
            <span class="n">var_dim</span><span class="o">=</span><span class="n">var_dim</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
            <span class="n">product_type</span><span class="o">=</span><span class="n">product_type</span><span class="p">,</span>
            <span class="n">rescale_factors</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span>
            <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
            <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="n">multiply_by_trunk</span><span class="p">,</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder_trunk</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder_branch</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder_trunk&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder_branch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">forward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_improved</span>

    <span class="k">def</span> <span class="nf">_forward_improved</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_trunk</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_branch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Improved forward method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        output_trunk: torch.Tensor</span>
<span class="sd">            The embedding generated by the trunk network.</span>
<span class="sd">        output_branch: torch.Tensor</span>
<span class="sd">            The embedding generated by the branch network.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The product between the two embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Forward method execution</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_trunk</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_branch</span><span class="p">)</span>

        <span class="n">output_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                                    <span class="n">input_data</span><span class="o">=</span><span class="n">input_trunk</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">),</span>
                                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="n">output_branch</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                                      <span class="n">input_data</span><span class="o">=</span><span class="n">input_branch</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">),</span>
                                      <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">output_trunk</span><span class="o">=</span><span class="n">output_trunk</span><span class="p">,</span> <span class="n">output_branch</span><span class="o">=</span><span class="n">output_branch</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_wrapper</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

    <span class="nd">@guarantee_device</span>
    <span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It evaluates the output of ImprovedDeepONet subnetworks.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        name : str</span>
<span class="sd">            Name of the subnetwork.</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The data used as input for the subnetwork.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        np.ndarray</span>
<span class="sd">            The evaluation performed by the subnetwork.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="n">network_instance</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">trunk_data</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">branch_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">network_instance</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trunk Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoder Trunk:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoder Branch:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Branch Network:&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branch_network</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.ImprovedDeepONet.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">trunk_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_trunk</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_branch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">var_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">product_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rescale_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>The so-called Improved DeepONet architecture aims at enhancing the communication
between the trunk and branch pipelines during the training process, thus allowing
            better generalization capabilities for the composite model.</p>
<h4 id="simulai.models.ImprovedDeepONet.__init__--parameters">Parameters</h4>

<details class="trunk_network-" open>
  <summary>NetworkTemplate</summary>
  <p>Subnetwork for processing the coordinates inputs.</p>
</details>      <p>branch_network : NetworkTemplate
    Subnetwork for processing the forcing/conditioning inputs.
decoder_network : NetworkTemplate
    Subnetwork for converting the embedding to the output (optional).
encoder_trunk : NetworkTemplate
    Shallow subnework used to map the trunk input to an auxiliary embedding
    employed in combination with the hidden spaces.
encoder_branch : NetworkTemplate
    Shallow subnework used to map the branch input to an auxiliary embedding
    employed in combination with the hidden spaces.
var_dim: int
    Number of output variables.
devices: Union[str, list]
    Devices in which the model will be executed.
product_type: str
    Type of product to execute in the embedding space.
rescale_factors: np.ndarray
    Values used for rescaling the network outputs for a given order of magnitude.
model_id: str
    Name for the model</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">trunk_network</span><span class="p">:</span> <span class="n">ConvexDenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_network</span><span class="p">:</span> <span class="n">ConvexDenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder_network</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_trunk</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_branch</span><span class="p">:</span> <span class="n">NetworkTemplate</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">var_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">product_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rescale_factors</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">multiply_by_trunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">model_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The so-called Improved DeepONet architecture aims at enhancing the communication</span>
<span class="sd">    between the trunk and branch pipelines during the training process, thus allowing</span>
<span class="sd">                better generalization capabilities for the composite model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    trunk_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the coordinates inputs.</span>
<span class="sd">    branch_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for processing the forcing/conditioning inputs.</span>
<span class="sd">    decoder_network : NetworkTemplate</span>
<span class="sd">        Subnetwork for converting the embedding to the output (optional).</span>
<span class="sd">    encoder_trunk : NetworkTemplate</span>
<span class="sd">        Shallow subnework used to map the trunk input to an auxiliary embedding</span>
<span class="sd">        employed in combination with the hidden spaces.</span>
<span class="sd">    encoder_branch : NetworkTemplate</span>
<span class="sd">        Shallow subnework used to map the branch input to an auxiliary embedding</span>
<span class="sd">        employed in combination with the hidden spaces.</span>
<span class="sd">    var_dim: int</span>
<span class="sd">        Number of output variables.</span>
<span class="sd">    devices: Union[str, list]</span>
<span class="sd">        Devices in which the model will be executed.</span>
<span class="sd">    product_type: str</span>
<span class="sd">        Type of product to execute in the embedding space.</span>
<span class="sd">    rescale_factors: np.ndarray</span>
<span class="sd">        Values used for rescaling the network outputs for a given order of magnitude.</span>
<span class="sd">    model_id: str</span>
<span class="sd">        Name for the model</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Guaranteeing the compatibility between the encoders and the branch and trunk networks</span>
    <span class="n">t_hs</span> <span class="o">=</span> <span class="n">trunk_network</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="n">et_os</span> <span class="o">=</span> <span class="n">encoder_trunk</span><span class="o">.</span><span class="n">output_size</span>
    <span class="n">b_hs</span> <span class="o">=</span> <span class="n">branch_network</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="n">eb_os</span> <span class="o">=</span> <span class="n">encoder_branch</span><span class="o">.</span><span class="n">output_size</span>

    <span class="k">assert</span> <span class="n">t_hs</span> <span class="o">==</span> <span class="n">et_os</span> <span class="o">==</span> <span class="n">b_hs</span> <span class="o">==</span> <span class="n">eb_os</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;The output of the trunk encoder must have the same dimension&quot;</span>
        <span class="s2">&quot; of the trunk network hidden size, but got&quot;</span>
        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">trunk_network</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">ImprovedDeepONet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">trunk_network</span><span class="o">=</span><span class="n">trunk_network</span><span class="p">,</span>
        <span class="n">branch_network</span><span class="o">=</span><span class="n">branch_network</span><span class="p">,</span>
        <span class="n">decoder_network</span><span class="o">=</span><span class="n">decoder_network</span><span class="p">,</span>
        <span class="n">var_dim</span><span class="o">=</span><span class="n">var_dim</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">product_type</span><span class="o">=</span><span class="n">product_type</span><span class="p">,</span>
        <span class="n">rescale_factors</span><span class="o">=</span><span class="n">rescale_factors</span><span class="p">,</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
        <span class="n">multiply_by_trunk</span><span class="o">=</span><span class="n">multiply_by_trunk</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder_trunk</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder_branch</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder_trunk&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder_branch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">forward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_improved</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.ImprovedDeepONet.eval_subnetwork" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">eval_subnetwork</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trunk_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">branch_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>It evaluates the output of ImprovedDeepONet subnetworks.</p>
<h4 id="simulai.models.ImprovedDeepONet.eval_subnetwork--parameters">Parameters</h4>

<details class="name-" open>
  <summary>str</summary>
  <p>Name of the subnetwork.</p>
</details>      <p>input_data : Union[np.ndarray, torch.Tensor]
    The data used as input for the subnetwork.</p>
<h4 id="simulai.models.ImprovedDeepONet.eval_subnetwork--returns">Returns</h4>
<p>np.ndarray
    The evaluation performed by the subnetwork.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@guarantee_device</span>
<span class="k">def</span> <span class="nf">eval_subnetwork</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">trunk_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">branch_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It evaluates the output of ImprovedDeepONet subnetworks.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    name : str</span>
<span class="sd">        Name of the subnetwork.</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The data used as input for the subnetwork.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    np.ndarray</span>
<span class="sd">        The evaluation performed by the subnetwork.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subnetworks_names</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The name </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is not a subnetwork of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="n">network_instance</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_network&quot;</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_trunk</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">trunk_data</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_branch</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">branch_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">network_instance</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="autoencodermlp">AutoencoderMLP</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.AutoencoderMLP"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>

  
      <p>This is an implementation of a Fully-connected AutoEncoder as
Reduced Order Model;</p>
<p>A MLP autoencoder architecture consists of two stages:
--&gt; Fully-connected encoder
--&gt; Fully connected decoder</p>

<details class="scheme" open>
  <summary>SCHEME</summary>
  <p>|         |
|  |   |  |</p>
</details>      <p>Z -&gt;    |  | | |  |  -&gt; Z_til
        |  |   |  |
        |         |</p>
<p>ENCODER       DECODER</p>

            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AutoencoderMLP</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of a Fully-connected AutoEncoder as</span>
<span class="sd">    Reduced Order Model;</span>

<span class="sd">    A MLP autoencoder architecture consists of two stages:</span>
<span class="sd">    --&gt; Fully-connected encoder</span>
<span class="sd">    --&gt; Fully connected decoder</span>

<span class="sd">    SCHEME:</span>
<span class="sd">            |         |</span>
<span class="sd">            |  |   |  |</span>
<span class="sd">    Z -&gt;    |  | | |  |  -&gt; Z_til</span>
<span class="sd">            |  |   |  |</span>
<span class="sd">            |         |</span>

<span class="sd">    ENCODER       DECODER</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">DenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder</span><span class="p">:</span> <span class="n">DenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the AutoencoderMLP network</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoder : DenseNetwork</span>
<span class="sd">            The encoder network architecture.</span>
<span class="sd">        decoder : DenseNetwork</span>
<span class="sd">            The decoder network architecture.</span>
<span class="sd">        input_dim : int, optional</span>
<span class="sd">            The input dimensions of the data, by default None.</span>
<span class="sd">        output_dim : int, optional</span>
<span class="sd">            The output dimensions of the data, by default None.</span>
<span class="sd">        latent_dim : int, optional</span>
<span class="sd">            The dimensions of the latent space, by default None.</span>
<span class="sd">        activation : Union[list, str], optional</span>
<span class="sd">            The activation functions used by the network, by default None.</span>
<span class="sd">        shallow : bool, optional</span>
<span class="sd">            Whether the network should be shallow or not, by default False.</span>
<span class="sd">        devices : Union[str, list], optional</span>
<span class="sd">            The device(s) to be used for allocating subnetworks, by default &quot;cpu&quot;.</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            The name of the network, by default None.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="c1"># This option is used when no network is provided</span>
        <span class="c1"># and it uses default choices for the architectures</span>
        <span class="k">if</span> <span class="n">encoder</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span> <span class="o">=</span> <span class="n">mlp_autoencoder_auto</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Determining the kind of device to be used for allocating the</span>
        <span class="c1"># subnetworks used in the DeepONet model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prints the summary of the network architecture</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">projection</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Project the input dataset into the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be projected, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The dataset projected over the latent space.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="k">def</span> <span class="nf">reconstruction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstruct the latent dataset to the original one.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be reconstructed, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The dataset reconstructed.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Execute the complete projection/reconstruction pipeline.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input dataset, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The dataset reconstructed.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">eval_projection</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the projection of the input dataset into the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be projected, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The dataset projected over the latent space.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shallow</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Initialize the AutoencoderMLP network</p>
<h4 id="simulai.models.AutoencoderMLP.__init__--parameters">Parameters</h4>
<p>encoder : DenseNetwork
    The encoder network architecture.
decoder : DenseNetwork
    The decoder network architecture.
input_dim : int, optional
    The input dimensions of the data, by default None.
output_dim : int, optional
    The output dimensions of the data, by default None.
latent_dim : int, optional
    The dimensions of the latent space, by default None.
activation : Union[list, str], optional
    The activation functions used by the network, by default None.
shallow : bool, optional
    Whether the network should be shallow or not, by default False.
devices : Union[str, list], optional
    The device(s) to be used for allocating subnetworks, by default "cpu".
name : str, optional
    The name of the network, by default None.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">DenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder</span><span class="p">:</span> <span class="n">DenseNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the AutoencoderMLP network</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : DenseNetwork</span>
<span class="sd">        The encoder network architecture.</span>
<span class="sd">    decoder : DenseNetwork</span>
<span class="sd">        The decoder network architecture.</span>
<span class="sd">    input_dim : int, optional</span>
<span class="sd">        The input dimensions of the data, by default None.</span>
<span class="sd">    output_dim : int, optional</span>
<span class="sd">        The output dimensions of the data, by default None.</span>
<span class="sd">    latent_dim : int, optional</span>
<span class="sd">        The dimensions of the latent space, by default None.</span>
<span class="sd">    activation : Union[list, str], optional</span>
<span class="sd">        The activation functions used by the network, by default None.</span>
<span class="sd">    shallow : bool, optional</span>
<span class="sd">        Whether the network should be shallow or not, by default False.</span>
<span class="sd">    devices : Union[str, list], optional</span>
<span class="sd">        The device(s) to be used for allocating subnetworks, by default &quot;cpu&quot;.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        The name of the network, by default None.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># This option is used when no network is provided</span>
    <span class="c1"># and it uses default choices for the architectures</span>
    <span class="k">if</span> <span class="n">encoder</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span> <span class="o">=</span> <span class="n">mlp_autoencoder_auto</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Determining the kind of device to be used for allocating the</span>
    <span class="c1"># subnetworks used in the DeepONet model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.eval_projection" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">eval_projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluate the projection of the input dataset into the latent space.</p>
<h4 id="simulai.models.AutoencoderMLP.eval_projection--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be projected, by default None.</p>
<h4 id="simulai.models.AutoencoderMLP.eval_projection--returns">Returns</h4>
<p>np.ndarray
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">eval_projection</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the projection of the input dataset into the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be projected, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The dataset projected over the latent space.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Execute the complete projection/reconstruction pipeline.</p>
<h4 id="simulai.models.AutoencoderMLP.forward--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input dataset, by default None.</p>
<h4 id="simulai.models.AutoencoderMLP.forward--returns">Returns</h4>
<p>torch.Tensor
    The dataset reconstructed.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Execute the complete projection/reconstruction pipeline.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input dataset, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The dataset reconstructed.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.projection" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Project the input dataset into the latent space.</p>
<h4 id="simulai.models.AutoencoderMLP.projection--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be projected, by default None.</p>
<h4 id="simulai.models.AutoencoderMLP.projection--returns">Returns</h4>
<p>torch.Tensor
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projection</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Project the input dataset into the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be projected, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The dataset projected over the latent space.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">latent</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.reconstruction" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstruct the latent dataset to the original one.</p>
<h4 id="simulai.models.AutoencoderMLP.reconstruction--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be reconstructed, by default None.</p>
<h4 id="simulai.models.AutoencoderMLP.reconstruction--returns">Returns</h4>
<p>torch.Tensor
    The dataset reconstructed.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstruct the latent dataset to the original one.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be reconstructed, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The dataset reconstructed.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderMLP.summary" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">summary</span><span class="p">()</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Prints the summary of the network architecture</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prints the summary of the network architecture</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="autoencodercnn">AutoencoderCNN</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.AutoencoderCNN"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>

  
      <p>This is an implementation of a convolutional autoencoder as Reduced Order Model.
An autoencoder architecture consists of three stages:</p>
<ul>
<li>The convolutional encoder</li>
</ul>
<p>The bottleneck stage, subdivided in:
    * Fully-connected encoder
    * Fully connected decoder
    * The convolutional decoder</p>
<p>SCHEME:</p>
<p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</p>
<pre><code>            ENCODER               DENSE BOTTLENECK           DECODER
</code></pre>

            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AutoencoderCNN</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of a convolutional autoencoder as Reduced Order Model.</span>
<span class="sd">    An autoencoder architecture consists of three stages:</span>

<span class="sd">    * The convolutional encoder</span>

<span class="sd">    The bottleneck stage, subdivided in:</span>
<span class="sd">        * Fully-connected encoder</span>
<span class="sd">        * Fully connected decoder</span>
<span class="sd">        * The convolutional decoder</span>

<span class="sd">    SCHEME:</span>

<span class="sd">    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</span>


<span class="sd">                    ENCODER               DENSE BOTTLENECK           DECODER</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">ConvolutionalNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Linear</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Linear</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder</span><span class="p">:</span> <span class="n">ConvolutionalNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the AutoencoderCNN network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoder : ConvolutionalNetwork, optional</span>
<span class="sd">            The encoder network architecture, by default None.</span>
<span class="sd">        bottleneck_encoder : Linear, optional</span>
<span class="sd">            The bottleneck encoder network architecture, by default None.</span>
<span class="sd">        bottleneck_decoder : Linear, optional</span>
<span class="sd">            The bottleneck decoder network architecture, by default None.</span>
<span class="sd">        decoder : ConvolutionalNetwork, optional</span>
<span class="sd">            The decoder network architecture, by default None.</span>
<span class="sd">        encoder_activation : str, optional</span>
<span class="sd">            The activation function used by the encoder network, by default &#39;relu&#39;.</span>
<span class="sd">        input_dim : Tuple[int, ...], optional</span>
<span class="sd">            The input dimensions of the data, by default None.</span>
<span class="sd">        output_dim : Tuple[int, ...], optional</span>
<span class="sd">            The output dimensions of the data, by default None.</span>
<span class="sd">        latent_dim : int, optional</span>
<span class="sd">            The dimensions of the latent space, by default None.</span>
<span class="sd">        activation : Union[list, str], optional</span>
<span class="sd">            The activation functions used by the network, by default None.</span>
<span class="sd">        channels : int, optional</span>
<span class="sd">            The number of channels of the convolutional layers, by default None.</span>
<span class="sd">        case : str, optional</span>
<span class="sd">            The type of convolutional encoder and decoder to be used, by default None.</span>
<span class="sd">        shallow : bool, optional</span>
<span class="sd">            Whether the network should be shallow or not, by default False.</span>
<span class="sd">        devices : Union[str, list], optional</span>
<span class="sd">            The device(s) to be used for allocating subnetworks, by default &#39;cpu&#39;.</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            The name of the network, by default None.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="c1"># Determining the kind of device to be used for allocating the</span>
        <span class="c1"># subnetworks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># If not network is provided, the automatic generation</span>
        <span class="c1"># pipeline is activated.</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

            <span class="p">(</span>
                <span class="n">encoder</span><span class="p">,</span>
                <span class="n">decoder</span><span class="p">,</span>
                <span class="n">bottleneck_encoder</span><span class="p">,</span>
                <span class="n">bottleneck_decoder</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">cnn_autoencoder_auto</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
                <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
                <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prints the summary of the network architecture.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : np.ndarray or torch.Tensor</span>
<span class="sd">            The input dataset.</span>
<span class="sd">        input_shape : list, optional</span>
<span class="sd">            The shape of the input data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The dataset projected over the latent space.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
                <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">input_shape</span>
                <span class="p">),</span> <span class="s2">&quot;It is necessary to have input_shape when input_data is None.&quot;</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span>
                <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

                <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

            <span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

            <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">before_flatten_dimension</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Saving the content of the subnetworks to the overall architecture dictionary</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Project input dataset into the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The dataset to be projected.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The dataset projected over the latent space.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

        <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span><span class="p">)))</span>

        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">reconstruction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstruct the latent dataset to the original one.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The dataset to be reconstructed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed dataset.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span>
        <span class="p">)</span>

        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Execute the complete projection/reconstruction pipeline.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The input dataset.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the autoencoder on the given dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be evaluated, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The dataset projected over the latent space.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Project the input dataset into the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be projected, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The dataset projected over the latent space.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">projected_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">projected_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the latent dataset to the original one.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The dataset to be reconstructed. If not provided, uses the original input data, by default None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The reconstructed dataset.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">case</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shallow</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Initialize the AutoencoderCNN network.</p>
<h4 id="simulai.models.AutoencoderCNN.__init__--parameters">Parameters</h4>
<p>encoder : ConvolutionalNetwork, optional
    The encoder network architecture, by default None.
bottleneck_encoder : Linear, optional
    The bottleneck encoder network architecture, by default None.
bottleneck_decoder : Linear, optional
    The bottleneck decoder network architecture, by default None.
decoder : ConvolutionalNetwork, optional
    The decoder network architecture, by default None.
encoder_activation : str, optional
    The activation function used by the encoder network, by default 'relu'.
input_dim : Tuple[int, ...], optional
    The input dimensions of the data, by default None.
output_dim : Tuple[int, ...], optional
    The output dimensions of the data, by default None.
latent_dim : int, optional
    The dimensions of the latent space, by default None.
activation : Union[list, str], optional
    The activation functions used by the network, by default None.
channels : int, optional
    The number of channels of the convolutional layers, by default None.
case : str, optional
    The type of convolutional encoder and decoder to be used, by default None.
shallow : bool, optional
    Whether the network should be shallow or not, by default False.
devices : Union[str, list], optional
    The device(s) to be used for allocating subnetworks, by default 'cpu'.
name : str, optional
    The name of the network, by default None.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">ConvolutionalNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Linear</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Linear</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder</span><span class="p">:</span> <span class="n">ConvolutionalNetwork</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the AutoencoderCNN network.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : ConvolutionalNetwork, optional</span>
<span class="sd">        The encoder network architecture, by default None.</span>
<span class="sd">    bottleneck_encoder : Linear, optional</span>
<span class="sd">        The bottleneck encoder network architecture, by default None.</span>
<span class="sd">    bottleneck_decoder : Linear, optional</span>
<span class="sd">        The bottleneck decoder network architecture, by default None.</span>
<span class="sd">    decoder : ConvolutionalNetwork, optional</span>
<span class="sd">        The decoder network architecture, by default None.</span>
<span class="sd">    encoder_activation : str, optional</span>
<span class="sd">        The activation function used by the encoder network, by default &#39;relu&#39;.</span>
<span class="sd">    input_dim : Tuple[int, ...], optional</span>
<span class="sd">        The input dimensions of the data, by default None.</span>
<span class="sd">    output_dim : Tuple[int, ...], optional</span>
<span class="sd">        The output dimensions of the data, by default None.</span>
<span class="sd">    latent_dim : int, optional</span>
<span class="sd">        The dimensions of the latent space, by default None.</span>
<span class="sd">    activation : Union[list, str], optional</span>
<span class="sd">        The activation functions used by the network, by default None.</span>
<span class="sd">    channels : int, optional</span>
<span class="sd">        The number of channels of the convolutional layers, by default None.</span>
<span class="sd">    case : str, optional</span>
<span class="sd">        The type of convolutional encoder and decoder to be used, by default None.</span>
<span class="sd">    shallow : bool, optional</span>
<span class="sd">        Whether the network should be shallow or not, by default False.</span>
<span class="sd">    devices : Union[str, list], optional</span>
<span class="sd">        The device(s) to be used for allocating subnetworks, by default &#39;cpu&#39;.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        The name of the network, by default None.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># Determining the kind of device to be used for allocating the</span>
    <span class="c1"># subnetworks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># If not network is provided, the automatic generation</span>
    <span class="c1"># pipeline is activated.</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
        <span class="p">]</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="p">(</span>
            <span class="n">encoder</span><span class="p">,</span>
            <span class="n">decoder</span><span class="p">,</span>
            <span class="n">bottleneck_encoder</span><span class="p">,</span>
            <span class="n">bottleneck_decoder</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">cnn_autoencoder_auto</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
            <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.eval" class="doc doc-heading">
          <code class="highlight language-python"><span class="nb">eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluate the autoencoder on the given dataset.</p>
<h4 id="simulai.models.AutoencoderCNN.eval--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be evaluated, by default None.</p>
<h4 id="simulai.models.AutoencoderCNN.eval--returns">Returns</h4>
<p>np.ndarray
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the autoencoder on the given dataset.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be evaluated, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The dataset projected over the latent space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Execute the complete projection/reconstruction pipeline.</p>
<h4 id="simulai.models.AutoencoderCNN.forward--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor]
    The input dataset.</p>
<h4 id="simulai.models.AutoencoderCNN.forward--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed dataset.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Execute the complete projection/reconstruction pipeline.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The input dataset.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.project" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">project</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Project the input dataset into the latent space.</p>
<h4 id="simulai.models.AutoencoderCNN.project--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be projected, by default None.</p>
<h4 id="simulai.models.AutoencoderCNN.project--returns">Returns</h4>
<p>np.ndarray
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Project the input dataset into the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be projected, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The dataset projected over the latent space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">projected_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">projected_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.projection" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Project input dataset into the latent space.</p>
<h4 id="simulai.models.AutoencoderCNN.projection--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor]
    The dataset to be projected.</p>
<h4 id="simulai.models.AutoencoderCNN.projection--returns">Returns</h4>
<p>torch.Tensor
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@as_tensor</span>
<span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Project input dataset into the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The dataset to be projected.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The dataset projected over the latent space.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

    <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span><span class="p">)))</span>

    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">latent</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.reconstruct" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruct</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstructs the latent dataset to the original one.</p>
<h4 id="simulai.models.AutoencoderCNN.reconstruct--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The dataset to be reconstructed. If not provided, uses the original input data, by default None.</p>
<h4 id="simulai.models.AutoencoderCNN.reconstruct--returns">Returns</h4>
<p>np.ndarray
    The reconstructed dataset.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstructs the latent dataset to the original one.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The dataset to be reconstructed. If not provided, uses the original input data, by default None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The reconstructed dataset.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.reconstruction" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstruct the latent dataset to the original one.</p>
<h4 id="simulai.models.AutoencoderCNN.reconstruction--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor]
    The dataset to be reconstructed.</p>
<h4 id="simulai.models.AutoencoderCNN.reconstruction--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed dataset.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@as_tensor</span>
<span class="k">def</span> <span class="nf">reconstruction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstruct the latent dataset to the original one.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The dataset to be reconstructed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed dataset.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span>
    <span class="p">)</span>

    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderCNN.summary" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Prints the summary of the network architecture.</p>
<h4 id="simulai.models.AutoencoderCNN.summary--parameters">Parameters</h4>
<p>input_data : np.ndarray or torch.Tensor
    The input dataset.
input_shape : list, optional
    The shape of the input data.</p>
<h4 id="simulai.models.AutoencoderCNN.summary--returns">Returns</h4>
<p>torch.Tensor
    The dataset projected over the latent space.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">summary</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prints the summary of the network architecture.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : np.ndarray or torch.Tensor</span>
<span class="sd">        The input dataset.</span>
<span class="sd">    input_shape : list, optional</span>
<span class="sd">        The shape of the input data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The dataset projected over the latent space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
            <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">input_shape</span>
            <span class="p">),</span> <span class="s2">&quot;It is necessary to have input_shape when input_data is None.&quot;</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span>
            <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">before_flatten_dimension</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Saving the content of the subnetworks to the overall architecture dictionary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="autoencoderkoopman">AutoencoderKoopman</h2>


<div class="doc doc-object doc-class">



<a id="simulai.models.AutoencoderKoopman"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>

  
      <p>This is an implementation of a Koopman autoencoder as a Reduced Order Model.</p>
<p>A Koopman autoencoder architecture consists of five stages:</p>
<ul>
<li>The convolutional encoder [Optional]</li>
<li>Fully-connected encoder</li>
<li>Koopman operator</li>
<li>Fully connected decoder</li>
<li>The convolutional decoder [Optional]</li>
</ul>

<details class="scheme" open>
  <summary>SCHEME</summary>
  <p>(Koopman OPERATOR)
         ^
  |      |      |
  |  |   |   |  |</p>
</details>      <p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til
                                  |  |       |  |
                                  |             |</p>
<pre><code>            ENCODER          DENSE BOTTLENECK        DECODER
</code></pre>

            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AutoencoderKoopman</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of a Koopman autoencoder as a Reduced Order Model.</span>

<span class="sd">    A Koopman autoencoder architecture consists of five stages:</span>

<span class="sd">    * The convolutional encoder [Optional]</span>
<span class="sd">    * Fully-connected encoder</span>
<span class="sd">    * Koopman operator</span>
<span class="sd">    * Fully connected decoder</span>
<span class="sd">    * The convolutional decoder [Optional]</span>

<span class="sd">    SCHEME:</span>
<span class="sd">                                    (Koopman OPERATOR)</span>
<span class="sd">                                             ^</span>
<span class="sd">                                      |      |      |</span>
<span class="sd">                                      |  |   |   |  |</span>
<span class="sd">    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</span>
<span class="sd">                                      |  |       |  |</span>
<span class="sd">                                      |             |</span>

<span class="sd">                    ENCODER          DENSE BOTTLENECK        DECODER</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">architecture</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_batch_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs a new instance of the Autoencoder</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">            The encoder network. Defaults to None.</span>
<span class="sd">        bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">            The bottleneck encoder network. Defaults to None.</span>
<span class="sd">        bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">            The bottleneck decoder network. Defaults to None.</span>
<span class="sd">        decoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">            The decoder network. Defaults to None.</span>
<span class="sd">        input_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">            The input dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        output_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">            The output dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        latent_dim : Optional[int], optional</span>
<span class="sd">            The latent dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        activation : Optional[Union[list, str]], optional</span>
<span class="sd">            The activation functions for each layer. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        channels : Optional[int], optional</span>
<span class="sd">            The number of channels. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        case : Optional[str], optional</span>
<span class="sd">            The type of problem. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        architecture : Optional[str], optional</span>
<span class="sd">            The network architecture. Used for automatic network generation. Defaults to None.</span>
<span class="sd">        shallow : Optional[bool], optional</span>
<span class="sd">            Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.</span>
<span class="sd">        encoder_activation : str, optional</span>
<span class="sd">            The activation function for the encoder. Defaults to &quot;relu&quot;.</span>
<span class="sd">        devices : Union[str, list], optional</span>
<span class="sd">            The devices to use. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            The name of the autoencoder. Defaults to None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderKoopman</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="c1"># Determining the kind of device to be used for allocating the</span>
        <span class="c1"># subnetworks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># If not network is provided, the automatic generation</span>
        <span class="c1"># pipeline is activated.</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

            <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="n">autoencoder_auto</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
                <span class="n">architecture</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span>
                <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
                <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
                <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="c1"># These subnetworks are optional</span>
        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="c1"># These subnetworks are optional</span>
        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection_with_bottleneck</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction_with_bottleneck</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">output_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">K_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
                <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">input_shape</span>
                <span class="p">),</span> <span class="s2">&quot;It is necessary to have input_shape when input_data is None.&quot;</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span>
                <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

                <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

            <span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

            <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Koopman Operator has shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">K_op</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">before_flatten_dimension</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Saving the content of the subnetworks to the overall architecture dictionary</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_projection_with_bottleneck</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the projection of the input data onto the bottleneck encoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The projected latent representation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

        <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span><span class="p">)))</span>

        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_projection</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the projection of the input data onto the encoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The projected latent representation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_reconstruction_with_bottleneck</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the input data using the bottleneck decoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[torch.Tensor, np.ndarray], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span>
        <span class="p">)</span>

        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_reconstruction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the input data using the decoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[torch.Tensor, np.ndarray], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">latent_forward_m</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the operation u^{u+m} = K^m u^{i}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>
<span class="sd">        m : int, optional</span>
<span class="sd">            The number of Koopman iterations. Defaults to 1.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The computed latent representation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K_op</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">latent_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the operation u^{u+1} = K u^{i}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The computed latent representation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_op</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reconstruction_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the operation  = D(E(U))</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">reconstruction_forward_m</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the operation _m = D(K^m E(U))</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>
<span class="sd">        m : int, optional</span>
<span class="sd">            The number of Koopman iterations. Defaults to 1.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">latent_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_forward_m</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">reconstructed_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent_m</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed_m</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>
<span class="sd">        n_steps : int, optional</span>
<span class="sd">            The number of extrapolations to perform. Defaults to 1.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The predicted reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">init_latent</span> <span class="o">=</span> <span class="n">latent</span>

        <span class="c1"># Extrapolating in the latent space over n_steps steps</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
            <span class="n">latent_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">init_latent</span><span class="p">)</span>
            <span class="n">init_latent</span> <span class="o">=</span> <span class="n">latent_s</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latent_s</span><span class="p">)</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

        <span class="n">reconstructed_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed_predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Projects the input data into the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The projected data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">projected_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">projected_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data. Defaults to None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">case</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shallow</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoder_activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Constructs a new instance of the Autoencoder</p>
<h4 id="simulai.models.AutoencoderKoopman.__init__--parameters">Parameters</h4>
<p>encoder : Union[ConvolutionalNetwork, DenseNetwork], optional
    The encoder network. Defaults to None.
bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional
    The bottleneck encoder network. Defaults to None.
bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional
    The bottleneck decoder network. Defaults to None.
decoder : Union[ConvolutionalNetwork, DenseNetwork], optional
    The decoder network. Defaults to None.
input_dim : Optional[Tuple[int, ...]], optional
    The input dimensions. Used for automatic network generation. Defaults to None.
output_dim : Optional[Tuple[int, ...]], optional
    The output dimensions. Used for automatic network generation. Defaults to None.
latent_dim : Optional[int], optional
    The latent dimensions. Used for automatic network generation. Defaults to None.
activation : Optional[Union[list, str]], optional
    The activation functions for each layer. Used for automatic network generation. Defaults to None.
channels : Optional[int], optional
    The number of channels. Used for automatic network generation. Defaults to None.
case : Optional[str], optional
    The type of problem. Used for automatic network generation. Defaults to None.
architecture : Optional[str], optional
    The network architecture. Used for automatic network generation. Defaults to None.
shallow : Optional[bool], optional
    Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.
encoder_activation : str, optional
    The activation function for the encoder. Defaults to "relu".
devices : Union[str, list], optional
    The devices to use. Defaults to "cpu".
name : str, optional
    The name of the autoencoder. Defaults to None.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">architecture</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_batch_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a new instance of the Autoencoder</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">        The encoder network. Defaults to None.</span>
<span class="sd">    bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">        The bottleneck encoder network. Defaults to None.</span>
<span class="sd">    bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">        The bottleneck decoder network. Defaults to None.</span>
<span class="sd">    decoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">        The decoder network. Defaults to None.</span>
<span class="sd">    input_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">        The input dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    output_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">        The output dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    latent_dim : Optional[int], optional</span>
<span class="sd">        The latent dimensions. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    activation : Optional[Union[list, str]], optional</span>
<span class="sd">        The activation functions for each layer. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    channels : Optional[int], optional</span>
<span class="sd">        The number of channels. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    case : Optional[str], optional</span>
<span class="sd">        The type of problem. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    architecture : Optional[str], optional</span>
<span class="sd">        The network architecture. Used for automatic network generation. Defaults to None.</span>
<span class="sd">    shallow : Optional[bool], optional</span>
<span class="sd">        Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.</span>
<span class="sd">    encoder_activation : str, optional</span>
<span class="sd">        The activation function for the encoder. Defaults to &quot;relu&quot;.</span>
<span class="sd">    devices : Union[str, list], optional</span>
<span class="sd">        The devices to use. Defaults to &quot;cpu&quot;.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        The name of the autoencoder. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderKoopman</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># Determining the kind of device to be used for allocating the</span>
    <span class="c1"># subnetworks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># If not network is provided, the automatic generation</span>
    <span class="c1"># pipeline is activated.</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
        <span class="p">]</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="n">autoencoder_auto</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">architecture</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span>
            <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
            <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
            <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="c1"># These subnetworks are optional</span>
    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="c1"># These subnetworks are optional</span>
    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection_with_bottleneck</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction_with_bottleneck</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">output_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">K_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.latent_forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">latent_forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluates the operation u^{u+1} = K u^{i}</p>
<h4 id="simulai.models.AutoencoderKoopman.latent_forward--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.</p>
<h4 id="simulai.models.AutoencoderKoopman.latent_forward--returns">Returns</h4>
<p>torch.Tensor
    The computed latent representation.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">latent_forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the operation u^{u+1} = K u^{i}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The computed latent representation.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_op</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.latent_forward_m" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">latent_forward_m</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluates the operation u^{u+m} = K^m u^{i}</p>
<h4 id="simulai.models.AutoencoderKoopman.latent_forward_m--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.
m : int, optional
    The number of Koopman iterations. Defaults to 1.</p>
<h4 id="simulai.models.AutoencoderKoopman.latent_forward_m--returns">Returns</h4>
<p>torch.Tensor
    The computed latent representation.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">latent_forward_m</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the operation u^{u+m} = K^m u^{i}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>
<span class="sd">    m : int, optional</span>
<span class="sd">        The number of Koopman iterations. Defaults to 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The computed latent representation.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K_op</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.predict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.</p>
<h4 id="simulai.models.AutoencoderKoopman.predict--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.
n_steps : int, optional
    The number of extrapolations to perform. Defaults to 1.</p>
<h4 id="simulai.models.AutoencoderKoopman.predict--returns">Returns</h4>
<p>np.ndarray
    The predicted reconstructed data.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>
<span class="sd">    n_steps : int, optional</span>
<span class="sd">        The number of extrapolations to perform. Defaults to 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The predicted reconstructed data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">init_latent</span> <span class="o">=</span> <span class="n">latent</span>

    <span class="c1"># Extrapolating in the latent space over n_steps steps</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">latent_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">init_latent</span><span class="p">)</span>
        <span class="n">init_latent</span> <span class="o">=</span> <span class="n">latent_s</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latent_s</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

    <span class="n">reconstructed_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed_predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.project" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">project</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Projects the input data into the latent space.</p>
<h4 id="simulai.models.AutoencoderKoopman.project--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.</p>
<h4 id="simulai.models.AutoencoderKoopman.project--returns">Returns</h4>
<p>np.ndarray
    The projected data.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Projects the input data into the latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The projected data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">projected_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">projected_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.reconstruct" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruct</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstructs the input data.</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruct--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruct--returns">Returns</h4>
<p>np.ndarray
    The reconstructed data.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstructs the input data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The reconstructed data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.reconstruction_forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction_forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluates the operation  = D(E(U))</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruction_forward--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruction_forward--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed data.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction_forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the operation  = D(E(U))</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderKoopman.reconstruction_forward_m" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction_forward_m</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Evaluates the operation _m = D(K^m E(U))</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruction_forward_m--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data. Defaults to None.
m : int, optional
    The number of Koopman iterations. Defaults to 1.</p>
<h4 id="simulai.models.AutoencoderKoopman.reconstruction_forward_m--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed data.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction_forward_m</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the operation _m = D(K^m E(U))</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data. Defaults to None.</span>
<span class="sd">    m : int, optional</span>
<span class="sd">        The number of Koopman iterations. Defaults to 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">latent_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_forward_m</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
    <span class="n">reconstructed_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent_m</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed_m</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div><h3 id="autoencodervariational">AutoencoderVariational</h3>


<div class="doc doc-object doc-class">



<a id="simulai.models.AutoencoderVariational"></a>
  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="simulai.templates.NetworkTemplate">NetworkTemplate</span></code></p>

  
      <p>This is an implementation of a Koopman autoencoder as a reduced order model.</p>
<p>A variational autoencoder architecture consists of five stages:
--&gt; The convolutional encoder [Optional]
--&gt; Fully-connected encoder
--&gt; Gaussian noise
--&gt; Fully connected decoder
--&gt; The convolutional decoder [Optional]</p>

<details class="scheme" open>
  <summary>SCHEME</summary>
  <p>Gaussian noise
^</p>
</details>      <pre><code>                                   |      |      |
                                   |  |   |   |  |
</code></pre>
<p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til
                                       |  |       |  |
                                       |             |</p>
<pre><code>           ENCODER               DENSE BOTTLENECK           DECODER
</code></pre>

            <details class="quote">
              <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AutoencoderVariational</span><span class="p">(</span><span class="n">NetworkTemplate</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of a Koopman autoencoder as a reduced order model.</span>

<span class="sd">    A variational autoencoder architecture consists of five stages:</span>
<span class="sd">    --&gt; The convolutional encoder [Optional]</span>
<span class="sd">    --&gt; Fully-connected encoder</span>
<span class="sd">    --&gt; Gaussian noise</span>
<span class="sd">    --&gt; Fully connected decoder</span>
<span class="sd">    --&gt; The convolutional decoder [Optional]</span>

<span class="sd">    SCHEME:</span>
<span class="sd">                                                  Gaussian noise</span>
<span class="sd">                                                  ^</span>
<span class="sd">                                           |      |      |</span>
<span class="sd">                                           |  |   |   |  |</span>
<span class="sd">    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</span>
<span class="sd">                                           |  |       |  |</span>
<span class="sd">                                           |             |</span>

<span class="sd">                   ENCODER               DENSE BOTTLENECK           DECODER</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">architecture</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_batch_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">            The encoder network. Defaults to None.</span>
<span class="sd">        bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">            The bottleneck encoder network. Defaults to None.</span>
<span class="sd">        bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">            The bottleneck decoder network. Defaults to None.</span>
<span class="sd">        decoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">            The decoder network. Defaults to None.</span>
<span class="sd">        encoder_activation : str, optional</span>
<span class="sd">            The activation function to use in the encoder. Defaults to &quot;relu&quot;.</span>
<span class="sd">        input_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">            The input dimension of the data. Defaults to None.</span>
<span class="sd">        output_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">            The output dimension of the data. Defaults to None.</span>
<span class="sd">        latent_dim : Optional[int], optional</span>
<span class="sd">            The size of the bottleneck layer. Defaults to None.</span>
<span class="sd">        activation : Optional[Union[list, str]], optional</span>
<span class="sd">            The activation function to use in the networks. Defaults to None.</span>
<span class="sd">        channels : Optional[int], optional</span>
<span class="sd">            The number of channels in the input data. Defaults to None.</span>
<span class="sd">        kernel_size : Optional[int]</span>
<span class="sd">            Convolutional kernel size.</span>
<span class="sd">        case : Optional[str], optional</span>
<span class="sd">            The name of the autoencoder variant. Defaults to None.</span>
<span class="sd">        architecture : Optional[str], optional</span>
<span class="sd">            The architecture of the networks. Defaults to None.</span>
<span class="sd">        shallow : Optional[bool], optional</span>
<span class="sd">            Whether to use a shallow network architecture. Defaults to False.</span>
<span class="sd">        scale : float, optional</span>
<span class="sd">            The scale of the initialization. Defaults to 1e-3.</span>
<span class="sd">        devices : Union[str, list], optional</span>
<span class="sd">            The device(s) to use for computation. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            The name of the autoencoder. Defaults to None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderVariational</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="c1"># Determining the kind of device to be used for allocating the</span>
        <span class="c1"># subnetworks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># If not network is provided, the automatic generation</span>
        <span class="c1"># pipeline is activated.</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

            <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="n">autoencoder_auto</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">architecture</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span>
                <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
                <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
                <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># These subnetworks are optional</span>
        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection_with_bottleneck</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction_with_bottleneck</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">output_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span>
                                                            <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;z_mean&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;z_log_var&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">display</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            Input data to pass through the encoder, by default None</span>
<span class="sd">        input_shape : list, optional</span>
<span class="sd">            The shape of the input data if input_data is None, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output of the autoencoder&#39;s decoder applied to the input data.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        Exception</span>
<span class="sd">            If self.input_dim is not a tuple or an integer.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If input_shape is None when input_data is None.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
                    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
                    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;input_dim is expected to be tuple or int, but received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
                <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="n">display</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">input_shape</span>
                <span class="p">),</span> <span class="s2">&quot;It is necessary to have input_shape when input_data is None.&quot;</span>

                <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

                <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

                <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

            <span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

            <span class="c1"># Bottleneck networks is are optional</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span><span class="p">:</span>
                <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>

                <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
                <span class="p">)</span>

                <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">before_flatten_dimension</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">btnk_input</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>

            <span class="c1"># Saving the content of the subnetworks to the overall architecture dictionary</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

            <span class="c1"># Bottleneck networks is are optional</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_projection_with_bottleneck</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the encoder and bottleneck encoder to input data and returns the output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the encoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output of the bottleneck encoder applied to the input data.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function is used for projection of the input data into the bottleneck space.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; output_data = autoencoder._projection_with_bottleneck(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

        <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span><span class="p">)))</span>

        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_projection</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the encoder to input data and returns the output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the encoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output of the encoder applied to the input data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; output_data = autoencoder._projection(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">latent</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_reconstruction_with_bottleneck</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the bottleneck decoder and decoder to input data and returns the output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the bottleneck decoder and decoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output of the decoder applied to the bottleneck decoder&#39;s output.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function is used for reconstruction of the input data from the bottleneck space.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; bottleneck_output = autoencoder._projection_with_bottleneck(input_data=input_data)</span>
<span class="sd">        &gt;&gt;&gt; output_data = autoencoder._reconstruction_with_bottleneck(input_data=bottleneck_output)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span>
        <span class="p">)</span>

        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="nd">@as_tensor</span>
    <span class="k">def</span> <span class="nf">_reconstruction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the decoder to input data and returns the output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the decoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output of the decoder applied to the input data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; output_data = autoencoder._reconstruction(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">Mu</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the mean of the encoded input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to encode and compute the mean, by default None</span>
<span class="sd">        to_numpy : bool, optional</span>
<span class="sd">            If True, returns the result as a NumPy array, by default False</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The mean of the encoded input data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">Sigma</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the standard deviation of the encoded input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to encode and compute the standard deviation, by default None</span>
<span class="sd">        to_numpy : bool, optional</span>
<span class="sd">            If True, returns the result as a NumPy array, by default False</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The standard deviation of the encoded input data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">CoVariance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the covariance matrix of the encoded input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to encode and compute the covariance matrix, by default None</span>
<span class="sd">        inv : bool, optional</span>
<span class="sd">            If True, returns the inverse of the covariance matrix, by default False</span>
<span class="sd">        to_numpy : bool, optional</span>
<span class="sd">            If True, returns the result as a NumPy array, by default False</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[np.ndarray, torch.Tensor]</span>
<span class="sd">            The covariance matrix (or its inverse) of the encoded input data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inv</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">Sigma_inv</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
            <span class="n">covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">Sigma_inv</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">Sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
            <span class="n">covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">covariance</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">covariance</span>

    <span class="k">def</span> <span class="nf">latent_gaussian_noisy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates a noisy latent representation of the input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to encode and generate a noisy latent representation, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            A noisy latent representation of the input data.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">reconstruction_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the autoencoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed output of the autoencoder.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">latent_noisy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_gaussian_noisy</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent_noisy</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">reconstruction_eval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to pass through the autoencoder, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The reconstructed output of the autoencoder.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed</span>

    <span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Projects the input data onto the autoencoder&#39;s latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to project onto the autoencoder&#39;s latent space, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The input data projected onto the autoencoder&#39;s latent space.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">projected_data_latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mu</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">projected_data_latent</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the input data using the trained autoencoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to reconstruct, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstructs the input data using the mean of the encoded data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">            The input data to reconstruct, by default None</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            The reconstructed data.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))</span>
<span class="sd">        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">        &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.CoVariance" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">CoVariance</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Computes the covariance matrix of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.CoVariance--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to encode and compute the covariance matrix, by default None
inv : bool, optional
    If True, returns the inverse of the covariance matrix, by default False
to_numpy : bool, optional
    If True, returns the result as a NumPy array, by default False</p>
<h4 id="simulai.models.AutoencoderVariational.CoVariance--returns">Returns</h4>
<p>Union[np.ndarray, torch.Tensor]
    The covariance matrix (or its inverse) of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.CoVariance--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
covariance = autoencoder.CoVariance(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">CoVariance</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the covariance matrix of the encoded input data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to encode and compute the covariance matrix, by default None</span>
<span class="sd">    inv : bool, optional</span>
<span class="sd">        If True, returns the inverse of the covariance matrix, by default False</span>
<span class="sd">    to_numpy : bool, optional</span>
<span class="sd">        If True, returns the result as a NumPy array, by default False</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The covariance matrix (or its inverse) of the encoded input data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inv</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">Sigma_inv</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">Sigma_inv</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">Sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">covariance</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">covariance</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.Mu" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">Mu</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Computes the mean of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.Mu--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to encode and compute the mean, by default None
to_numpy : bool, optional
    If True, returns the result as a NumPy array, by default False</p>
<h4 id="simulai.models.AutoencoderVariational.Mu--returns">Returns</h4>
<p>Union[np.ndarray, torch.Tensor]
    The mean of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.Mu--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
mu = autoencoder.Mu(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">Mu</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the mean of the encoded input data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to encode and compute the mean, by default None</span>
<span class="sd">    to_numpy : bool, optional</span>
<span class="sd">        If True, returns the result as a NumPy array, by default False</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The mean of the encoded input data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.Sigma" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">Sigma</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Computes the standard deviation of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.Sigma--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to encode and compute the standard deviation, by default None
to_numpy : bool, optional
    If True, returns the result as a NumPy array, by default False</p>
<h4 id="simulai.models.AutoencoderVariational.Sigma--returns">Returns</h4>
<p>Union[np.ndarray, torch.Tensor]
    The standard deviation of the encoded input data.</p>
<h4 id="simulai.models.AutoencoderVariational.Sigma--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
sigma = autoencoder.Sigma(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">Sigma</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the standard deviation of the encoded input data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to encode and compute the standard deviation, by default None</span>
<span class="sd">    to_numpy : bool, optional</span>
<span class="sd">        If True, returns the result as a NumPy array, by default False</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Union[np.ndarray, torch.Tensor]</span>
<span class="sd">        The standard deviation of the encoded input data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">to_numpy</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">case</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">shallow</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Constructor method.</p>
<h4 id="simulai.models.AutoencoderVariational.__init__--parameters">Parameters</h4>
<p>encoder : Union[ConvolutionalNetwork, DenseNetwork], optional
    The encoder network. Defaults to None.
bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional
    The bottleneck encoder network. Defaults to None.
bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional
    The bottleneck decoder network. Defaults to None.
decoder : Union[ConvolutionalNetwork, DenseNetwork], optional
    The decoder network. Defaults to None.
encoder_activation : str, optional
    The activation function to use in the encoder. Defaults to "relu".
input_dim : Optional[Tuple[int, ...]], optional
    The input dimension of the data. Defaults to None.
output_dim : Optional[Tuple[int, ...]], optional
    The output dimension of the data. Defaults to None.
latent_dim : Optional[int], optional
    The size of the bottleneck layer. Defaults to None.
activation : Optional[Union[list, str]], optional
    The activation function to use in the networks. Defaults to None.
channels : Optional[int], optional
    The number of channels in the input data. Defaults to None.
kernel_size : Optional[int]
    Convolutional kernel size.
case : Optional[str], optional
    The name of the autoencoder variant. Defaults to None.
architecture : Optional[str], optional
    The architecture of the networks. Defaults to None.
shallow : Optional[bool], optional
    Whether to use a shallow network architecture. Defaults to False.
scale : float, optional
    The scale of the initialization. Defaults to 1e-3.
devices : Union[str, list], optional
    The device(s) to use for computation. Defaults to "cpu".
name : str, optional
    The name of the autoencoder. Defaults to None.</p>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bottleneck_decoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Linear</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvolutionalNetwork</span><span class="p">,</span> <span class="n">DenseNetwork</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">case</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">architecture</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_batch_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shallow</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructor method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">        The encoder network. Defaults to None.</span>
<span class="sd">    bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">        The bottleneck encoder network. Defaults to None.</span>
<span class="sd">    bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional</span>
<span class="sd">        The bottleneck decoder network. Defaults to None.</span>
<span class="sd">    decoder : Union[ConvolutionalNetwork, DenseNetwork], optional</span>
<span class="sd">        The decoder network. Defaults to None.</span>
<span class="sd">    encoder_activation : str, optional</span>
<span class="sd">        The activation function to use in the encoder. Defaults to &quot;relu&quot;.</span>
<span class="sd">    input_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">        The input dimension of the data. Defaults to None.</span>
<span class="sd">    output_dim : Optional[Tuple[int, ...]], optional</span>
<span class="sd">        The output dimension of the data. Defaults to None.</span>
<span class="sd">    latent_dim : Optional[int], optional</span>
<span class="sd">        The size of the bottleneck layer. Defaults to None.</span>
<span class="sd">    activation : Optional[Union[list, str]], optional</span>
<span class="sd">        The activation function to use in the networks. Defaults to None.</span>
<span class="sd">    channels : Optional[int], optional</span>
<span class="sd">        The number of channels in the input data. Defaults to None.</span>
<span class="sd">    kernel_size : Optional[int]</span>
<span class="sd">        Convolutional kernel size.</span>
<span class="sd">    case : Optional[str], optional</span>
<span class="sd">        The name of the autoencoder variant. Defaults to None.</span>
<span class="sd">    architecture : Optional[str], optional</span>
<span class="sd">        The architecture of the networks. Defaults to None.</span>
<span class="sd">    shallow : Optional[bool], optional</span>
<span class="sd">        Whether to use a shallow network architecture. Defaults to False.</span>
<span class="sd">    scale : float, optional</span>
<span class="sd">        The scale of the initialization. Defaults to 1e-3.</span>
<span class="sd">    devices : Union[str, list], optional</span>
<span class="sd">        The device(s) to use for computation. Defaults to &quot;cpu&quot;.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        The name of the autoencoder. Defaults to None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AutoencoderVariational</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># Determining the kind of device to be used for allocating the</span>
    <span class="c1"># subnetworks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># If not network is provided, the automatic generation</span>
    <span class="c1"># pipeline is activated.</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">isn</span> <span class="o">==</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">isn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span><span class="p">]</span>
        <span class="p">]</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="n">autoencoder_auto</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">architecture</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span>
            <span class="n">case</span><span class="o">=</span><span class="n">case</span><span class="p">,</span>
            <span class="n">shallow</span><span class="o">=</span><span class="n">shallow</span><span class="p">,</span>
            <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weights</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># These subnetworks are optional</span>
    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bottleneck_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_encoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">bottleneck_decoder</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">weights</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection_with_bottleneck</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction_with_bottleneck</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruction</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">last_encoder_channels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">bottleneck_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">output_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimension</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;z_mean&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;z_log_var&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation</span><span class="p">(</span><span class="n">operation</span><span class="o">=</span><span class="n">encoder_activation</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.eval" class="doc doc-heading">
          <code class="highlight language-python"><span class="nb">eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstructs the input data using the mean of the encoded data.</p>
<h4 id="simulai.models.AutoencoderVariational.eval--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to reconstruct, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.eval--returns">Returns</h4>
<p>np.ndarray
    The reconstructed data.</p>
<h4 id="simulai.models.AutoencoderVariational.eval--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = Autoencoder(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
reconstructed_data = autoencoder.eval(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstructs the input data using the mean of the encoded data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to reconstruct, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The reconstructed data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.latent_gaussian_noisy" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">latent_gaussian_noisy</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Generates a noisy latent representation of the input data.</p>
<h4 id="simulai.models.AutoencoderVariational.latent_gaussian_noisy--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to encode and generate a noisy latent representation, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.latent_gaussian_noisy--returns">Returns</h4>
<p>torch.Tensor
    A noisy latent representation of the input data.</p>
<h4 id="simulai.models.AutoencoderVariational.latent_gaussian_noisy--notes">Notes</h4>
<p>This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.</p>
<h4 id="simulai.models.AutoencoderVariational.latent_gaussian_noisy--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">latent_gaussian_noisy</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a noisy latent representation of the input data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to encode and generate a noisy latent representation, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        A noisy latent representation of the input data.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_v</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.project" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">project</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Projects the input data onto the autoencoder's latent space.</p>
<h4 id="simulai.models.AutoencoderVariational.project--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to project onto the autoencoder's latent space, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.project--returns">Returns</h4>
<p>np.ndarray
    The input data projected onto the autoencoder's latent space.</p>
<h4 id="simulai.models.AutoencoderVariational.project--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
projected_data = autoencoder.project(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Projects the input data onto the autoencoder&#39;s latent space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to project onto the autoencoder&#39;s latent space, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The input data projected onto the autoencoder&#39;s latent space.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">projected_data_latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mu</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">projected_data_latent</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.reconstruct" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruct</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstructs the input data using the trained autoencoder.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruct--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to reconstruct, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruct--returns">Returns</h4>
<p>np.ndarray
    The reconstructed data.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruct--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = Autoencoder(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
reconstructed_data = autoencoder.reconstruct(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstructs the input data using the trained autoencoder.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to reconstruct, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        The reconstructed data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ARRAY_DTYPE</span><span class="p">))</span>

    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">reconstructed_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.reconstruction_eval" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction_eval</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_eval--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to pass through the autoencoder, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_eval--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed output of the autoencoder.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_eval--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction_eval</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to pass through the autoencoder, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed output of the autoencoder.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.reconstruction_forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction_forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_forward--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    The input data to pass through the autoencoder, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_forward--returns">Returns</h4>
<p>torch.Tensor
    The reconstructed output of the autoencoder.</p>
<h4 id="simulai.models.AutoencoderVariational.reconstruction_forward--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction_forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        The input data to pass through the autoencoder, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The reconstructed output of the autoencoder.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">latent_noisy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_gaussian_noisy</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent_noisy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="simulai.models.AutoencoderVariational.summary" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.</p>
<h4 id="simulai.models.AutoencoderVariational.summary--parameters">Parameters</h4>
<p>input_data : Union[np.ndarray, torch.Tensor], optional
    Input data to pass through the encoder, by default None
input_shape : list, optional
    The shape of the input data if input_data is None, by default None</p>
<h4 id="simulai.models.AutoencoderVariational.summary--returns">Returns</h4>
<p>torch.Tensor
    The output of the autoencoder's decoder applied to the input data.</p>
<h4 id="simulai.models.AutoencoderVariational.summary--raises">Raises</h4>
<p>Exception
    If self.input_dim is not a tuple or an integer.</p>
<p>AssertionError
    If input_shape is None when input_data is None.</p>
<h4 id="simulai.models.AutoencoderVariational.summary--notes">Notes</h4>
<p>The summary method calls the <code>summary</code> method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.</p>
<h4 id="simulai.models.AutoencoderVariational.summary--examples">Examples</h4>
<blockquote>
<blockquote>
<blockquote>
<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))
input_data = np.random.rand(1, 28, 28, 1)
output_data = autoencoder.summary(input_data=input_data)</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">summary</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">display</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_data : Union[np.ndarray, torch.Tensor], optional</span>
<span class="sd">        Input data to pass through the encoder, by default None</span>
<span class="sd">    input_shape : list, optional</span>
<span class="sd">        The shape of the input data if input_data is None, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The output of the autoencoder&#39;s decoder applied to the input data.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    Exception</span>
<span class="sd">        If self.input_dim is not a tuple or an integer.</span>

<span class="sd">    AssertionError</span>
<span class="sd">        If input_shape is None when input_data is None.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)</span>
<span class="sd">    &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;input_dim is expected to be tuple or int, but received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
            <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="n">display</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">input_size</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">input_shape</span>
            <span class="p">),</span> <span class="s2">&quot;It is necessary to have input_shape when input_data is None.&quot;</span>

            <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_wrap</span><span class="p">(</span><span class="n">entity</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="n">btnk_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

        <span class="n">before_flatten_dimension</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">btnk_input</span> <span class="o">=</span> <span class="n">btnk_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">btnk_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

        <span class="c1"># Bottleneck networks is are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span><span class="p">:</span>
            <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">btnk_input</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_activation</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">latent</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">bottleneck_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">before_flatten_dimension</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bottleneck_output</span> <span class="o">=</span> <span class="n">btnk_input</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">bottleneck_output</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="n">display</span><span class="p">)</span>

        <span class="c1"># Saving the content of the subnetworks to the overall architecture dictionary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

        <span class="c1"># Bottleneck networks is are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">there_is_bottleneck</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_encoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_encoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;bottleneck_decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">}</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;decoder&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">shapes_dict</span><span class="p">})</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.aecac24b.min.js"></script>
      
    
  </body>
</html>