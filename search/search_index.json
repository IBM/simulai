{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SimulAI","text":"<p>An extensible Python package with data-driven pipelines for physics-informed machine learning.</p> <p>The SimulAI toolkit provides easy access to state-of-the-art models and algorithms for physics-informed machine learning. Currently, it includes the following methods described in the literature:</p> <ul> <li>Physics-Informed Neural Networks (PINNs)</li> <li>Deep Operator Networks (DeepONets)</li> <li>Variational Encoder-Decoders (VED)</li> <li>Operator Inference (OpInf)</li> <li>Koopman Autoencoders (experimental)</li> <li>Echo State Networks (experimental GPU support)</li> <li>Transformers</li> <li>U-Nets</li> </ul> <p>In addition to the methods above, many more techniques for model reduction and regularization are included in SimulAI. See documentation.</p>"},{"location":"#installing","title":"Installing","text":"<p>Python version requirements: 3.9 \\&lt;= python \\&lt;= 3.11</p>"},{"location":"#using-pip","title":"Using pip","text":"<p>For installing the most recent stable version from PyPI:</p> <pre><code>pip install simulai-toolkit\n</code></pre> <p>For installing from the latest commit sent to GitHub (just for testing and developing purposes):</p> <pre><code>pip uninstall simulai_toolkit\npip install -U git+https://github.com/IBM/simulai@$(git ls-remote git@github.com:IBM/simulai.git  | head -1 | awk '{print $1;}')#egg=simulai_toolkit\n</code></pre>"},{"location":"#contributing-code-to-simulai","title":"Contributing code to SimulAI","text":"<p>If you are interested in directly contributing to this project, please see CONTRIBUTING.</p>"},{"location":"#using-mpi","title":"Using MPI","text":"<p>Some methods implemented on SimulAI support multiprocessing with MPI.</p> <p>In order to use it, you will need a valid MPI distribution, e.g. MPICH, OpenMPI. As an example, you can use <code>conda</code> to install MPICH as follows:</p> <pre><code>conda install -c conda-forge mpich gcc\n</code></pre>"},{"location":"#issues-with-macos","title":"Issues with macOS","text":"<p>If you have problems installing <code>gcc</code> using the command above, we recommend you to install it using Homebrew.</p>"},{"location":"#using-tensorboard","title":"Using Tensorboard","text":"<p>Tensorboard is supported for monitoring neural network training tasks. For a tutorial about how to set it see this example.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Please, refer to the SimulAI API documentation before using the toolkit.</p>"},{"location":"#examples","title":"Examples","text":"<p>Additionally, you can refer to examples in the respective folder.</p>"},{"location":"#license","title":"License","text":"<p>This software is licensed under Apache license 2.0. See LICENSE.</p>"},{"location":"#contributing-code-to-simulai_1","title":"Contributing code to SimulAI","text":"<p>If you are interested in directly contributing to this project, please see CONTRIBUTING.</p>"},{"location":"#how-to-cite-simulai-in-your-publications","title":"How to cite SimulAI in your publications","text":"<p>If you find SimulAI to be useful, please consider citing it in your published work:</p> <pre><code>@misc{simulai,\n  author = {IBM},\n  title = {SimulAI Toolkit},\n  subtitle = {A Python package with data-driven pipelines for physics-informed machine learning},\n  note = \"https://github.com/IBM/simulai\",\n  doi = {10.5281/zenodo.7351516},\n  year = {2022},\n}\n</code></pre> <p>or, via Zenodo:</p> <pre><code>@software{joao_lucas_de_sousa_almeida_2023_7566603,\n      author       = {Jo\u00e3o Lucas de Sousa Almeida and\n                      Leonardo Martins and\n                      Tar\u0131k Kaan Ko\u00e7},\n      title        = {IBM/simulai: 0.99.13},\n      month        = jan,\n      year         = 2023,\n      publisher    = {Zenodo},\n      version      = {0.99.25},\n      doi          = {10.5281/zenodo.7566603},\n      url          = {https://doi.org/10.5281/zenodo.7566603}\n    }\n</code></pre>"},{"location":"#publications","title":"Publications","text":"<p>Jo\u00e3o Lucas de Sousa Almeida, Pedro Roberto Barbosa Rocha, Allan Moreira de Carvalho and Alberto Costa Nogueira Jr. A coupled Variational Encoder-Decoder - DeepONet surrogate model for the Rayleigh-B\u00e9nard convection problem. In When Machine Learning meets Dynamical Systems: Theory and Applications, AAAI, 2023.</p> <p>Jo\u00e3o Lucas S. Almeida, Arthur C. Pires, Klaus F. V. Cid, and Alberto C. Nogueira Jr. Non-intrusive operator inference for chaotic systems. IEEE Transactions on Artificial Intelligence, pages 1--14, 2022.</p> <p>Pedro Roberto Barbosa Rocha, Marcos Sebasti\u00e3o de Paula Gomes, Allan Moreira de Carvalho, Jo\u00e3o Lucas de Sousa Almeida and Alberto Costa Nogueira Jr. Data-driven reduced-order model for atmospheric CO2 dispersion. In AAAI 2022 Fall Symposium: The Role of AI in Responding to Climate Challenges, 2022.</p> <p>Pedro Roberto Barbosa Rocha, Jo\u00e3o Lucas de Sousa Almeida, Marcos Sebasti\u00e3o de Paula Gomes, Alberto Costa Nogueira, Reduced-order modeling of the two-dimensional Rayleigh--B\u00e9nard convection flow through a non-intrusive operator inference, Engineering Applications of Artificial Intelligence, Volume 126, Part B, 2023, 106923, ISSN 0952-1976, https://doi.org/10.1016/j.engappai.2023.106923. (https://www.sciencedirect.com/science/article/pii/S0952197623011077)</p>"},{"location":"#references","title":"References","text":"<p>Jaeger, H., Haas, H. (2004). \\\"Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication,\\\" Science, 304 (5667): 78--80.  \\&lt;https://doi.org/10.1126/science.1091277&gt;`_. <p>Lu, L., Jin, P., Pang, G., Zhang, Z., Karniadakis, G. E. (2021). \\\"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,\\\" Nature Machine Intelligence, 3 (1): 218--229. ISSN: 2522-5839.  \\&lt;https://doi.org/10.1038/s42256-021-00302-5&gt;`_. <p>Eivazi, H., Le Clainche, S., Hoyas, S., Vinuesa, R. (2022) \\\"Towards extraction of orthogonal and parsimonious non-linear modes from turbulent flows\\\" Expert Systems with Applications, 202. ISSN: 0957-4174.  \\&lt;https://doi.org/10.1016/j.eswa.2022.117038&gt;`_. <p>Raissi, M., Perdikaris, P., Karniadakis, G. E. (2019). \\\"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,\\\" Journal of Computational Physics, 378 (1): 686-707. ISSN: 0021-9991.  \\&lt;https://doi.org/10.1016/j.jcp.2018.10.045&gt;`_. <p>Lusch, B., Kutz, J. N., Brunton, S.L. (2018). \\\"Deep learning for universal linear embeddings of nonlinear dynamics,\\\" Nature Communications, 9: 4950. ISSN: 2041-1723.  \\&lt;https://doi.org/10.1038/s41467-018-07210-0&gt;`_. <p>McQuarrie, S., Huang, C. and Willcox, K. (2021). \\\"Data-driven reduced-order models via regularized operator inference for a single-injector combustion process,\\\" Journal of the Royal Society of New Zealand, 51(2): 194-211. ISSN: 0303-6758.  \\&lt;https://doi.org/10.1080/03036758.2020.1863237&gt;`_."},{"location":"simulai_models/","title":"simulai.models","text":""},{"location":"simulai_models/#transformer","title":"Transformer","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>class Transformer(NetworkTemplate):\n\n    def __init__(self, num_heads_encoder:int=1,\n                       num_heads_decoder:int=1,\n                       embed_dim_encoder:int=Union[int, Tuple],\n                       embed_dim_decoder:int=Union[int, Tuple],\n                       encoder_activation: Union[str, torch.nn.Module]='relu',\n                       decoder_activation: Union[str, torch.nn.Module]='relu',\n                       encoder_mlp_layer_config:dict=None,\n                       decoder_mlp_layer_config:dict=None,\n                       number_of_encoders:int=1,\n                       number_of_decoders:int=1) -&gt; None:\n        r\"\"\"A classical encoder-decoder transformer:\n\n             U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n            (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n\n        Args:\n            num_heads_encoder (int, optional): The number of heads for the self-attention layer of the encoder. (Default value = 1)\n            num_heads_decoder (int, optional): The number of heads for the self-attention layer of the decoder. (Default value = 1)\n            embed_dim_encoder (int, optional): The dimension of the embedding for the encoder. (Default value = Union[int, Tuple])\n            embed_dim_decoder (int, optional): The dimension of the embedding for the decoder. (Default value = Union[int, Tuple])\n            encoder_activation (Union[str, torch.nn.Module], optional): The activation to be used in all the encoder layers. (Default value = 'relu')\n            decoder_activation (Union[str, torch.nn.Module], optional): The activation to be used in all the decoder layers. (Default value = 'relu')\n            encoder_mlp_layer_config (dict, optional): A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)\n            decoder_mlp_layer_config (dict, optional): A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)\n            number_of_encoders (int, optional): The number of encoders to be used. (Default value = 1)\n            number_of_decoders (int, optional): The number of decoders to be used. (Default value = 1)\n\n        \"\"\"\n\n        super(Transformer, self).__init__()\n\n        self.num_heads_encoder = num_heads_encoder\n        self.num_heads_decoder = num_heads_decoder\n\n        self.embed_dim_encoder = embed_dim_encoder\n        self.embed_dim_decoder = embed_dim_decoder\n\n        self.encoder_mlp_layer_dict = encoder_mlp_layer_config\n        self.decoder_mlp_layer_dict = decoder_mlp_layer_config\n\n        self.number_of_encoders = number_of_encoders\n        self.number_of_decoders = number_of_encoders\n\n        self.encoder_activation = encoder_activation\n        self.decoder_activation = decoder_activation\n\n        self.encoder_mlp_layers_list = list()\n        self.decoder_mlp_layers_list = list()\n\n        # Creating independent copies for the MLP layers which will be used \n        # by the multiple encoders/decoders.\n        for e in range(self.number_of_encoders):\n            self.encoder_mlp_layers_list.append(\n                                                    DenseNetwork(**self.encoder_mlp_layer_dict)\n                                               )\n\n        for d in range(self.number_of_decoders):\n            self.decoder_mlp_layers_list.append(\n\n                                                    DenseNetwork(**self.decoder_mlp_layer_dict)\n                                               )\n\n        # Defining the encoder architecture\n        self.EncoderStage = torch.nn.Sequential(\n                                *[BasicEncoder(num_heads=self.num_heads_encoder,\n                                              activation=self.encoder_activation,\n                                              mlp_layer=self.encoder_mlp_layers_list[e],\n                                              embed_dim=self.embed_dim_encoder) for e in range(self.number_of_encoders)]\n                            )\n\n        # Defining the decoder architecture\n        self.DecoderStage =  torch.nn.ModuleList([BasicDecoder(num_heads=self.num_heads_decoder,\n                                                               activation=self.decoder_activation,\n                                                               mlp_layer=self.decoder_mlp_layers_list[d],\n                                                               embed_dim=self.embed_dim_decoder) for d in range(self.number_of_decoders)\n                              ])\n\n\n        self.weights = list()\n\n        for e, encoder_e in enumerate(self.EncoderStage):\n            self.weights += encoder_e.weights\n            self.add_module(f\"encoder_{e}\", encoder_e)\n\n        for d, decoder_d in enumerate(self.DecoderStage):\n            self.weights += decoder_d.weights\n            self.add_module(f\"decoder_{d}\", decoder_d)\n\n    @as_tensor\n    def forward(self, input_data: Union[torch.Tensor, np.ndarray] = None) -&gt; torch.Tensor:\n\n        \"\"\"\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input dataset. (Default value = None)\n\n        Returns:\n            torch.Tensor: The transformer output.\n\n        \"\"\"\n\n        encoder_output = self.EncoderStage(input_data)\n\n        current_input = input_data\n        for decoder in self.DecoderStage:\n            output = decoder(input_data=current_input, encoder_output=encoder_output)\n            current_input = output\n\n        return output\n\n    def summary(self):\n        \"\"\"It prints a general view of the architecture.\"\"\"\n\n        print(self)\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.__init__","title":"<code>__init__(num_heads_encoder=1, num_heads_decoder=1, embed_dim_encoder=Union[int, Tuple], embed_dim_decoder=Union[int, Tuple], encoder_activation='relu', decoder_activation='relu', encoder_mlp_layer_config=None, decoder_mlp_layer_config=None, number_of_encoders=1, number_of_decoders=1)</code>","text":"<p>A classical encoder-decoder transformer:</p> <pre><code> U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n(u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_heads_encoder</code> <code>int</code> <p>The number of heads for the self-attention layer of the encoder. (Default value = 1)</p> <code>1</code> <code>num_heads_decoder</code> <code>int</code> <p>The number of heads for the self-attention layer of the decoder. (Default value = 1)</p> <code>1</code> <code>embed_dim_encoder</code> <code>int</code> <p>The dimension of the embedding for the encoder. (Default value = Union[int, Tuple])</p> <code>Union[int, Tuple]</code> <code>embed_dim_decoder</code> <code>int</code> <p>The dimension of the embedding for the decoder. (Default value = Union[int, Tuple])</p> <code>Union[int, Tuple]</code> <code>encoder_activation</code> <code>Union[str, Module]</code> <p>The activation to be used in all the encoder layers. (Default value = 'relu')</p> <code>'relu'</code> <code>decoder_activation</code> <code>Union[str, Module]</code> <p>The activation to be used in all the decoder layers. (Default value = 'relu')</p> <code>'relu'</code> <code>encoder_mlp_layer_config</code> <code>dict</code> <p>A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)</p> <code>None</code> <code>decoder_mlp_layer_config</code> <code>dict</code> <p>A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)</p> <code>None</code> <code>number_of_encoders</code> <code>int</code> <p>The number of encoders to be used. (Default value = 1)</p> <code>1</code> <code>number_of_decoders</code> <code>int</code> <p>The number of decoders to be used. (Default value = 1)</p> <code>1</code> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>def __init__(self, num_heads_encoder:int=1,\n                   num_heads_decoder:int=1,\n                   embed_dim_encoder:int=Union[int, Tuple],\n                   embed_dim_decoder:int=Union[int, Tuple],\n                   encoder_activation: Union[str, torch.nn.Module]='relu',\n                   decoder_activation: Union[str, torch.nn.Module]='relu',\n                   encoder_mlp_layer_config:dict=None,\n                   decoder_mlp_layer_config:dict=None,\n                   number_of_encoders:int=1,\n                   number_of_decoders:int=1) -&gt; None:\n    r\"\"\"A classical encoder-decoder transformer:\n\n         U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n        (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n\n    Args:\n        num_heads_encoder (int, optional): The number of heads for the self-attention layer of the encoder. (Default value = 1)\n        num_heads_decoder (int, optional): The number of heads for the self-attention layer of the decoder. (Default value = 1)\n        embed_dim_encoder (int, optional): The dimension of the embedding for the encoder. (Default value = Union[int, Tuple])\n        embed_dim_decoder (int, optional): The dimension of the embedding for the decoder. (Default value = Union[int, Tuple])\n        encoder_activation (Union[str, torch.nn.Module], optional): The activation to be used in all the encoder layers. (Default value = 'relu')\n        decoder_activation (Union[str, torch.nn.Module], optional): The activation to be used in all the decoder layers. (Default value = 'relu')\n        encoder_mlp_layer_config (dict, optional): A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)\n        decoder_mlp_layer_config (dict, optional): A configuration dictionary to instantiate the encoder MLP layer.weights (Default value = None)\n        number_of_encoders (int, optional): The number of encoders to be used. (Default value = 1)\n        number_of_decoders (int, optional): The number of decoders to be used. (Default value = 1)\n\n    \"\"\"\n\n    super(Transformer, self).__init__()\n\n    self.num_heads_encoder = num_heads_encoder\n    self.num_heads_decoder = num_heads_decoder\n\n    self.embed_dim_encoder = embed_dim_encoder\n    self.embed_dim_decoder = embed_dim_decoder\n\n    self.encoder_mlp_layer_dict = encoder_mlp_layer_config\n    self.decoder_mlp_layer_dict = decoder_mlp_layer_config\n\n    self.number_of_encoders = number_of_encoders\n    self.number_of_decoders = number_of_encoders\n\n    self.encoder_activation = encoder_activation\n    self.decoder_activation = decoder_activation\n\n    self.encoder_mlp_layers_list = list()\n    self.decoder_mlp_layers_list = list()\n\n    # Creating independent copies for the MLP layers which will be used \n    # by the multiple encoders/decoders.\n    for e in range(self.number_of_encoders):\n        self.encoder_mlp_layers_list.append(\n                                                DenseNetwork(**self.encoder_mlp_layer_dict)\n                                           )\n\n    for d in range(self.number_of_decoders):\n        self.decoder_mlp_layers_list.append(\n\n                                                DenseNetwork(**self.decoder_mlp_layer_dict)\n                                           )\n\n    # Defining the encoder architecture\n    self.EncoderStage = torch.nn.Sequential(\n                            *[BasicEncoder(num_heads=self.num_heads_encoder,\n                                          activation=self.encoder_activation,\n                                          mlp_layer=self.encoder_mlp_layers_list[e],\n                                          embed_dim=self.embed_dim_encoder) for e in range(self.number_of_encoders)]\n                        )\n\n    # Defining the decoder architecture\n    self.DecoderStage =  torch.nn.ModuleList([BasicDecoder(num_heads=self.num_heads_decoder,\n                                                           activation=self.decoder_activation,\n                                                           mlp_layer=self.decoder_mlp_layers_list[d],\n                                                           embed_dim=self.embed_dim_decoder) for d in range(self.number_of_decoders)\n                          ])\n\n\n    self.weights = list()\n\n    for e, encoder_e in enumerate(self.EncoderStage):\n        self.weights += encoder_e.weights\n        self.add_module(f\"encoder_{e}\", encoder_e)\n\n    for d, decoder_d in enumerate(self.DecoderStage):\n        self.weights += decoder_d.weights\n        self.add_module(f\"decoder_{d}\", decoder_d)\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>The input dataset. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The transformer output.</p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>@as_tensor\ndef forward(self, input_data: Union[torch.Tensor, np.ndarray] = None) -&gt; torch.Tensor:\n\n    \"\"\"\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): The input dataset. (Default value = None)\n\n    Returns:\n        torch.Tensor: The transformer output.\n\n    \"\"\"\n\n    encoder_output = self.EncoderStage(input_data)\n\n    current_input = input_data\n    for decoder in self.DecoderStage:\n        output = decoder(input_data=current_input, encoder_output=encoder_output)\n        current_input = output\n\n    return output\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.summary","title":"<code>summary()</code>","text":"<p>It prints a general view of the architecture.</p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>def summary(self):\n    \"\"\"It prints a general view of the architecture.\"\"\"\n\n    print(self)\n</code></pre>"},{"location":"simulai_models/#u-net","title":"U-Net","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>class UNet(NetworkTemplate):\n\n\n    def __init__(self, layers_config:Dict=None,\n                 intermediary_outputs_indices:List[int]=None,\n                 intermediary_inputs_indices:List[int]=None,\n                 encoder_extra_args:Dict=dict(),\n                 decoder_extra_args:Dict=dict()) -&gt; None:\n        \"\"\"U-Net.\n\n        Args:\n            layers_config (Dict, optional): A dictionary containing the complete configuration for the\n            U-Net encoder and decoder. (Default value = None)\n            intermediary_outputs_indices (List[int], optional): A list of indices for indicating the encoder outputs. (Default value = None)\n            intermediary_inputs_indices (List[int], optional): A list of indices for indicating the decoder inputs. (Default value = None)\n            encoder_extra_args (Dict, optional): A dictionary containing extra arguments for the encoder. (Default value = dict())\n            decoder_extra_args (Dict, optional): A dictionary containing extra arguments for the decoder. (Default value = dict())\n\n        \"\"\"\n\n        super(UNet, self).__init__()\n\n        self.layers_config = layers_config\n        self.intermediary_outputs_indices = intermediary_outputs_indices\n        self.intermediary_inputs_indices = intermediary_inputs_indices\n\n        self.layers_config_encoder = self.layers_config[\"encoder\"] \n        self.layers_config_decoder = self.layers_config[\"decoder\"] \n\n        self.encoder_activations = self.layers_config[\"encoder_activations\"]\n        self.decoder_activations = self.layers_config[\"decoder_activations\"]\n\n        self.encoder_horizontal_outputs = dict()\n\n        # Configuring the encoder\n        encoder_type = self.layers_config_encoder.get(\"type\")\n        layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n        if encoder_type == \"cnn\":\n            self.encoder = CNNUnetEncoder(layers=self.layers_config_encoder[\"architecture\"],\n                                          activations=self.encoder_activations,\n                                          intermediary_outputs_indices=self.intermediary_outputs_indices,\n                                          case=\"2d\", name=\"encoder\",\n                                          **encoder_extra_args)\n        else:\n            raise Exception(f\"Option {encoder_type} is not available.\")\n\n         # Configuring the decoder\n        decoder_type = self.layers_config_decoder.get(\"type\")\n        layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n        if encoder_type == \"cnn\":\n            self.decoder = CNNUnetDecoder(layers=self.layers_config_decoder[\"architecture\"],\n                                          activations=self.decoder_activations,\n                                          intermediary_inputs_indices=self.intermediary_inputs_indices,\n                                          case=\"2d\", name=\"decoder\",\n                                          **decoder_extra_args)\n        else:\n            raise Exception(f\"Option {encoder_type} is not available.\")\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n    @as_tensor\n    def forward(self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"The U-Net forward method.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): A dataset to be inputted in the CNN U-Net encoder. (Default value = None)\n\n        Returns:\n            torch.Tensor: The U-Net output.\n\n        \"\"\"\n\n        encoder_main_output, encoder_intermediary_outputs = self.encoder(input_data=input_data)\n        output = self.decoder(input_data = encoder_main_output,\n                              intermediary_encoder_outputs=encoder_intermediary_outputs)\n\n        return output\n\n    def summary(self):\n        \"\"\"It shows a general view of the architecture.\"\"\"\n\n        print(self)\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.__init__","title":"<code>__init__(layers_config=None, intermediary_outputs_indices=None, intermediary_inputs_indices=None, encoder_extra_args=dict(), decoder_extra_args=dict())</code>","text":"<p>U-Net.</p> <p>Parameters:</p> Name Type Description Default <code>layers_config</code> <code>Dict</code> <p>A dictionary containing the complete configuration for the</p> <code>None</code> <code>intermediary_outputs_indices</code> <code>List[int]</code> <p>A list of indices for indicating the encoder outputs. (Default value = None)</p> <code>None</code> <code>intermediary_inputs_indices</code> <code>List[int]</code> <p>A list of indices for indicating the decoder inputs. (Default value = None)</p> <code>None</code> <code>encoder_extra_args</code> <code>Dict</code> <p>A dictionary containing extra arguments for the encoder. (Default value = dict())</p> <code>dict()</code> <code>decoder_extra_args</code> <code>Dict</code> <p>A dictionary containing extra arguments for the decoder. (Default value = dict())</p> <code>dict()</code> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>def __init__(self, layers_config:Dict=None,\n             intermediary_outputs_indices:List[int]=None,\n             intermediary_inputs_indices:List[int]=None,\n             encoder_extra_args:Dict=dict(),\n             decoder_extra_args:Dict=dict()) -&gt; None:\n    \"\"\"U-Net.\n\n    Args:\n        layers_config (Dict, optional): A dictionary containing the complete configuration for the\n        U-Net encoder and decoder. (Default value = None)\n        intermediary_outputs_indices (List[int], optional): A list of indices for indicating the encoder outputs. (Default value = None)\n        intermediary_inputs_indices (List[int], optional): A list of indices for indicating the decoder inputs. (Default value = None)\n        encoder_extra_args (Dict, optional): A dictionary containing extra arguments for the encoder. (Default value = dict())\n        decoder_extra_args (Dict, optional): A dictionary containing extra arguments for the decoder. (Default value = dict())\n\n    \"\"\"\n\n    super(UNet, self).__init__()\n\n    self.layers_config = layers_config\n    self.intermediary_outputs_indices = intermediary_outputs_indices\n    self.intermediary_inputs_indices = intermediary_inputs_indices\n\n    self.layers_config_encoder = self.layers_config[\"encoder\"] \n    self.layers_config_decoder = self.layers_config[\"decoder\"] \n\n    self.encoder_activations = self.layers_config[\"encoder_activations\"]\n    self.decoder_activations = self.layers_config[\"decoder_activations\"]\n\n    self.encoder_horizontal_outputs = dict()\n\n    # Configuring the encoder\n    encoder_type = self.layers_config_encoder.get(\"type\")\n    layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n    if encoder_type == \"cnn\":\n        self.encoder = CNNUnetEncoder(layers=self.layers_config_encoder[\"architecture\"],\n                                      activations=self.encoder_activations,\n                                      intermediary_outputs_indices=self.intermediary_outputs_indices,\n                                      case=\"2d\", name=\"encoder\",\n                                      **encoder_extra_args)\n    else:\n        raise Exception(f\"Option {encoder_type} is not available.\")\n\n     # Configuring the decoder\n    decoder_type = self.layers_config_decoder.get(\"type\")\n    layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n    if encoder_type == \"cnn\":\n        self.decoder = CNNUnetDecoder(layers=self.layers_config_decoder[\"architecture\"],\n                                      activations=self.decoder_activations,\n                                      intermediary_inputs_indices=self.intermediary_inputs_indices,\n                                      case=\"2d\", name=\"decoder\",\n                                      **decoder_extra_args)\n    else:\n        raise Exception(f\"Option {encoder_type} is not available.\")\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.forward","title":"<code>forward(input_data=None)</code>","text":"<p>The U-Net forward method.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>A dataset to be inputted in the CNN U-Net encoder. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The U-Net output.</p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>@as_tensor\ndef forward(self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n    \"\"\"The U-Net forward method.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): A dataset to be inputted in the CNN U-Net encoder. (Default value = None)\n\n    Returns:\n        torch.Tensor: The U-Net output.\n\n    \"\"\"\n\n    encoder_main_output, encoder_intermediary_outputs = self.encoder(input_data=input_data)\n    output = self.decoder(input_data = encoder_main_output,\n                          intermediary_encoder_outputs=encoder_intermediary_outputs)\n\n    return output\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.summary","title":"<code>summary()</code>","text":"<p>It shows a general view of the architecture.</p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>def summary(self):\n    \"\"\"It shows a general view of the architecture.\"\"\"\n\n    print(self)\n</code></pre>"},{"location":"simulai_models/#deeponet","title":"DeepONet","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>class DeepONet(NetworkTemplate):\n    name = \"deeponet\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        trunk_network: NetworkTemplate = None,\n        branch_network: NetworkTemplate = None,\n        decoder_network: NetworkTemplate = None,  # The decoder network is optional and considered\n        var_dim: int = 1,  # less effective than the output reshaping alternative\n        devices: Union[str, list] = \"cpu\",\n        product_type: str = None,\n        rescale_factors: np.ndarray = None,\n        model_id:str=None,\n        use_bias:bool=False,\n    ) -&gt; None:\n        \"\"\"Classical Deep Operator Network (DeepONet), a deep learning version\n        of the Universal Approximation Theorem.\n\n        Args:\n            trunk_network (NetworkTemplate, optional): Subnetwork for processing the coordinates inputs. (Default value = None)\n            branch_network (NetworkTemplate, optional): Subnetwork for processing the forcing/conditioning inputs. (Default value = None)\n            decoder_network (NetworkTemplate, optional): Subnetworks for converting the embedding to the output (optional). (Default value = None)\n            # (Union[str, list], optional):  (Default value = \"cpu\")\n            product_type (str, optional): Type of product to execute in the embedding space. (Default value = None)\n            rescale_factors (np.ndarray, optional): Values used for rescaling the network outputs for a given order of magnitude. (Default value = None)\n            model_id (str, optional): Name for the model (Default value = None)\n            use_bias (bool, optional):  (Default value = False)\n\n        \"\"\"\n\n        super(DeepONet, self).__init__(devices=devices)\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks used in the DeepONet model\n        self.device = self._set_device(devices=devices)\n        self.use_bias = use_bias\n\n        self.trunk_network = self.to_wrap(entity=trunk_network, device=self.device)\n        self.branch_network = self.to_wrap(entity=branch_network, device=self.device)\n\n        self.add_module(\"trunk_network\", self.trunk_network)\n        self.add_module(\"branch_network\", self.branch_network)\n\n        if decoder_network is not None:\n            self.decoder_network = self.to_wrap(entity=decoder_network, device=self.device)\n            self.add_module(\"decoder_network\", self.decoder_network)\n        else:\n            self.decoder_network = decoder_network\n\n        self.product_type = product_type\n        self.model_id = model_id\n        self.var_dim = var_dim\n\n        # Rescaling factors for the output\n        if rescale_factors is not None:\n            assert (\n                len(rescale_factors) == var_dim\n            ), \"The number of rescaling factors must be equal to var_dim.\"\n            rescale_factors = torch.from_numpy(rescale_factors.astype(\"float32\"))\n            self.rescale_factors = self.to_wrap(entity=rescale_factors, device=self.device)\n        else:\n            self.rescale_factors = None\n\n        # Checking up whether the output of each subnetwork are in correct shape\n        assert self._latent_dimension_is_correct(self.trunk_network.output_size), (\n            \"The trunk network must have\"\n            \" one-dimensional output , \"\n            \"but received\"\n            f\"{self.trunk_network.output_size}\"\n        )\n\n        assert self._latent_dimension_is_correct(self.branch_network.output_size), (\n            \"The branch network must have\"\n            \" one-dimensional output,\"\n            \" but received\"\n            f\"{self.branch_network.output_size}\"\n        )\n\n        # If bias is being used, check whether the network outputs are compatible.\n        if self.use_bias:\n            print(\"Bias is being used.\")\n            self._bias_compatibility_is_correct(dim_trunk=self.trunk_network.output_size,\n                                                dim_branch=self.branch_network.output_size)\n            self.bias_wrapper = self._wrapper_bias_active\n        else:\n            self.bias_wrapper = self._wrapper_bias_inactive\n\n        # Using a decoder on top of the model or not\n        if self.decoder_network is not None:\n            self.decoder_wrapper = self._wrapper_decoder_active\n        else:\n            self.decoder_wrapper = self._wrapper_decoder_inactive\n\n        # Using rescaling factors or not\n        if rescale_factors is not None:\n            self.rescale_wrapper = self._wrapper_rescale_active\n        else:\n            self.rescale_wrapper = self._wrapper_rescale_inactive\n\n        # Checking the compatibility of the subnetworks outputs for each kind of product being employed.\n        if self.product_type != \"dense\":\n            output_branch = self.branch_network.output_size\n            output_trunk = self.trunk_network.output_size\n\n            # It checks if the inner product operation can be performed.\n            if not self.use_bias:\n                assert output_branch == output_trunk, (\n                    f\"The output dimensions for the sub-networks\"\n                    f\" trunk and branch must be equal but are\"\n                    f\" {output_branch}\"\n                    f\" and {output_trunk}\"\n                )\n            else:\n                print(\"Bias compatibility was already verified.\")\n        else:\n            output_branch = self.branch_network.output_size\n\n            assert not output_branch % self.var_dim, (\n                f\"The number of branch latent outputs must\"\n                f\" be divisible by the number of variables,\"\n                f\" but received {output_branch}\"\n                f\" and {self.var_dim}\"\n            )\n\n        self.subnetworks = [\n            net\n            for net in [self.trunk_network, self.branch_network, self.decoder_network]\n            if net is not None\n        ]\n\n        self.input_trunk = None\n        self.input_branch = None\n\n        self.output = None\n        self.var_map = dict()\n\n        #TODO Checking up if the input of the decoder network has the correct dimension\n        if self.decoder_network is not None:\n            print(\"Decoder is being used.\")\n        else:\n            pass\n\n        # Selecting the correct forward approach to be used\n        self._forward = self._forward_selector_()\n\n        self.subnetworks_names = [\"trunk\", \"branch\"]\n\n    def _latent_dimension_is_correct(self, dim: Union[int, tuple]) -&gt; bool:\n        \"\"\"It checks if the latent dimension is consistent.\n\n        Args:\n            dim (Union[int, tuple]): Latent_space_dimension.\n\n        Returns:\n            bool: The confirmation about the dimensionality correctness.\n\n        \"\"\"\n\n        if type(dim) == int:\n            return True\n        elif type(dim) == tuple:\n            if len(tuple) == 1:\n                return True\n            else:\n                return False\n\n    def _bias_compatibility_is_correct(self, dim_trunk: Union[int, tuple],\n                                       dim_branch: Union[int, tuple]) -&gt; bool:\n\n        assert dim_branch == dim_trunk + self.var_dim, (\"When using bias, the dimension\"+\n                                                        \"of the branch output should be\" +\n                                                        \"trunk output + var_dim.\")\n\n    def _forward_dense(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward method used when the embeddings are multiplied using a matrix-like product, it means, the trunk\n        network outputs serve as \"interpolation basis\" for the branch outputs.\n\n        Args:\n            output_trunk (torch.Tensor, optional): The embedding generated by the trunk network. (Default value = None)\n            output_branch (torch.Tensor, optional): The embedding generated by the branch network. (Default value = None)\n\n        Returns:\n            torch.Tensor: The product between the two embeddings.\n\n        \"\"\"\n\n        latent_dim = int(output_branch.shape[-1] / self.var_dim)\n        output_branch_reshaped = torch.reshape(\n            output_branch, (-1, self.var_dim, latent_dim)\n        )\n\n        output = torch.matmul(output_branch_reshaped, output_trunk[..., None])\n        output = torch.squeeze(output)\n\n        return output\n\n    def _forward_pointwise(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward method used when the embeddings are multiplied using a simple point-wise product, after that a\n        reshaping is applied in order to produce multiple outputs.\n\n        Args:\n            output_trunk (torch.Tensor, optional): The embedding generated by the trunk network. (Default value = None)\n            output_branch (torch.Tensor, optional): The embedding generated by the branch network. (Default value = None)\n\n        Returns:\n            torch.Tensor: The product between the two embeddings.\n\n        \"\"\"\n\n        latent_dim = int(output_trunk.shape[-1] / self.var_dim)\n        output_trunk_reshaped = torch.reshape(\n            output_trunk, (-1, latent_dim, self.var_dim)\n        )\n        output_branch_reshaped = torch.reshape(\n            output_branch, (-1, latent_dim, self.var_dim)\n        )\n        output = torch.sum(\n            output_trunk_reshaped * output_branch_reshaped, dim=-2, keepdim=False\n        )\n\n        return output\n\n    def _forward_vanilla(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward method used when the embeddings are multiplied using a simple point-wise product.\n\n        Args:\n            output_trunk (torch.Tensor, optional): The embedding generated by the trunk network. (Default value = None)\n            output_branch (torch.Tensor, optional): The embedding generated by the branch network. (Default value = None)\n\n        Returns:\n            torch.Tensor: The product between the two embeddings.\n\n        \"\"\"\n\n        output = torch.sum(output_trunk * output_branch, dim=-1, keepdim=True)\n\n        return output\n\n    def _forward_selector_(self) -&gt; callable:\n        \"\"\"It selects the forward method to be used.\n\n\n        Returns:\n            callable : The callable corresponding to the required forward method.\n\n        \"\"\"\n\n        if self.var_dim &gt; 1:\n\n            # It operates as a typical dense layer\n            if self.product_type == \"dense\":\n                return self._forward_dense\n            # It executes an inner product by parts between the outputs\n            # of the subnetworks branch and trunk\n            else:\n                return self._forward_pointwise\n        else:\n            return self._forward_vanilla\n\n    @property\n    def _var_map(self) -&gt; dict:\n        # It checks all the data arrays in self.var_map have the same\n        # batches dimension\n        batches_dimensions = set([value.shape[0] for value in self.var_map.values()])\n\n        assert (\n            len(batches_dimensions) == 1\n        ), \"This dataset is not proper to apply shuffling\"\n\n        dim = list(batches_dimensions)[0]\n\n        indices = np.arange(dim)\n\n        np.random.shuffle(indices)\n\n        var_map_shuffled = {key: value[indices] for key, value in self.var_map.items()}\n\n        return var_map_shuffled\n\n    @property\n    def weights(self) -&gt; list:\n        return sum([net.weights for net in self.subnetworks], [])\n\n    # Now, a sequence of wrappers\n    def _wrapper_bias_inactive(\n        self,\n        output_trunk: Union[np.ndarray, torch.Tensor] = None,\n        output_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        output = self._forward(output_trunk=output_trunk, output_branch=output_branch)\n\n        return output\n\n    def _wrapper_bias_active(\n        self,\n        output_trunk: Union[np.ndarray, torch.Tensor] = None,\n        output_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        output_branch_ = output_branch[:, :-self.var_dim]\n        bias = output_branch[:, -self.var_dim:]\n\n        output = self._forward(output_trunk=output_trunk, output_branch=output_branch_) + bias\n\n        return output\n\n    def _wrapper_decoder_active(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return self.decoder_network.forward(input_data=input_data)\n\n    def _wrapper_decoder_inactive(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data\n\n    def _wrapper_rescale_active(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data * self.rescale_factors\n\n    def _wrapper_rescale_inactive(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data\n\n    def forward(\n        self,\n        input_trunk: Union[np.ndarray, torch.Tensor] = None,\n        input_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Wrapper forward method.\n\n        Args:\n            input_trunk (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n            input_branch (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n\n        Returns:\n            torch.Tensor: The result of all the hidden operations in the network.\n\n        \"\"\"\n\n        # Forward method execution\n        output_trunk = self.to_wrap(entity=self.trunk_network.forward(input_trunk),\n                                    device=self.device)\n\n        output_branch = self.to_wrap(entity=self.branch_network.forward(input_branch),\n                                     device=self.device)\n\n        # Wrappers are applied to execute user-defined operations.\n        # When those operations are not selected, these wrappers simply\n        # bypass the inputs. \n        output = self.bias_wrapper(output_trunk=output_trunk, output_branch=output_branch)\n\n        return self.rescale_wrapper(input_data=self.decoder_wrapper(input_data=output))\n\n    @guarantee_device\n    def eval(\n        self,\n        trunk_data: Union[np.ndarray, torch.Tensor] = None,\n        branch_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"It uses the network to make evaluations.\n\n        Args:\n            trunk_data (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n            branch_data (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n\n        Returns:\n            np.ndarray: The result of all the hidden operations in the network.\n\n        \"\"\"\n\n        output_tensor = self.forward(input_trunk=trunk_data, input_branch=branch_data)\n\n        return output_tensor.cpu().detach().numpy()\n\n    @guarantee_device\n    def eval_subnetwork(\n        self, name: str = None, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"It evaluates the output of DeepONet subnetworks.\n\n        Args:\n            name (str, optional): Name of the subnetwork. (Default value = None)\n            input_data (Union[np.ndarray, torch.Tensor], optional): The data used as input for the subnetwork. (Default value = None)\n\n        Returns:\n            np.ndarray: The evaluation performed by the subnetwork.\n\n        \"\"\"\n\n        assert (\n            name in self.subnetworks_names\n        ), f\"The name {name} is not a subnetwork of {self}.\"\n\n        network_to_be_used = getattr(self, name + \"_network\")\n\n        return network_to_be_used.forward(input_data).cpu().detach().numpy()\n\n    def summary(self) -&gt; None:\n        print(\"Trunk Network:\")\n        self.trunk_network.summary()\n        print(\"Branch Network:\")\n        self.branch_network.summary()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.__init__","title":"<code>__init__(trunk_network=None, branch_network=None, decoder_network=None, var_dim=1, devices='cpu', product_type=None, rescale_factors=None, model_id=None, use_bias=False)</code>","text":"<p>Classical Deep Operator Network (DeepONet), a deep learning version of the Universal Approximation Theorem.</p> <p>Parameters:</p> Name Type Description Default <code>trunk_network</code> <code>NetworkTemplate</code> <p>Subnetwork for processing the coordinates inputs. (Default value = None)</p> <code>None</code> <code>branch_network</code> <code>NetworkTemplate</code> <p>Subnetwork for processing the forcing/conditioning inputs. (Default value = None)</p> <code>None</code> <code>decoder_network</code> <code>NetworkTemplate</code> <p>Subnetworks for converting the embedding to the output (optional). (Default value = None)</p> <code>None</code> <code>#</code> <code>Union[str, list]</code> <p>(Default value = \"cpu\")</p> required <code>product_type</code> <code>str</code> <p>Type of product to execute in the embedding space. (Default value = None)</p> <code>None</code> <code>rescale_factors</code> <code>ndarray</code> <p>Values used for rescaling the network outputs for a given order of magnitude. (Default value = None)</p> <code>None</code> <code>model_id</code> <code>str</code> <p>Name for the model (Default value = None)</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>(Default value = False)</p> <code>False</code> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>def __init__(\n    self,\n    trunk_network: NetworkTemplate = None,\n    branch_network: NetworkTemplate = None,\n    decoder_network: NetworkTemplate = None,  # The decoder network is optional and considered\n    var_dim: int = 1,  # less effective than the output reshaping alternative\n    devices: Union[str, list] = \"cpu\",\n    product_type: str = None,\n    rescale_factors: np.ndarray = None,\n    model_id:str=None,\n    use_bias:bool=False,\n) -&gt; None:\n    \"\"\"Classical Deep Operator Network (DeepONet), a deep learning version\n    of the Universal Approximation Theorem.\n\n    Args:\n        trunk_network (NetworkTemplate, optional): Subnetwork for processing the coordinates inputs. (Default value = None)\n        branch_network (NetworkTemplate, optional): Subnetwork for processing the forcing/conditioning inputs. (Default value = None)\n        decoder_network (NetworkTemplate, optional): Subnetworks for converting the embedding to the output (optional). (Default value = None)\n        # (Union[str, list], optional):  (Default value = \"cpu\")\n        product_type (str, optional): Type of product to execute in the embedding space. (Default value = None)\n        rescale_factors (np.ndarray, optional): Values used for rescaling the network outputs for a given order of magnitude. (Default value = None)\n        model_id (str, optional): Name for the model (Default value = None)\n        use_bias (bool, optional):  (Default value = False)\n\n    \"\"\"\n\n    super(DeepONet, self).__init__(devices=devices)\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks used in the DeepONet model\n    self.device = self._set_device(devices=devices)\n    self.use_bias = use_bias\n\n    self.trunk_network = self.to_wrap(entity=trunk_network, device=self.device)\n    self.branch_network = self.to_wrap(entity=branch_network, device=self.device)\n\n    self.add_module(\"trunk_network\", self.trunk_network)\n    self.add_module(\"branch_network\", self.branch_network)\n\n    if decoder_network is not None:\n        self.decoder_network = self.to_wrap(entity=decoder_network, device=self.device)\n        self.add_module(\"decoder_network\", self.decoder_network)\n    else:\n        self.decoder_network = decoder_network\n\n    self.product_type = product_type\n    self.model_id = model_id\n    self.var_dim = var_dim\n\n    # Rescaling factors for the output\n    if rescale_factors is not None:\n        assert (\n            len(rescale_factors) == var_dim\n        ), \"The number of rescaling factors must be equal to var_dim.\"\n        rescale_factors = torch.from_numpy(rescale_factors.astype(\"float32\"))\n        self.rescale_factors = self.to_wrap(entity=rescale_factors, device=self.device)\n    else:\n        self.rescale_factors = None\n\n    # Checking up whether the output of each subnetwork are in correct shape\n    assert self._latent_dimension_is_correct(self.trunk_network.output_size), (\n        \"The trunk network must have\"\n        \" one-dimensional output , \"\n        \"but received\"\n        f\"{self.trunk_network.output_size}\"\n    )\n\n    assert self._latent_dimension_is_correct(self.branch_network.output_size), (\n        \"The branch network must have\"\n        \" one-dimensional output,\"\n        \" but received\"\n        f\"{self.branch_network.output_size}\"\n    )\n\n    # If bias is being used, check whether the network outputs are compatible.\n    if self.use_bias:\n        print(\"Bias is being used.\")\n        self._bias_compatibility_is_correct(dim_trunk=self.trunk_network.output_size,\n                                            dim_branch=self.branch_network.output_size)\n        self.bias_wrapper = self._wrapper_bias_active\n    else:\n        self.bias_wrapper = self._wrapper_bias_inactive\n\n    # Using a decoder on top of the model or not\n    if self.decoder_network is not None:\n        self.decoder_wrapper = self._wrapper_decoder_active\n    else:\n        self.decoder_wrapper = self._wrapper_decoder_inactive\n\n    # Using rescaling factors or not\n    if rescale_factors is not None:\n        self.rescale_wrapper = self._wrapper_rescale_active\n    else:\n        self.rescale_wrapper = self._wrapper_rescale_inactive\n\n    # Checking the compatibility of the subnetworks outputs for each kind of product being employed.\n    if self.product_type != \"dense\":\n        output_branch = self.branch_network.output_size\n        output_trunk = self.trunk_network.output_size\n\n        # It checks if the inner product operation can be performed.\n        if not self.use_bias:\n            assert output_branch == output_trunk, (\n                f\"The output dimensions for the sub-networks\"\n                f\" trunk and branch must be equal but are\"\n                f\" {output_branch}\"\n                f\" and {output_trunk}\"\n            )\n        else:\n            print(\"Bias compatibility was already verified.\")\n    else:\n        output_branch = self.branch_network.output_size\n\n        assert not output_branch % self.var_dim, (\n            f\"The number of branch latent outputs must\"\n            f\" be divisible by the number of variables,\"\n            f\" but received {output_branch}\"\n            f\" and {self.var_dim}\"\n        )\n\n    self.subnetworks = [\n        net\n        for net in [self.trunk_network, self.branch_network, self.decoder_network]\n        if net is not None\n    ]\n\n    self.input_trunk = None\n    self.input_branch = None\n\n    self.output = None\n    self.var_map = dict()\n\n    #TODO Checking up if the input of the decoder network has the correct dimension\n    if self.decoder_network is not None:\n        print(\"Decoder is being used.\")\n    else:\n        pass\n\n    # Selecting the correct forward approach to be used\n    self._forward = self._forward_selector_()\n\n    self.subnetworks_names = [\"trunk\", \"branch\"]\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.eval","title":"<code>eval(trunk_data=None, branch_data=None)</code>","text":"<p>It uses the network to make evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>trunk_data</code> <code>Union[ndarray, Tensor]</code> <p>(Default value = None)</p> <code>None</code> <code>branch_data</code> <code>Union[ndarray, Tensor]</code> <p>(Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The result of all the hidden operations in the network.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>@guarantee_device\ndef eval(\n    self,\n    trunk_data: Union[np.ndarray, torch.Tensor] = None,\n    branch_data: Union[np.ndarray, torch.Tensor] = None,\n) -&gt; np.ndarray:\n    \"\"\"It uses the network to make evaluations.\n\n    Args:\n        trunk_data (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n        branch_data (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n\n    Returns:\n        np.ndarray: The result of all the hidden operations in the network.\n\n    \"\"\"\n\n    output_tensor = self.forward(input_trunk=trunk_data, input_branch=branch_data)\n\n    return output_tensor.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.eval_subnetwork","title":"<code>eval_subnetwork(name=None, input_data=None)</code>","text":"<p>It evaluates the output of DeepONet subnetworks.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the subnetwork. (Default value = None)</p> <code>None</code> <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The data used as input for the subnetwork. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The evaluation performed by the subnetwork.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>@guarantee_device\ndef eval_subnetwork(\n    self, name: str = None, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"It evaluates the output of DeepONet subnetworks.\n\n    Args:\n        name (str, optional): Name of the subnetwork. (Default value = None)\n        input_data (Union[np.ndarray, torch.Tensor], optional): The data used as input for the subnetwork. (Default value = None)\n\n    Returns:\n        np.ndarray: The evaluation performed by the subnetwork.\n\n    \"\"\"\n\n    assert (\n        name in self.subnetworks_names\n    ), f\"The name {name} is not a subnetwork of {self}.\"\n\n    network_to_be_used = getattr(self, name + \"_network\")\n\n    return network_to_be_used.forward(input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.forward","title":"<code>forward(input_trunk=None, input_branch=None)</code>","text":"<p>Wrapper forward method.</p> <p>Parameters:</p> Name Type Description Default <code>input_trunk</code> <code>Union[ndarray, Tensor]</code> <p>(Default value = None)</p> <code>None</code> <code>input_branch</code> <code>Union[ndarray, Tensor]</code> <p>(Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The result of all the hidden operations in the network.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>def forward(\n    self,\n    input_trunk: Union[np.ndarray, torch.Tensor] = None,\n    input_branch: Union[np.ndarray, torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Wrapper forward method.\n\n    Args:\n        input_trunk (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n        input_branch (Union[np.ndarray, torch.Tensor], optional):  (Default value = None)\n\n    Returns:\n        torch.Tensor: The result of all the hidden operations in the network.\n\n    \"\"\"\n\n    # Forward method execution\n    output_trunk = self.to_wrap(entity=self.trunk_network.forward(input_trunk),\n                                device=self.device)\n\n    output_branch = self.to_wrap(entity=self.branch_network.forward(input_branch),\n                                 device=self.device)\n\n    # Wrappers are applied to execute user-defined operations.\n    # When those operations are not selected, these wrappers simply\n    # bypass the inputs. \n    output = self.bias_wrapper(output_trunk=output_trunk, output_branch=output_branch)\n\n    return self.rescale_wrapper(input_data=self.decoder_wrapper(input_data=output))\n</code></pre>"},{"location":"simulai_models/#autoencodermlp","title":"AutoencoderMLP","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Fully-connected AutoEncoder as Reduced Order Model;</p> <p>A MLP autoencoder architecture consists of two stages: --&gt; Fully-connected encoder --&gt; Fully connected decoder</p> <p>SCHEME         |         |         |  |   |  | Z -&gt;    |  | | |  |  -&gt; Z_til         |  |   |  |         |         |</p> <p>ENCODER       DECODER</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderMLP(NetworkTemplate):\n    r\"\"\"This is an implementation of a Fully-connected AutoEncoder as\n    Reduced Order Model;\n\n    A MLP autoencoder architecture consists of two stages:\n    --&gt; Fully-connected encoder\n    --&gt; Fully connected decoder\n\n    SCHEME\n            |         |\n            |  |   |  |\n    Z -&gt;    |  | | |  |  -&gt; Z_til\n            |  |   |  |\n            |         |\n\n    ENCODER       DECODER\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: DenseNetwork = None,\n        decoder: DenseNetwork = None,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        shallow: Optional[bool] = False,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n    ) -&gt; None:\n        \"\"\"Initialize the AutoencoderMLP network\n\n        Args:\n            encoder (DenseNetwork, optional): The encoder network architecture. (Default value = None)\n            decoder (DenseNetwork, optional): The decoder network architecture. (Default value = None)\n            input_dim (Optional[int], optional): The input dimensions of the data, by default None.\n            output_dim (Optional[int], optional): The output dimensions of the data, by default None.\n            latent_dim (Optional[int], optional): The dimensions of the latent space, by default None.\n            activation (Optional[Union[list, str]], optional): The activation functions used by the network, by default None.\n            shallow (Optional[bool], optional): Whether the network should be shallow or not, by default False.\n            devices (Union[str, list], optional): The device(s) to be used for allocating subnetworks, by default \"cpu\".\n            name (str, optional): The name of the network, by default None.\n\n        \"\"\"\n\n        super(AutoencoderMLP, self).__init__(name=name)\n\n        self.weights = list()\n\n        # This option is used when no network is provided\n        # and it uses default choices for the architectures\n        if encoder == None and decoder == None:\n            encoder, decoder = mlp_autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                shallow=shallow,\n            )\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks used in the DeepONet model\n        self.device = self._set_device(devices=devices)\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        self.last_encoder_channels = None\n\n        self.shapes_dict = dict()\n\n    def summary(self) -&gt; None:\n        \"\"\"Prints the summary of the network architecture\n\n        \"\"\"\n        self.encoder.summary()\n        self.decoder.summary()\n\n    def projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Project the input dataset into the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n        Returns:\n            torch.Tensor: The dataset projected over the latent space.\n\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    def reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstruct the latent dataset to the original one.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The dataset to be reconstructed, by default None.\n\n        Returns:\n            torch.Tensor: The dataset reconstructed.\n\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute the complete projection/reconstruction pipeline.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input dataset, by default None.\n\n        Returns:\n            torch.Tensor: The dataset reconstructed.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def eval_projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Evaluate the projection of the input dataset into the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n        Returns:\n            np.ndarray: The dataset projected over the latent space.\n\n        \"\"\"\n        return self.projection(input_data=input_data).detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.__init__","title":"<code>__init__(encoder=None, decoder=None, input_dim=None, output_dim=None, latent_dim=None, activation=None, shallow=False, devices='cpu', name=None)</code>","text":"<p>Initialize the AutoencoderMLP network</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>DenseNetwork</code> <p>The encoder network architecture. (Default value = None)</p> <code>None</code> <code>decoder</code> <code>DenseNetwork</code> <p>The decoder network architecture. (Default value = None)</p> <code>None</code> <code>input_dim</code> <code>Optional[int]</code> <p>The input dimensions of the data, by default None.</p> <code>None</code> <code>output_dim</code> <code>Optional[int]</code> <p>The output dimensions of the data, by default None.</p> <code>None</code> <code>latent_dim</code> <code>Optional[int]</code> <p>The dimensions of the latent space, by default None.</p> <code>None</code> <code>activation</code> <code>Optional[Union[list, str]]</code> <p>The activation functions used by the network, by default None.</p> <code>None</code> <code>shallow</code> <code>Optional[bool]</code> <p>Whether the network should be shallow or not, by default False.</p> <code>False</code> <code>devices</code> <code>Union[str, list]</code> <p>The device(s) to be used for allocating subnetworks, by default \"cpu\".</p> <code>'cpu'</code> <code>name</code> <code>str</code> <p>The name of the network, by default None.</p> <code>None</code> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: DenseNetwork = None,\n    decoder: DenseNetwork = None,\n    input_dim: Optional[int] = None,\n    output_dim: Optional[int] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    shallow: Optional[bool] = False,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n) -&gt; None:\n    \"\"\"Initialize the AutoencoderMLP network\n\n    Args:\n        encoder (DenseNetwork, optional): The encoder network architecture. (Default value = None)\n        decoder (DenseNetwork, optional): The decoder network architecture. (Default value = None)\n        input_dim (Optional[int], optional): The input dimensions of the data, by default None.\n        output_dim (Optional[int], optional): The output dimensions of the data, by default None.\n        latent_dim (Optional[int], optional): The dimensions of the latent space, by default None.\n        activation (Optional[Union[list, str]], optional): The activation functions used by the network, by default None.\n        shallow (Optional[bool], optional): Whether the network should be shallow or not, by default False.\n        devices (Union[str, list], optional): The device(s) to be used for allocating subnetworks, by default \"cpu\".\n        name (str, optional): The name of the network, by default None.\n\n    \"\"\"\n\n    super(AutoencoderMLP, self).__init__(name=name)\n\n    self.weights = list()\n\n    # This option is used when no network is provided\n    # and it uses default choices for the architectures\n    if encoder == None and decoder == None:\n        encoder, decoder = mlp_autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            shallow=shallow,\n        )\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks used in the DeepONet model\n    self.device = self._set_device(devices=devices)\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    self.last_encoder_channels = None\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.eval_projection","title":"<code>eval_projection(input_data=None)</code>","text":"<p>Evaluate the projection of the input dataset into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be projected, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval_projection(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"Evaluate the projection of the input dataset into the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n    Returns:\n        np.ndarray: The dataset projected over the latent space.\n\n    \"\"\"\n    return self.projection(input_data=input_data).detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Execute the complete projection/reconstruction pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input dataset, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The dataset reconstructed.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute the complete projection/reconstruction pipeline.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input dataset, by default None.\n\n    Returns:\n        torch.Tensor: The dataset reconstructed.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.projection","title":"<code>projection(input_data=None)</code>","text":"<p>Project the input dataset into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be projected, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def projection(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Project the input dataset into the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n    Returns:\n        torch.Tensor: The dataset projected over the latent space.\n\n    \"\"\"\n    latent = self.encoder.forward(input_data=input_data)\n\n    return latent\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.reconstruction","title":"<code>reconstruction(input_data=None)</code>","text":"<p>Reconstruct the latent dataset to the original one.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>The dataset to be reconstructed, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The dataset reconstructed.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n    \"\"\"Reconstruct the latent dataset to the original one.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): The dataset to be reconstructed, by default None.\n\n    Returns:\n        torch.Tensor: The dataset reconstructed.\n\n    \"\"\"\n    reconstructed = self.decoder.forward(input_data=input_data)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.summary","title":"<code>summary()</code>","text":"<p>Prints the summary of the network architecture</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Prints the summary of the network architecture\n\n    \"\"\"\n    self.encoder.summary()\n    self.decoder.summary()\n</code></pre>"},{"location":"simulai_models/#autoencodercnn","title":"AutoencoderCNN","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a convolutional autoencoder as Reduced Order Model. An autoencoder architecture consists of three stages:</p> <ul> <li>The convolutional encoder</li> </ul> <p>The bottleneck stage, subdivided in:     * Fully-connected encoder     * Fully connected decoder     * The convolutional decoder </p> <p>SCHEME</p> <p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</p> <pre><code>            ENCODER               DENSE BOTTLENECK           DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderCNN(NetworkTemplate):\n    \"\"\"This is an implementation of a convolutional autoencoder as Reduced Order Model.\n    An autoencoder architecture consists of three stages:\n\n    * The convolutional encoder\n\n    The bottleneck stage, subdivided in:\n        * Fully-connected encoder\n        * Fully connected decoder\n        * The convolutional decoder \n\n    SCHEME\n\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n\n\n                    ENCODER               DENSE BOTTLENECK           DECODER\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: ConvolutionalNetwork = None,\n        bottleneck_encoder: Linear = None,\n        bottleneck_decoder: Linear = None,\n        decoder: ConvolutionalNetwork = None,\n        encoder_activation: str = \"relu\",\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        kernel_size: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        case: Optional[str] = None,\n        shallow: Optional[bool] = False,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the AutoencoderCNN network.\n\n        Args:\n            encoder (ConvolutionalNetwork, optional): The encoder network architecture, by default None.\n            bottleneck_encoder (Linear, optional): The bottleneck encoder network architecture, by default None.\n            bottleneck_decoder (Linear, optional): The bottleneck decoder network architecture, by default None.\n            decoder (ConvolutionalNetwork, optional): The decoder network architecture, by default None.\n            encoder_activation (str, optional): The activation function used by the encoder network, by default 'relu'.\n            input_dim (Optional[Tuple[int, ...]], optional): The input dimensions of the data, by default None.\n            output_dim (Optional[Tuple[int, ...]], optional): The output dimensions of the data, by default None.\n            latent_dim (Optional[int], optional): The dimensions of the latent space, by default None.\n            kernel_size (Optional[int], optional):  (Default value = None)\n            activation (Optional[Union[list, str]], optional): The activation functions used by the network, by default None.\n            channels (Optional[int], optional): The number of channels of the convolutional layers, by default None.\n            case (Optional[str], optional): The type of convolutional encoder and decoder to be used, by default None.\n            shallow (Optional[bool], optional): Whether the network should be shallow or not, by default False.\n            devices (Union[str, list], optional): The device(s) to be used for allocating subnetworks, by default 'cpu'.\n            name (str, optional): The name of the network, by default None.\n            **kwargs \n\n        \"\"\"\n\n        super(AutoencoderCNN, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            (\n                encoder,\n                decoder,\n                bottleneck_encoder,\n                bottleneck_decoder,\n            ) = cnn_autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                kernel_size=kernel_size,\n                channels=channels,\n                case=case,\n                shallow=shallow,\n            )\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n        self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n        self.weights += self.decoder.weights\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Prints the summary of the network architecture.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input dataset. (Default value = None)\n            input_shape (list, optional): The shape of the input data. (Default value = None)\n            verbose (bool, optional):  (Default value = True)\n\n        Returns:\n            torch.Tensor: The dataset projected over the latent space.\n\n        \"\"\"\n\n        if verbose == True:\n            if self.input_dim != None:\n                input_shape = self.input_dim\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device\n            )\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n                input_shape = self.encoder.input_size\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary()\n            self.bottleneck_decoder.summary()\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def projection(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"Project input dataset into the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor]): The dataset to be projected.\n\n        Returns:\n            torch.Tensor: The dataset projected over the latent space.\n\n        \"\"\"\n\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray]\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstruct the latent dataset to the original one.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray]): The dataset to be reconstructed.\n\n        Returns:\n            torch.Tensor: The reconstructed dataset.\n\n        \"\"\"\n\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=input_data)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    def forward(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"Execute the complete projection/reconstruction pipeline.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor]): The input dataset.\n\n        Returns:\n            torch.Tensor: The reconstructed dataset.\n\n        \"\"\"\n\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"Evaluate the autoencoder on the given dataset.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be evaluated, by default None.\n\n        Returns:\n            np.ndarray: The dataset projected over the latent space.\n\n        \"\"\"\n\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        return super().eval(input_data=input_data)\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"Project the input dataset into the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n        Returns:\n            np.ndarray: The dataset projected over the latent space.\n\n        \"\"\"\n\n        projected_data = self.projection(input_data=input_data)\n\n        return projected_data.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Reconstructs the latent dataset to the original one.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be reconstructed. If not provided, uses the original input data, by default None.\n\n        Returns:\n            np.ndarray: The reconstructed dataset.\n\n        \"\"\"\n        reconstructed_data = self.reconstruction(input_data=input_data)\n        return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, encoder_activation='relu', input_dim=None, output_dim=None, latent_dim=None, kernel_size=None, activation=None, channels=None, case=None, shallow=False, devices='cpu', name=None, **kwargs)</code>","text":"<p>Initialize the AutoencoderCNN network.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>ConvolutionalNetwork</code> <p>The encoder network architecture, by default None.</p> <code>None</code> <code>bottleneck_encoder</code> <code>Linear</code> <p>The bottleneck encoder network architecture, by default None.</p> <code>None</code> <code>bottleneck_decoder</code> <code>Linear</code> <p>The bottleneck decoder network architecture, by default None.</p> <code>None</code> <code>decoder</code> <code>ConvolutionalNetwork</code> <p>The decoder network architecture, by default None.</p> <code>None</code> <code>encoder_activation</code> <code>str</code> <p>The activation function used by the encoder network, by default 'relu'.</p> <code>'relu'</code> <code>input_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The input dimensions of the data, by default None.</p> <code>None</code> <code>output_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The output dimensions of the data, by default None.</p> <code>None</code> <code>latent_dim</code> <code>Optional[int]</code> <p>The dimensions of the latent space, by default None.</p> <code>None</code> <code>kernel_size</code> <code>Optional[int]</code> <p>(Default value = None)</p> <code>None</code> <code>activation</code> <code>Optional[Union[list, str]]</code> <p>The activation functions used by the network, by default None.</p> <code>None</code> <code>channels</code> <code>Optional[int]</code> <p>The number of channels of the convolutional layers, by default None.</p> <code>None</code> <code>case</code> <code>Optional[str]</code> <p>The type of convolutional encoder and decoder to be used, by default None.</p> <code>None</code> <code>shallow</code> <code>Optional[bool]</code> <p>Whether the network should be shallow or not, by default False.</p> <code>False</code> <code>devices</code> <code>Union[str, list]</code> <p>The device(s) to be used for allocating subnetworks, by default 'cpu'.</p> <code>'cpu'</code> <code>name</code> <code>str</code> <p>The name of the network, by default None.</p> <code>None</code> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: ConvolutionalNetwork = None,\n    bottleneck_encoder: Linear = None,\n    bottleneck_decoder: Linear = None,\n    decoder: ConvolutionalNetwork = None,\n    encoder_activation: str = \"relu\",\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    kernel_size: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    case: Optional[str] = None,\n    shallow: Optional[bool] = False,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the AutoencoderCNN network.\n\n    Args:\n        encoder (ConvolutionalNetwork, optional): The encoder network architecture, by default None.\n        bottleneck_encoder (Linear, optional): The bottleneck encoder network architecture, by default None.\n        bottleneck_decoder (Linear, optional): The bottleneck decoder network architecture, by default None.\n        decoder (ConvolutionalNetwork, optional): The decoder network architecture, by default None.\n        encoder_activation (str, optional): The activation function used by the encoder network, by default 'relu'.\n        input_dim (Optional[Tuple[int, ...]], optional): The input dimensions of the data, by default None.\n        output_dim (Optional[Tuple[int, ...]], optional): The output dimensions of the data, by default None.\n        latent_dim (Optional[int], optional): The dimensions of the latent space, by default None.\n        kernel_size (Optional[int], optional):  (Default value = None)\n        activation (Optional[Union[list, str]], optional): The activation functions used by the network, by default None.\n        channels (Optional[int], optional): The number of channels of the convolutional layers, by default None.\n        case (Optional[str], optional): The type of convolutional encoder and decoder to be used, by default None.\n        shallow (Optional[bool], optional): Whether the network should be shallow or not, by default False.\n        devices (Union[str, list], optional): The device(s) to be used for allocating subnetworks, by default 'cpu'.\n        name (str, optional): The name of the network, by default None.\n        **kwargs \n\n    \"\"\"\n\n    super(AutoencoderCNN, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        (\n            encoder,\n            decoder,\n            bottleneck_encoder,\n            bottleneck_decoder,\n        ) = cnn_autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            kernel_size=kernel_size,\n            channels=channels,\n            case=case,\n            shallow=shallow,\n        )\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n    self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n    self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n    self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.bottleneck_encoder.weights\n    self.weights += self.bottleneck_decoder.weights\n    self.weights += self.decoder.weights\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.eval","title":"<code>eval(input_data=None)</code>","text":"<p>Evaluate the autoencoder on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be evaluated, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the autoencoder on the given dataset.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be evaluated, by default None.\n\n    Returns:\n        np.ndarray: The dataset projected over the latent space.\n\n    \"\"\"\n\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    return super().eval(input_data=input_data)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.forward","title":"<code>forward(input_data)</code>","text":"<p>Execute the complete projection/reconstruction pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def forward(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Execute the complete projection/reconstruction pipeline.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor]): The input dataset.\n\n    Returns:\n        torch.Tensor: The reconstructed dataset.\n\n    \"\"\"\n\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.project","title":"<code>project(input_data=None)</code>","text":"<p>Project the input dataset into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be projected, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"Project the input dataset into the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be projected, by default None.\n\n    Returns:\n        np.ndarray: The dataset projected over the latent space.\n\n    \"\"\"\n\n    projected_data = self.projection(input_data=input_data)\n\n    return projected_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.projection","title":"<code>projection(input_data)</code>","text":"<p>Project input dataset into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be projected.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>@as_tensor\ndef projection(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Project input dataset into the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor]): The dataset to be projected.\n\n    Returns:\n        torch.Tensor: The dataset projected over the latent space.\n\n    \"\"\"\n\n    btnk_input = self.encoder.forward(input_data=input_data)\n\n    self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n    btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n    latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n    return latent\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the latent dataset to the original one.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The dataset to be reconstructed. If not provided, uses the original input data, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"Reconstructs the latent dataset to the original one.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The dataset to be reconstructed. If not provided, uses the original input data, by default None.\n\n    Returns:\n        np.ndarray: The reconstructed dataset.\n\n    \"\"\"\n    reconstructed_data = self.reconstruction(input_data=input_data)\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruction","title":"<code>reconstruction(input_data)</code>","text":"<p>Reconstruct the latent dataset to the original one.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>The dataset to be reconstructed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>@as_tensor\ndef reconstruction(\n    self, input_data: Union[torch.Tensor, np.ndarray]\n) -&gt; torch.Tensor:\n    \"\"\"Reconstruct the latent dataset to the original one.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray]): The dataset to be reconstructed.\n\n    Returns:\n        torch.Tensor: The reconstructed dataset.\n\n    \"\"\"\n\n    bottleneck_output = self.encoder_activation(\n        self.bottleneck_decoder.forward(input_data=input_data)\n    )\n\n    bottleneck_output = bottleneck_output.reshape(\n        (-1,) + self.before_flatten_dimension\n    )\n\n    reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.summary","title":"<code>summary(input_data=None, input_shape=None, verbose=True)</code>","text":"<p>Prints the summary of the network architecture.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input dataset. (Default value = None)</p> <code>None</code> <code>input_shape</code> <code>list</code> <p>The shape of the input data. (Default value = None)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>(Default value = True)</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    input_shape: list = None,\n    verbose: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Prints the summary of the network architecture.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input dataset. (Default value = None)\n        input_shape (list, optional): The shape of the input data. (Default value = None)\n        verbose (bool, optional):  (Default value = True)\n\n    Returns:\n        torch.Tensor: The dataset projected over the latent space.\n\n    \"\"\"\n\n    if verbose == True:\n        if self.input_dim != None:\n            input_shape = self.input_dim\n        else:\n            pass\n\n        self.encoder.summary(\n            input_data=input_data, input_shape=input_shape, device=self.device\n        )\n\n        if isinstance(input_data, np.ndarray):\n            btnk_input = self.encoder.forward(input_data=input_data)\n        else:\n            assert (\n                input_shape\n            ), \"It is necessary to have input_shape when input_data is None.\"\n            input_shape = self.encoder.input_size\n            input_shape[0] = 1\n\n            input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n            btnk_input = self.encoder.forward(input_data=input_data)\n\n        before_flatten_dimension = tuple(btnk_input.shape[1:])\n        btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        self.bottleneck_encoder.summary()\n        self.bottleneck_decoder.summary()\n\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=latent)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1, *before_flatten_dimension)\n        )\n\n        self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n        # Saving the content of the subnetworks to the overall architecture dictionary\n        self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n        self.shapes_dict.update(\n            {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n        )\n        self.shapes_dict.update(\n            {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n        )\n        self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n    else:\n        print(self)\n</code></pre>"},{"location":"simulai_models/#autoencoderkoopman","title":"AutoencoderKoopman","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Koopman autoencoder as a Reduced Order Model.</p> <p>A Koopman autoencoder architecture consists of five stages:</p> <ul> <li>The convolutional encoder [Optional]</li> <li>Fully-connected encoder</li> <li>Koopman operator</li> <li>Fully connected decoder</li> <li>The convolutional decoder [Optional]</li> </ul> <p>SCHEME                                 (Koopman OPERATOR)                                          ^                                   |      |      |                                   |  |   |   |  | Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til                                   |  |       |  |                                   |             |</p> <pre><code>            ENCODER          DENSE BOTTLENECK        DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderKoopman(NetworkTemplate):\n    r\"\"\"This is an implementation of a Koopman autoencoder as a Reduced Order Model.\n\n    A Koopman autoencoder architecture consists of five stages:\n\n    * The convolutional encoder [Optional]\n    * Fully-connected encoder\n    * Koopman operator\n    * Fully connected decoder\n    * The convolutional decoder [Optional]\n\n    SCHEME\n                                    (Koopman OPERATOR)\n                                             ^\n                                      |      |      |\n                                      |  |   |   |  |\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n                                      |  |       |  |\n                                      |             |\n\n                    ENCODER          DENSE BOTTLENECK        DECODER\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n        bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n        decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        case: Optional[str] = None,\n        architecture: Optional[str] = None,\n        shallow: Optional[bool] = False,\n        use_batch_norm: Optional[bool] = False,\n        encoder_activation: str = \"relu\",\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n    ) -&gt; None:\n        \"\"\"Constructs a new instance of the Autoencoder\n\n        Args:\n            encoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The encoder network. Defaults to None.\n            bottleneck_encoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck encoder network. Defaults to None.\n            bottleneck_decoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck decoder network. Defaults to None.\n            decoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The decoder network. Defaults to None.\n            input_dim (Optional[Tuple[int, ...]], optional): The input dimensions. Used for automatic network generation. Defaults to None.\n            output_dim (Optional[Tuple[int, ...]], optional): The output dimensions. Used for automatic network generation. Defaults to None.\n            latent_dim (Optional[int], optional): The latent dimensions. Used for automatic network generation. Defaults to None.\n            activation (Optional[Union[list, str]], optional): The activation functions for each layer. Used for automatic network generation. Defaults to None.\n            channels (Optional[int], optional): The number of channels. Used for automatic network generation. Defaults to None.\n            case (Optional[str], optional): The type of problem. Used for automatic network generation. Defaults to None.\n            architecture (Optional[str], optional): The network architecture. Used for automatic network generation. Defaults to None.\n            shallow (Optional[bool], optional): Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.\n            use_batch_norm (Optional[bool], optional):  (Default value = False)\n            encoder_activation (str, optional): The activation function for the encoder. Defaults to \"relu\".\n            devices (Union[str, list], optional): The devices to use. Defaults to \"cpu\".\n            name (str, optional): The name of the autoencoder. Defaults to None.\n\n        \"\"\"\n        super(AutoencoderKoopman, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                channels=channels,\n                architecture=architecture,\n                case=case,\n                shallow=shallow,\n                use_batch_norm=use_batch_norm,\n            )\n\n        self.encoder = encoder.to(self.device)\n        self.decoder = decoder.to(self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.projection = self._projection_with_bottleneck\n            self.reconstruction = self._reconstruction_with_bottleneck\n        else:\n            self.projection = self._projection\n            self.reconstruction = self._reconstruction\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.latent_dimension = None\n\n        if bottleneck_encoder is not None:\n            self.latent_dimension = bottleneck_encoder.output_size\n        else:\n            self.latent_dimension = self.encoder.output_size\n\n        self.K_op = self.to_wrap(entity=torch.nn.Linear(\n            self.latent_dimension, self.latent_dimension, bias=False\n        ).weight, device=self.device)\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n    ) -&gt; torch.Tensor:\n        if verbose == True:\n            if self.input_dim != None:\n                input_shape = list(self.input_dim)\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device\n            )\n\n            self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n                input_shape = self.encoder.input_size\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary()\n\n            print(f\"The Koopman Operator has shape: {self.K_op.shape} \")\n\n            self.bottleneck_decoder.summary()\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def _projection_with_bottleneck(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes the projection of the input data onto the bottleneck encoder.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The projected latent representation.\n\n        \"\"\"\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def _projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes the projection of the input data onto the encoder.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The projected latent representation.\n\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    @as_tensor\n    def _reconstruction_with_bottleneck(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstructs the input data using the bottleneck decoder.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The reconstructed data.\n\n        \"\"\"\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=input_data)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    @as_tensor\n    def _reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstructs the input data using the decoder.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The reconstructed data.\n\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def latent_forward_m(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates the operation u^{u+m} = K^m u^{i}\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n            m (int, optional): The number of Koopman iterations. Defaults to 1.\n\n        Returns:\n            torch.Tensor: The computed latent representation.\n\n        \"\"\"\n        return torch.matmul(input_data, torch.pow(self.K_op.T, m))\n\n    def latent_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates the operation u^{u+1} = K u^{i}\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The computed latent representation.\n\n        \"\"\"\n        return torch.matmul(input_data, self.K_op.T)\n\n    def reconstruction_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates the operation \u0168 = D(E(U))\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The reconstructed data.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def reconstruction_forward_m(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates the operation \u0168_m = D(K^m E(U))\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n            m (int, optional): The number of Koopman iterations. Defaults to 1.\n\n        Returns:\n            torch.Tensor: The reconstructed data.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        latent_m = self.latent_forward_m(input_data=latent, m=m)\n        reconstructed_m = self.reconstruction(input_data=latent_m)\n\n        return reconstructed_m\n\n    def predict(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, n_steps: int = 1\n    ) -&gt; np.ndarray:\n        \"\"\"Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n            n_steps (int, optional): The number of extrapolations to perform. Defaults to 1.\n\n        Returns:\n            np.ndarray: The predicted reconstructed data.\n\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        predictions = list()\n        latent = self.projection(input_data=input_data)\n        init_latent = latent\n\n        # Extrapolating in the latent space over n_steps steps\n        for s in range(n_steps):\n            latent_s = self.latent_forward(input_data=init_latent)\n            init_latent = latent_s\n            predictions.append(latent_s)\n\n        predictions = torch.vstack(predictions)\n\n        reconstructed_predictions = self.reconstruction(input_data=predictions)\n\n        return reconstructed_predictions.detach().numpy()\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"Projects the input data into the latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            np.ndarray: The projected data.\n\n        \"\"\"\n        projected_data = self.projection(input_data=input_data)\n\n        return projected_data.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Reconstructs the input data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n        Returns:\n            np.ndarray: The reconstructed data.\n\n        \"\"\"\n        reconstructed_data = self.reconstruction(input_data=input_data)\n\n        return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, input_dim=None, output_dim=None, latent_dim=None, activation=None, channels=None, case=None, architecture=None, shallow=False, use_batch_norm=False, encoder_activation='relu', devices='cpu', name=None)</code>","text":"<p>Constructs a new instance of the Autoencoder</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Union[ConvolutionalNetwork, DenseNetwork]</code> <p>The encoder network. Defaults to None.</p> <code>None</code> <code>bottleneck_encoder</code> <code>Optional[Union[Linear, DenseNetwork]]</code> <p>The bottleneck encoder network. Defaults to None.</p> <code>None</code> <code>bottleneck_decoder</code> <code>Optional[Union[Linear, DenseNetwork]]</code> <p>The bottleneck decoder network. Defaults to None.</p> <code>None</code> <code>decoder</code> <code>Union[ConvolutionalNetwork, DenseNetwork]</code> <p>The decoder network. Defaults to None.</p> <code>None</code> <code>input_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The input dimensions. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>output_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The output dimensions. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>latent_dim</code> <code>Optional[int]</code> <p>The latent dimensions. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>activation</code> <code>Optional[Union[list, str]]</code> <p>The activation functions for each layer. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>channels</code> <code>Optional[int]</code> <p>The number of channels. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>case</code> <code>Optional[str]</code> <p>The type of problem. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>architecture</code> <code>Optional[str]</code> <p>The network architecture. Used for automatic network generation. Defaults to None.</p> <code>None</code> <code>shallow</code> <code>Optional[bool]</code> <p>Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.</p> <code>False</code> <code>use_batch_norm</code> <code>Optional[bool]</code> <p>(Default value = False)</p> <code>False</code> <code>encoder_activation</code> <code>str</code> <p>The activation function for the encoder. Defaults to \"relu\".</p> <code>'relu'</code> <code>devices</code> <code>Union[str, list]</code> <p>The devices to use. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>name</code> <code>str</code> <p>The name of the autoencoder. Defaults to None.</p> <code>None</code> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n    bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n    decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    case: Optional[str] = None,\n    architecture: Optional[str] = None,\n    shallow: Optional[bool] = False,\n    use_batch_norm: Optional[bool] = False,\n    encoder_activation: str = \"relu\",\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n) -&gt; None:\n    \"\"\"Constructs a new instance of the Autoencoder\n\n    Args:\n        encoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The encoder network. Defaults to None.\n        bottleneck_encoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck encoder network. Defaults to None.\n        bottleneck_decoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck decoder network. Defaults to None.\n        decoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The decoder network. Defaults to None.\n        input_dim (Optional[Tuple[int, ...]], optional): The input dimensions. Used for automatic network generation. Defaults to None.\n        output_dim (Optional[Tuple[int, ...]], optional): The output dimensions. Used for automatic network generation. Defaults to None.\n        latent_dim (Optional[int], optional): The latent dimensions. Used for automatic network generation. Defaults to None.\n        activation (Optional[Union[list, str]], optional): The activation functions for each layer. Used for automatic network generation. Defaults to None.\n        channels (Optional[int], optional): The number of channels. Used for automatic network generation. Defaults to None.\n        case (Optional[str], optional): The type of problem. Used for automatic network generation. Defaults to None.\n        architecture (Optional[str], optional): The network architecture. Used for automatic network generation. Defaults to None.\n        shallow (Optional[bool], optional): Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.\n        use_batch_norm (Optional[bool], optional):  (Default value = False)\n        encoder_activation (str, optional): The activation function for the encoder. Defaults to \"relu\".\n        devices (Union[str, list], optional): The devices to use. Defaults to \"cpu\".\n        name (str, optional): The name of the autoencoder. Defaults to None.\n\n    \"\"\"\n    super(AutoencoderKoopman, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            channels=channels,\n            architecture=architecture,\n            case=case,\n            shallow=shallow,\n            use_batch_norm=use_batch_norm,\n        )\n\n    self.encoder = encoder.to(self.device)\n    self.decoder = decoder.to(self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.projection = self._projection_with_bottleneck\n        self.reconstruction = self._reconstruction_with_bottleneck\n    else:\n        self.projection = self._projection\n        self.reconstruction = self._reconstruction\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.latent_dimension = None\n\n    if bottleneck_encoder is not None:\n        self.latent_dimension = bottleneck_encoder.output_size\n    else:\n        self.latent_dimension = self.encoder.output_size\n\n    self.K_op = self.to_wrap(entity=torch.nn.Linear(\n        self.latent_dimension, self.latent_dimension, bias=False\n    ).weight, device=self.device)\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward","title":"<code>latent_forward(input_data=None)</code>","text":"<p>Evaluates the operation u^{u+1} = K u^{i}</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed latent representation.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates the operation u^{u+1} = K u^{i}\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n    Returns:\n        torch.Tensor: The computed latent representation.\n\n    \"\"\"\n    return torch.matmul(input_data, self.K_op.T)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward_m","title":"<code>latent_forward_m(input_data=None, m=1)</code>","text":"<p>Evaluates the operation u^{u+m} = K^m u^{i}</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <code>m</code> <code>int</code> <p>The number of Koopman iterations. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed latent representation.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_forward_m(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates the operation u^{u+m} = K^m u^{i}\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n        m (int, optional): The number of Koopman iterations. Defaults to 1.\n\n    Returns:\n        torch.Tensor: The computed latent representation.\n\n    \"\"\"\n    return torch.matmul(input_data, torch.pow(self.K_op.T, m))\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.predict","title":"<code>predict(input_data=None, n_steps=1)</code>","text":"<p>Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>The number of extrapolations to perform. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def predict(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, n_steps: int = 1\n) -&gt; np.ndarray:\n    \"\"\"Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n        n_steps (int, optional): The number of extrapolations to perform. Defaults to 1.\n\n    Returns:\n        np.ndarray: The predicted reconstructed data.\n\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    predictions = list()\n    latent = self.projection(input_data=input_data)\n    init_latent = latent\n\n    # Extrapolating in the latent space over n_steps steps\n    for s in range(n_steps):\n        latent_s = self.latent_forward(input_data=init_latent)\n        init_latent = latent_s\n        predictions.append(latent_s)\n\n    predictions = torch.vstack(predictions)\n\n    reconstructed_predictions = self.reconstruction(input_data=predictions)\n\n    return reconstructed_predictions.detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.project","title":"<code>project(input_data=None)</code>","text":"<p>Projects the input data into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The projected data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"Projects the input data into the latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n    Returns:\n        np.ndarray: The projected data.\n\n    \"\"\"\n    projected_data = self.projection(input_data=input_data)\n\n    return projected_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"Reconstructs the input data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n    Returns:\n        np.ndarray: The reconstructed data.\n\n    \"\"\"\n    reconstructed_data = self.reconstruction(input_data=input_data)\n\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward","title":"<code>reconstruction_forward(input_data=None)</code>","text":"<p>Evaluates the operation \u0168 = D(E(U))</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates the operation \u0168 = D(E(U))\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n\n    Returns:\n        torch.Tensor: The reconstructed data.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward_m","title":"<code>reconstruction_forward_m(input_data=None, m=1)</code>","text":"<p>Evaluates the operation \u0168_m = D(K^m E(U))</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data. Defaults to None.</p> <code>None</code> <code>m</code> <code>int</code> <p>The number of Koopman iterations. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward_m(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates the operation \u0168_m = D(K^m E(U))\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data. Defaults to None.\n        m (int, optional): The number of Koopman iterations. Defaults to 1.\n\n    Returns:\n        torch.Tensor: The reconstructed data.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    latent_m = self.latent_forward_m(input_data=latent, m=m)\n    reconstructed_m = self.reconstruction(input_data=latent_m)\n\n    return reconstructed_m\n</code></pre>"},{"location":"simulai_models/#autoencodervariational","title":"AutoencoderVariational","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Koopman autoencoder as a reduced order model.</p> <p>A variational autoencoder architecture consists of five stages: --&gt; The convolutional encoder [Optional] --&gt; Fully-connected encoder --&gt; Gaussian noise --&gt; Fully connected decoder --&gt; The convolutional decoder [Optional]</p> <p>SCHEME                                               Gaussian noise                                               ^                                        |      |      |                                        |  |   |   |  | Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til                                        |  |       |  |                                        |             |</p> <pre><code>           ENCODER               DENSE BOTTLENECK           DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderVariational(NetworkTemplate):\n    r\"\"\"This is an implementation of a Koopman autoencoder as a reduced order model.\n\n    A variational autoencoder architecture consists of five stages:\n    --&gt; The convolutional encoder [Optional]\n    --&gt; Fully-connected encoder\n    --&gt; Gaussian noise\n    --&gt; Fully connected decoder\n    --&gt; The convolutional decoder [Optional]\n\n    SCHEME\n                                                  Gaussian noise\n                                                  ^\n                                           |      |      |\n                                           |  |   |   |  |\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n                                           |  |       |  |\n                                           |             |\n\n                   ENCODER               DENSE BOTTLENECK           DECODER\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n        bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n        decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        encoder_activation: str = \"relu\",\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        kernel_size: Optional[int] = None,\n        case: Optional[str] = None,\n        architecture: Optional[str] = None,\n        use_batch_norm: Optional[bool] = False,\n        shallow: Optional[bool] = False,\n        scale: float = 1e-3,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n        **kwargs,\n    ) -&gt; None:\n        r\"\"\"Constructor method.\n\n        Args:\n            encoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The encoder network. Defaults to None.\n            bottleneck_encoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck encoder network. Defaults to None.\n            bottleneck_decoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck decoder network. Defaults to None.\n            decoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The decoder network. Defaults to None.\n            encoder_activation (str, optional): The activation function to use in the encoder. Defaults to \"relu\".\n            input_dim (Optional[Tuple[int, ...]], optional): The input dimension of the data. Defaults to None.\n            output_dim (Optional[Tuple[int, ...]], optional): The output dimension of the data. Defaults to None.\n            latent_dim (Optional[int], optional): The size of the bottleneck layer. Defaults to None.\n            activation (Optional[Union[list, str]], optional): The activation function to use in the networks. Defaults to None.\n            channels (Optional[int], optional): The number of channels in the input data. Defaults to None.\n            kernel_size (Optional[int], optional): Convolutional kernel size. (Default value = None)\n            case (Optional[str], optional): The name of the autoencoder variant. Defaults to None.\n            architecture (Optional[str], optional): The architecture of the networks. Defaults to None.\n            use_batch_norm (Optional[bool], optional):  (Default value = False)\n            shallow (Optional[bool], optional): Whether to use a shallow network architecture. Defaults to False.\n            scale (float, optional): The scale of the initialization. Defaults to 1e-3.\n            devices (Union[str, list], optional): The device(s) to use for computation. Defaults to \"cpu\".\n            name (str, optional): The name of the autoencoder. Defaults to None.\n            **kwargs \n\n        \"\"\"\n        super(AutoencoderVariational, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                channels=channels,\n                kernel_size=kernel_size,\n                architecture=architecture,\n                case=case,\n                shallow=shallow,\n                use_batch_norm=use_batch_norm,\n                name=self.name,\n                **kwargs\n            )\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.decoder = decoder.to(self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        self.there_is_bottleneck = False\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n            self.projection = self._projection_with_bottleneck\n            self.reconstruction = self._reconstruction_with_bottleneck\n\n            self.there_is_bottleneck = True\n\n        else:\n            self.projection = self._projection\n            self.reconstruction = self._reconstruction\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.latent_dimension = None\n\n        if bottleneck_encoder is not None:\n            self.latent_dimension = bottleneck_encoder.output_size\n        else:\n            self.latent_dimension = self.encoder.output_size\n\n        self.z_mean = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                          self.latent_dimension),\n            device=self.device\n        )\n\n        self.z_log_var = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                            self.latent_dimension),\n            device=self.device\n        )\n\n        self.add_module(\"z_mean\", self.z_mean)\n        self.add_module(\"z_log_var\", self.z_log_var)\n\n        self.weights += [self.z_mean.weight]\n        self.weights += [self.z_log_var.weight]\n\n        self.mu = None\n        self.log_v = None\n        self.scale = scale\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n        display: bool = True,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): Input data to pass through the encoder, by default None\n            input_shape (list, optional): The shape of the input data if input_data is None, by default None\n            verbose (bool, optional):  (Default value = True)\n            display (bool, optional):  (Default value = True)\n\n        Returns:\n            torch.Tensor: The output of the autoencoder's decoder applied to the input data.\n\n        Raises:\n            Exception: If self.input_dim is not a tuple or an integer.\n            AssertionError: If input_shape is None when input_data is None.\n\n        Note:\n            The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)\n        \"\"\"\n\n        if verbose == True:\n            if self.input_dim != None:\n                if type(self.input_dim) == tuple:\n                    input_shape = list(self.input_dim)\n                elif type(self.input_dim) == int:\n                    input_shape = [None, self.input_dim]\n                else:\n                    raise Exception(\n                        f\"input_dim is expected to be tuple or int, but received {type(self.input_dim)}\"\n                    )\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device, display=display\n            )\n\n            if type(self.encoder.output_size) == tuple:\n                self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n                input_shape = self.encoder.input_size\n            elif type(self.encoder.output_size) == int:\n                input_shape = [None, self.encoder.input_size]\n            else:\n                pass\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            # Bottleneck networks is are optional\n            if self.there_is_bottleneck:\n                latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n                self.bottleneck_encoder.summary(display=display)\n                self.bottleneck_decoder.summary(display=display)\n\n                bottleneck_output = self.encoder_activation(\n                    self.bottleneck_decoder.forward(input_data=latent)\n                )\n\n                bottleneck_output = bottleneck_output.reshape(\n                    (-1, *before_flatten_dimension)\n                )\n            else:\n                bottleneck_output = btnk_input\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device, display=display)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n\n            # Bottleneck networks is are optional\n            if self.there_is_bottleneck:\n                self.shapes_dict.update(\n                    {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n                )\n                self.shapes_dict.update(\n                    {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n                )\n\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def _projection_with_bottleneck(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the encoder and bottleneck encoder to input data and returns the output.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the encoder, by default None\n\n        Returns:\n            torch.Tensor: The output of the bottleneck encoder applied to the input data.\n        Note:\n            This function is used for projection of the input data into the bottleneck space.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; output_data = autoencoder._projection_with_bottleneck(input_data=input_data)\n        \"\"\"\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def _projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the encoder to input data and returns the output.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the encoder, by default None\n\n        Returns:\n            torch.Tensor: The output of the encoder applied to the input data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; output_data = autoencoder._projection(input_data=input_data)\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    @as_tensor\n    def _reconstruction_with_bottleneck(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the bottleneck decoder and decoder to input data and returns the output.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input data to pass through the bottleneck decoder and decoder, by default None\n\n        Returns:\n            torch.Tensor: The output of the decoder applied to the bottleneck decoder's output.\n        Note:\n            This function is used for reconstruction of the input data from the bottleneck space.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; bottleneck_output = autoencoder._projection_with_bottleneck(input_data=input_data)\n            &gt;&gt;&gt; output_data = autoencoder._reconstruction_with_bottleneck(input_data=bottleneck_output)\n        \"\"\"\n        bottleneck_output = self.encoder_activation(\n            (self.bottleneck_decoder.forward(input_data=input_data))\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    @as_tensor\n    def _reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the decoder to input data and returns the output.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input data to pass through the decoder, by default None\n\n        Returns:\n            torch.Tensor: The output of the decoder applied to the input data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; output_data = autoencoder._reconstruction(input_data=input_data)\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def Mu(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        r\"\"\"Computes the mean of the encoded input data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the mean, by default None\n            to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n        Returns:\n            Union[np.ndarray, torch.Tensor]: The mean of the encoded input data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n\n        if to_numpy == True:\n            return self.z_mean(latent).detach().numpy()\n        else:\n            return self.z_mean(latent)\n\n    def Sigma(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        r\"\"\"Computes the standard deviation of the encoded input data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the standard deviation, by default None\n            to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n        Returns:\n            Union[np.ndarray, torch.Tensor]: The standard deviation of the encoded input data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n\n        if to_numpy == True:\n            return torch.exp(self.z_log_var(latent) / 2).detach().numpy()\n        else:\n            return torch.exp(self.z_log_var(latent) / 2)\n\n    def CoVariance(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        inv: bool = False,\n        to_numpy: bool = False,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        r\"\"\"Computes the covariance matrix of the encoded input data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the covariance matrix, by default None\n            inv (bool, optional): If True, returns the inverse of the covariance matrix, by default False\n            to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n        Returns:\n            Union[np.ndarray, torch.Tensor]: The covariance matrix (or its inverse) of the encoded input data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)\n        \"\"\"\n        if inv == False:\n            Sigma_inv = 1 / self.Sigma(input_data=input_data)\n            covariance = torch.diag_embed(Sigma_inv)\n\n        else:\n            Sigma = self.Sigma(input_data=input_data)\n            covariance = torch.diag_embed(Sigma)\n\n        if to_numpy == True:\n            return covariance.detach().numpy()\n        else:\n            return covariance\n\n    def latent_gaussian_noisy(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Generates a noisy latent representation of the input data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and generate a noisy latent representation, by default None\n\n        Returns:\n            torch.Tensor: A noisy latent representation of the input data.\n        Note:\n            This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)\n        \"\"\"\n        self.mu = self.z_mean(input_data)\n        self.log_v = self.z_log_var(input_data)\n        eps = self.scale * torch.autograd.Variable(\n            torch.randn(*self.log_v.size())\n        ).type_as(self.log_v)\n\n        return self.mu + torch.exp(self.log_v / 2.0) * eps\n\n    def reconstruction_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the autoencoder, by default None\n\n        Returns:\n            torch.Tensor: The reconstructed output of the autoencoder.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        latent_noisy = self.latent_gaussian_noisy(input_data=latent)\n        reconstructed = self.reconstruction(input_data=latent_noisy)\n\n        return reconstructed\n\n    def reconstruction_eval(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the autoencoder, by default None\n\n        Returns:\n            torch.Tensor: The reconstructed output of the autoencoder.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)\n        \"\"\"\n        encoder_output = self.projection(input_data=input_data)\n        latent = self.z_mean(encoder_output)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        r\"\"\"Projects the input data onto the autoencoder's latent space.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to project onto the autoencoder's latent space, by default None\n\n        Returns:\n            np.ndarray: The input data projected onto the autoencoder's latent space.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        projected_data_latent = self.Mu(input_data=input_data)\n\n        return projected_data_latent.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        r\"\"\"Reconstructs the input data using the trained autoencoder.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to reconstruct, by default None\n\n        Returns:\n            np.ndarray: The reconstructed data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        reconstructed_data = self.reconstruction(input_data=input_data)\n\n        return reconstructed_data.cpu().detach().numpy()\n\n    def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        r\"\"\"Reconstructs the input data using the mean of the encoded data.\n\n        Args:\n            input_data (Union[np.ndarray, torch.Tensor], optional): The input data to reconstruct, by default None\n\n        Returns:\n            np.ndarray: The reconstructed data.\n        Example::\n\n            &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n            &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n            &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)\n        \"\"\"\n\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        return self.reconstruction_eval(input_data=input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.CoVariance","title":"<code>CoVariance(input_data=None, inv=False, to_numpy=False)</code>","text":"<p>Computes the covariance matrix of the encoded input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to encode and compute the covariance matrix, by default None</p> <code>None</code> <code>inv</code> <code>bool</code> <p>If True, returns the inverse of the covariance matrix, by default False</p> <code>False</code> <code>to_numpy</code> <code>bool</code> <p>If True, returns the result as a NumPy array, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Union[np.ndarray, torch.Tensor]: The covariance matrix (or its inverse) of the encoded input data.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def CoVariance(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    inv: bool = False,\n    to_numpy: bool = False,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    r\"\"\"Computes the covariance matrix of the encoded input data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the covariance matrix, by default None\n        inv (bool, optional): If True, returns the inverse of the covariance matrix, by default False\n        to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n    Returns:\n        Union[np.ndarray, torch.Tensor]: The covariance matrix (or its inverse) of the encoded input data.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)\n    \"\"\"\n    if inv == False:\n        Sigma_inv = 1 / self.Sigma(input_data=input_data)\n        covariance = torch.diag_embed(Sigma_inv)\n\n    else:\n        Sigma = self.Sigma(input_data=input_data)\n        covariance = torch.diag_embed(Sigma)\n\n    if to_numpy == True:\n        return covariance.detach().numpy()\n    else:\n        return covariance\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Mu","title":"<code>Mu(input_data=None, to_numpy=False)</code>","text":"<p>Computes the mean of the encoded input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to encode and compute the mean, by default None</p> <code>None</code> <code>to_numpy</code> <code>bool</code> <p>If True, returns the result as a NumPy array, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Union[np.ndarray, torch.Tensor]: The mean of the encoded input data.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def Mu(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    r\"\"\"Computes the mean of the encoded input data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the mean, by default None\n        to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n    Returns:\n        Union[np.ndarray, torch.Tensor]: The mean of the encoded input data.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n\n    if to_numpy == True:\n        return self.z_mean(latent).detach().numpy()\n    else:\n        return self.z_mean(latent)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Sigma","title":"<code>Sigma(input_data=None, to_numpy=False)</code>","text":"<p>Computes the standard deviation of the encoded input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to encode and compute the standard deviation, by default None</p> <code>None</code> <code>to_numpy</code> <code>bool</code> <p>If True, returns the result as a NumPy array, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Union[np.ndarray, torch.Tensor]: The standard deviation of the encoded input data.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def Sigma(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    r\"\"\"Computes the standard deviation of the encoded input data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and compute the standard deviation, by default None\n        to_numpy (bool, optional): If True, returns the result as a NumPy array, by default False\n\n    Returns:\n        Union[np.ndarray, torch.Tensor]: The standard deviation of the encoded input data.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n\n    if to_numpy == True:\n        return torch.exp(self.z_log_var(latent) / 2).detach().numpy()\n    else:\n        return torch.exp(self.z_log_var(latent) / 2)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, encoder_activation='relu', input_dim=None, output_dim=None, latent_dim=None, activation=None, channels=None, kernel_size=None, case=None, architecture=None, use_batch_norm=False, shallow=False, scale=0.001, devices='cpu', name=None, **kwargs)</code>","text":"<p>Constructor method.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Union[ConvolutionalNetwork, DenseNetwork]</code> <p>The encoder network. Defaults to None.</p> <code>None</code> <code>bottleneck_encoder</code> <code>Optional[Union[Linear, DenseNetwork]]</code> <p>The bottleneck encoder network. Defaults to None.</p> <code>None</code> <code>bottleneck_decoder</code> <code>Optional[Union[Linear, DenseNetwork]]</code> <p>The bottleneck decoder network. Defaults to None.</p> <code>None</code> <code>decoder</code> <code>Union[ConvolutionalNetwork, DenseNetwork]</code> <p>The decoder network. Defaults to None.</p> <code>None</code> <code>encoder_activation</code> <code>str</code> <p>The activation function to use in the encoder. Defaults to \"relu\".</p> <code>'relu'</code> <code>input_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The input dimension of the data. Defaults to None.</p> <code>None</code> <code>output_dim</code> <code>Optional[Tuple[int, ...]]</code> <p>The output dimension of the data. Defaults to None.</p> <code>None</code> <code>latent_dim</code> <code>Optional[int]</code> <p>The size of the bottleneck layer. Defaults to None.</p> <code>None</code> <code>activation</code> <code>Optional[Union[list, str]]</code> <p>The activation function to use in the networks. Defaults to None.</p> <code>None</code> <code>channels</code> <code>Optional[int]</code> <p>The number of channels in the input data. Defaults to None.</p> <code>None</code> <code>kernel_size</code> <code>Optional[int]</code> <p>Convolutional kernel size. (Default value = None)</p> <code>None</code> <code>case</code> <code>Optional[str]</code> <p>The name of the autoencoder variant. Defaults to None.</p> <code>None</code> <code>architecture</code> <code>Optional[str]</code> <p>The architecture of the networks. Defaults to None.</p> <code>None</code> <code>use_batch_norm</code> <code>Optional[bool]</code> <p>(Default value = False)</p> <code>False</code> <code>shallow</code> <code>Optional[bool]</code> <p>Whether to use a shallow network architecture. Defaults to False.</p> <code>False</code> <code>scale</code> <code>float</code> <p>The scale of the initialization. Defaults to 1e-3.</p> <code>0.001</code> <code>devices</code> <code>Union[str, list]</code> <p>The device(s) to use for computation. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>name</code> <code>str</code> <p>The name of the autoencoder. Defaults to None.</p> <code>None</code> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n    bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n    decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    encoder_activation: str = \"relu\",\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    kernel_size: Optional[int] = None,\n    case: Optional[str] = None,\n    architecture: Optional[str] = None,\n    use_batch_norm: Optional[bool] = False,\n    shallow: Optional[bool] = False,\n    scale: float = 1e-3,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n    **kwargs,\n) -&gt; None:\n    r\"\"\"Constructor method.\n\n    Args:\n        encoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The encoder network. Defaults to None.\n        bottleneck_encoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck encoder network. Defaults to None.\n        bottleneck_decoder (Optional[Union[Linear, DenseNetwork]], optional): The bottleneck decoder network. Defaults to None.\n        decoder (Union[ConvolutionalNetwork, DenseNetwork], optional): The decoder network. Defaults to None.\n        encoder_activation (str, optional): The activation function to use in the encoder. Defaults to \"relu\".\n        input_dim (Optional[Tuple[int, ...]], optional): The input dimension of the data. Defaults to None.\n        output_dim (Optional[Tuple[int, ...]], optional): The output dimension of the data. Defaults to None.\n        latent_dim (Optional[int], optional): The size of the bottleneck layer. Defaults to None.\n        activation (Optional[Union[list, str]], optional): The activation function to use in the networks. Defaults to None.\n        channels (Optional[int], optional): The number of channels in the input data. Defaults to None.\n        kernel_size (Optional[int], optional): Convolutional kernel size. (Default value = None)\n        case (Optional[str], optional): The name of the autoencoder variant. Defaults to None.\n        architecture (Optional[str], optional): The architecture of the networks. Defaults to None.\n        use_batch_norm (Optional[bool], optional):  (Default value = False)\n        shallow (Optional[bool], optional): Whether to use a shallow network architecture. Defaults to False.\n        scale (float, optional): The scale of the initialization. Defaults to 1e-3.\n        devices (Union[str, list], optional): The device(s) to use for computation. Defaults to \"cpu\".\n        name (str, optional): The name of the autoencoder. Defaults to None.\n        **kwargs \n\n    \"\"\"\n    super(AutoencoderVariational, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            channels=channels,\n            kernel_size=kernel_size,\n            architecture=architecture,\n            case=case,\n            shallow=shallow,\n            use_batch_norm=use_batch_norm,\n            name=self.name,\n            **kwargs\n        )\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.decoder = decoder.to(self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    self.there_is_bottleneck = False\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n        self.projection = self._projection_with_bottleneck\n        self.reconstruction = self._reconstruction_with_bottleneck\n\n        self.there_is_bottleneck = True\n\n    else:\n        self.projection = self._projection\n        self.reconstruction = self._reconstruction\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.latent_dimension = None\n\n    if bottleneck_encoder is not None:\n        self.latent_dimension = bottleneck_encoder.output_size\n    else:\n        self.latent_dimension = self.encoder.output_size\n\n    self.z_mean = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                      self.latent_dimension),\n        device=self.device\n    )\n\n    self.z_log_var = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                        self.latent_dimension),\n        device=self.device\n    )\n\n    self.add_module(\"z_mean\", self.z_mean)\n    self.add_module(\"z_log_var\", self.z_log_var)\n\n    self.weights += [self.z_mean.weight]\n    self.weights += [self.z_log_var.weight]\n\n    self.mu = None\n    self.log_v = None\n    self.scale = scale\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.eval","title":"<code>eval(input_data=None)</code>","text":"<p>Reconstructs the input data using the mean of the encoded data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to reconstruct, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed data.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    r\"\"\"Reconstructs the input data using the mean of the encoded data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to reconstruct, by default None\n\n    Returns:\n        np.ndarray: The reconstructed data.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)\n    \"\"\"\n\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    return self.reconstruction_eval(input_data=input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy","title":"<code>latent_gaussian_noisy(input_data=None)</code>","text":"<p>Generates a noisy latent representation of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to encode and generate a noisy latent representation, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A noisy latent representation of the input data.</p> <p>Note:     This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation. Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_gaussian_noisy(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    r\"\"\"Generates a noisy latent representation of the input data.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to encode and generate a noisy latent representation, by default None\n\n    Returns:\n        torch.Tensor: A noisy latent representation of the input data.\n    Note:\n        This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)\n    \"\"\"\n    self.mu = self.z_mean(input_data)\n    self.log_v = self.z_log_var(input_data)\n    eps = self.scale * torch.autograd.Variable(\n        torch.randn(*self.log_v.size())\n    ).type_as(self.log_v)\n\n    return self.mu + torch.exp(self.log_v / 2.0) * eps\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.project","title":"<code>project(input_data=None)</code>","text":"<p>Projects the input data onto the autoencoder's latent space.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to project onto the autoencoder's latent space, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The input data projected onto the autoencoder's latent space.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    r\"\"\"Projects the input data onto the autoencoder's latent space.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to project onto the autoencoder's latent space, by default None\n\n    Returns:\n        np.ndarray: The input data projected onto the autoencoder's latent space.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    projected_data_latent = self.Mu(input_data=input_data)\n\n    return projected_data_latent.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the input data using the trained autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to reconstruct, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed data.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    r\"\"\"Reconstructs the input data using the trained autoencoder.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to reconstruct, by default None\n\n    Returns:\n        np.ndarray: The reconstructed data.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    reconstructed_data = self.reconstruction(input_data=input_data)\n\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_eval","title":"<code>reconstruction_eval(input_data=None)</code>","text":"<p>Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to pass through the autoencoder, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed output of the autoencoder.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_eval(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    r\"\"\"Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the autoencoder, by default None\n\n    Returns:\n        torch.Tensor: The reconstructed output of the autoencoder.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)\n    \"\"\"\n    encoder_output = self.projection(input_data=input_data)\n    latent = self.z_mean(encoder_output)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_forward","title":"<code>reconstruction_forward(input_data=None)</code>","text":"<p>Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>The input data to pass through the autoencoder, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The reconstructed output of the autoencoder.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    r\"\"\"Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): The input data to pass through the autoencoder, by default None\n\n    Returns:\n        torch.Tensor: The reconstructed output of the autoencoder.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    latent_noisy = self.latent_gaussian_noisy(input_data=latent)\n    reconstructed = self.reconstruction(input_data=latent_noisy)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary","title":"<code>summary(input_data=None, input_shape=None, verbose=True, display=True)</code>","text":"<p>Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[ndarray, Tensor]</code> <p>Input data to pass through the encoder, by default None</p> <code>None</code> <code>input_shape</code> <code>list</code> <p>The shape of the input data if input_data is None, by default None</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>(Default value = True)</p> <code>True</code> <code>display</code> <code>bool</code> <p>(Default value = True)</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the autoencoder's decoder applied to the input data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If self.input_dim is not a tuple or an integer.</p> <code>AssertionError</code> <p>If input_shape is None when input_data is None.</p> Note <p>The summary method calls the <code>summary</code> method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n&gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n&gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    input_shape: list = None,\n    verbose: bool = True,\n    display: bool = True,\n) -&gt; torch.Tensor:\n    r\"\"\"Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.\n\n    Args:\n        input_data (Union[np.ndarray, torch.Tensor], optional): Input data to pass through the encoder, by default None\n        input_shape (list, optional): The shape of the input data if input_data is None, by default None\n        verbose (bool, optional):  (Default value = True)\n        display (bool, optional):  (Default value = True)\n\n    Returns:\n        torch.Tensor: The output of the autoencoder's decoder applied to the input data.\n\n    Raises:\n        Exception: If self.input_dim is not a tuple or an integer.\n        AssertionError: If input_shape is None when input_data is None.\n\n    Note:\n        The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.\n    Example::\n\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)\n    \"\"\"\n\n    if verbose == True:\n        if self.input_dim != None:\n            if type(self.input_dim) == tuple:\n                input_shape = list(self.input_dim)\n            elif type(self.input_dim) == int:\n                input_shape = [None, self.input_dim]\n            else:\n                raise Exception(\n                    f\"input_dim is expected to be tuple or int, but received {type(self.input_dim)}\"\n                )\n        else:\n            pass\n\n        self.encoder.summary(\n            input_data=input_data, input_shape=input_shape, device=self.device, display=display\n        )\n\n        if type(self.encoder.output_size) == tuple:\n            self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n            input_shape = self.encoder.input_size\n        elif type(self.encoder.output_size) == int:\n            input_shape = [None, self.encoder.input_size]\n        else:\n            pass\n\n        if isinstance(input_data, np.ndarray):\n            btnk_input = self.encoder.forward(input_data=input_data)\n        else:\n            assert (\n                input_shape\n            ), \"It is necessary to have input_shape when input_data is None.\"\n\n            input_shape[0] = 1\n\n            input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n            btnk_input = self.encoder.forward(input_data=input_data)\n\n        before_flatten_dimension = tuple(btnk_input.shape[1:])\n        btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n        # Bottleneck networks is are optional\n        if self.there_is_bottleneck:\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary(display=display)\n            self.bottleneck_decoder.summary(display=display)\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n        else:\n            bottleneck_output = btnk_input\n\n        self.decoder.summary(input_data=bottleneck_output, device=self.device, display=display)\n\n        # Saving the content of the subnetworks to the overall architecture dictionary\n        self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n\n        # Bottleneck networks is are optional\n        if self.there_is_bottleneck:\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n\n        self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n    else:\n        print(self)\n</code></pre>"},{"location":"simulai_regression/","title":"simulai.regression","text":""},{"location":"simulai_regression/#dense","title":"Dense","text":""},{"location":"simulai_regression/#linear","title":"Linear","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class Linear(NetworkTemplate):\n\n    name = \"linear\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        input_size: int = None,\n        output_size: int = None,\n        bias: bool = True,\n        name: str = None,\n    ) -&gt; None:\n\n        \"\"\"Linear operator F(u) = Au + b\n\n        Args:\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            bias (bool, optional): Using bias tensor or not. (Default value = True)\n            name (str, optional): A name for identifying the model. (Default value = None)\n\n        \"\"\"\n\n        super(Linear, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.activations_str = None\n\n        self.layers = [torch.nn.Linear(input_size, output_size, bias=bias)]\n\n        self.add_module(self.name + \"_\" + \"linear_op\", self.layers[0])\n\n        self.weights = [item.weight for item in self.layers]\n\n        self.bias = [item.bias for item in self.layers]\n\n        self.name = name\n\n    @as_tensor\n    def forward(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n\n        \"\"\"Applying the operator Linear.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): Data to be processed using Linear. (Default value = None)\n\n        \"\"\"\n\n        return self.layers[0](input_data)\n\n    def to_numpy(self):\n\n        \"\"\"It converts the tensors in Linear to numpy.ndarray.\"\"\"\n\n        return LinearNumpy(layer=self.layers[0], name=self.name)\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.Linear.__init__","title":"<code>__init__(input_size=None, output_size=None, bias=True, name=None)</code>","text":"<p>Linear operator F(u) = Au + b</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Using bias tensor or not. (Default value = True)</p> <code>True</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    input_size: int = None,\n    output_size: int = None,\n    bias: bool = True,\n    name: str = None,\n) -&gt; None:\n\n    \"\"\"Linear operator F(u) = Au + b\n\n    Args:\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        bias (bool, optional): Using bias tensor or not. (Default value = True)\n        name (str, optional): A name for identifying the model. (Default value = None)\n\n    \"\"\"\n\n    super(Linear, self).__init__(name=name)\n\n    self.input_size = input_size\n    self.output_size = output_size\n\n    self.activations_str = None\n\n    self.layers = [torch.nn.Linear(input_size, output_size, bias=bias)]\n\n    self.add_module(self.name + \"_\" + \"linear_op\", self.layers[0])\n\n    self.weights = [item.weight for item in self.layers]\n\n    self.bias = [item.bias for item in self.layers]\n\n    self.name = name\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.Linear.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Applying the operator Linear.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>Data to be processed using Linear. (Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>@as_tensor\ndef forward(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n\n    \"\"\"Applying the operator Linear.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): Data to be processed using Linear. (Default value = None)\n\n    \"\"\"\n\n    return self.layers[0](input_data)\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.Linear.to_numpy","title":"<code>to_numpy()</code>","text":"<p>It converts the tensors in Linear to numpy.ndarray.</p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def to_numpy(self):\n\n    \"\"\"It converts the tensors in Linear to numpy.ndarray.\"\"\"\n\n    return LinearNumpy(layer=self.layers[0], name=self.name)\n</code></pre>"},{"location":"simulai_regression/#slfnn","title":"SLFNN","text":"<p>             Bases: <code>Linear</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class SLFNN(Linear):\n    def __init__(\n        self,\n        input_size: int = None,\n        output_size: int = None,\n        bias: bool = True,\n        name: str = None,\n        activation: str = \"tanh\",\n    ) -&gt; None:\n        \"\"\"Single layer fully-connected (dense) neural network\n\n        Args:\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            bias (bool, optional): Using bias tensor or not. (Default value = True)\n            name (str, optional): A name for identifying the model. (Default value = None)\n            activation (str, optional): Activation function. (Default value = \"tanh\")\n\n        \"\"\"\n\n        super(SLFNN, self).__init__(\n            input_size=input_size, output_size=output_size, bias=bias, name=name\n        )\n\n        self.activation = self._get_operation(operation=activation)\n\n    def forward(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n\n        \"\"\"Applying the operator Linear.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): Data to be processed using SLFNN. (Default value = None)\n\n        \"\"\"\n\n\n        return self.activation(super().forward(input_data=input_data))\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.SLFNN.__init__","title":"<code>__init__(input_size=None, output_size=None, bias=True, name=None, activation='tanh')</code>","text":"<p>Single layer fully-connected (dense) neural network</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Using bias tensor or not. (Default value = True)</p> <code>True</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = None)</p> <code>None</code> <code>activation</code> <code>str</code> <p>Activation function. (Default value = \"tanh\")</p> <code>'tanh'</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    input_size: int = None,\n    output_size: int = None,\n    bias: bool = True,\n    name: str = None,\n    activation: str = \"tanh\",\n) -&gt; None:\n    \"\"\"Single layer fully-connected (dense) neural network\n\n    Args:\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        bias (bool, optional): Using bias tensor or not. (Default value = True)\n        name (str, optional): A name for identifying the model. (Default value = None)\n        activation (str, optional): Activation function. (Default value = \"tanh\")\n\n    \"\"\"\n\n    super(SLFNN, self).__init__(\n        input_size=input_size, output_size=output_size, bias=bias, name=name\n    )\n\n    self.activation = self._get_operation(operation=activation)\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.SLFNN.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Applying the operator Linear.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>Data to be processed using SLFNN. (Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def forward(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n\n    \"\"\"Applying the operator Linear.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): Data to be processed using SLFNN. (Default value = None)\n\n    \"\"\"\n\n\n    return self.activation(super().forward(input_data=input_data))\n</code></pre>"},{"location":"simulai_regression/#shallownetwork","title":"ShallowNetwork","text":"<p>             Bases: <code>SLFNN</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class ShallowNetwork(SLFNN):\n    def __init__(\n        self,\n        input_size: int = None,\n        hidden_size: int = None,\n        output_size: int = None,\n        bias: bool = True,\n        name: str = None,\n        activation: str = \"tanh\",\n    ) -&gt; None:\n\n        \"\"\"ELM-like (Extreme Learning Machine) shallow network\n\n        Args:\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            hidden_size (int, optional): Dimension of the hidden (intermediary) state. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            bias (bool, optional): Using bias or not for the last layer. (Default value = True)\n            name (str, optional): A name for identifying the model. (Default value = None)\n            activation (str, optional): Activation function. (Default value = \"tanh\")\n\n        \"\"\"\n\n        super(ShallowNetwork, self).__init__(\n            input_size=input_size, output_size=hidden_size, bias=bias, name=name\n        )\n\n        self.output_layer = Linear(\n            input_size=hidden_size, output_size=output_size, bias=False, name=\"output\"\n        )\n\n        self.output_size = output_size\n\n    def forward(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional):  (Default value = None)\n\n        \"\"\"\n\n        hidden_state = self.activation(super().forward(input_data=input_data))\n\n        return self.output_layer.forward(input_data=hidden_state)\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ShallowNetwork.__init__","title":"<code>__init__(input_size=None, hidden_size=None, output_size=None, bias=True, name=None, activation='tanh')</code>","text":"<p>ELM-like (Extreme Learning Machine) shallow network</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>hidden_size</code> <code>int</code> <p>Dimension of the hidden (intermediary) state. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Using bias or not for the last layer. (Default value = True)</p> <code>True</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = None)</p> <code>None</code> <code>activation</code> <code>str</code> <p>Activation function. (Default value = \"tanh\")</p> <code>'tanh'</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    input_size: int = None,\n    hidden_size: int = None,\n    output_size: int = None,\n    bias: bool = True,\n    name: str = None,\n    activation: str = \"tanh\",\n) -&gt; None:\n\n    \"\"\"ELM-like (Extreme Learning Machine) shallow network\n\n    Args:\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        hidden_size (int, optional): Dimension of the hidden (intermediary) state. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        bias (bool, optional): Using bias or not for the last layer. (Default value = True)\n        name (str, optional): A name for identifying the model. (Default value = None)\n        activation (str, optional): Activation function. (Default value = \"tanh\")\n\n    \"\"\"\n\n    super(ShallowNetwork, self).__init__(\n        input_size=input_size, output_size=hidden_size, bias=bias, name=name\n    )\n\n    self.output_layer = Linear(\n        input_size=hidden_size, output_size=output_size, bias=False, name=\"output\"\n    )\n\n    self.output_size = output_size\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ShallowNetwork.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>(Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def forward(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional):  (Default value = None)\n\n    \"\"\"\n\n    hidden_state = self.activation(super().forward(input_data=input_data))\n\n    return self.output_layer.forward(input_data=hidden_state)\n</code></pre>"},{"location":"simulai_regression/#densenetwork","title":"DenseNetwork","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class DenseNetwork(NetworkTemplate):\n    name = \"dense\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        layers_units: list = None,\n        activations: Union[list, str] = None,\n        input_size: int = None,\n        output_size: int = None,\n        normalization: str = \"bypass\",\n        name: str = \"\",\n        last_bias: bool = True,\n        last_activation: str = \"identity\",\n        **kwargs,\n    ) -&gt; None:\n\n        \"\"\"Dense (fully-connected) neural network written in PyTorch\n\n        Args:\n            layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n            activations (Union[list, str], optional): List of activations for each layer or a single string\n            informing the activation used for all of them. (Default value = None)\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n            name (str, optional): A name for identifying the model. (Default value = \"\")\n            last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n            last_activation (str, optional): Activation for the last layer (default is 'identity').\n            **kwargs \n\n        \"\"\"\n\n        super(DenseNetwork, self).__init__()\n\n        assert layers_units, \"Please, set a list of units for each layer\"\n\n        assert activations, (\n            \"Please, set a list of activation functions\" \"or a string for all of them.\"\n        )\n\n        # These activations support gain evaluation for the initial state\n        self.gain_supported_activations = [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"]\n\n        # Default attributes\n        self.layers_units = layers_units\n        self.input_size = input_size\n        self.output_size = output_size\n        self.normalization = normalization\n        self.name = name\n        self.last_bias = last_bias\n\n        # For extra and not ever required parameters\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n        # Getting up parameters from host\n        self._get_from_guest(activation=activations)\n\n        self.weights = list()\n\n        # The total number of layers includes the output layer\n        self.n_layers = len(self.layers_units) + 1\n\n        self.default_last_activation = last_activation\n\n        self.activations, self.activations_str = self._setup_activations(\n            activation=activations\n        )\n\n        self.initializations = [\n            self._determine_initialization(activation)\n            for activation in self.activations_str\n        ]\n\n        self.layers = self._setup_hidden_layers(last_bias=last_bias)\n\n        array_layers = self._numpy_layers()\n        n_layers = len(self.layers)\n\n        self.shapes = [item.shape for item in list(sum(array_layers, []))]\n\n        self.stitch_idx = self._make_stitch_idx()\n\n        self.layers_map = [[ll, ll + 1] for ll in range(0, 2 * n_layers, 2)]\n\n    def _calculate_gain(self, activation: str = \"Tanh\") -&gt; float:\n\n        \"\"\"It evaluates a multiplier coefficient, named as `gain`,\n        which is used to enhance the funcionality of each kind of activation\n        function.\n\n        Args:\n            activation (str, optional):  (Default value = \"Tanh\")\n\n        \"\"\"\n\n        if type(activation) is not str:\n            assert hasattr(\n                activation, \"name\"\n            ), f\"Activation object {type(activation)} must have attribute \u00b4name\u00b4.\"\n            name = getattr(activation, \"name\")\n        else:\n            name = activation\n\n        if name.lower() in self.gain_supported_activations:\n            return torch.nn.init.calculate_gain(name.lower())\n        else:\n            return 1\n\n    @staticmethod\n    def _determine_initialization(activation: str = \"Tanh\") -&gt; str:\n\n        \"\"\"It determines the most proper initialization method for each\n        activation function.\n\n        Args:\n            activation (str, optional): Activation function. (Default value = \"Tanh\")\n\n        \"\"\"\n\n        if type(activation) is not str:\n            assert hasattr(\n                activation, \"name\"\n            ), f\"Activation object {type(activation)} must have attribute \u00b4name\u00b4.\"\n            name = getattr(activation, \"name\")\n        else:\n            name = activation\n\n        if name in [\"ReLU\"]:\n            return \"kaiming\"\n        elif name == \"Siren\":\n            return \"siren\"\n        else:\n            return \"xavier\"\n\n    def _setup_layer(\n        self,\n        input_size: int = 0,\n        output_size: int = 0,\n        initialization: str = None,\n        bias: bool = True,\n        first_layer: bool = False,\n    ) -&gt; torch.nn.Linear:\n\n        \"\"\"\n\n        Args:\n            input_size (int, optional): Dimension of the input. (Default value = 0)\n            output_size (int, optional): Dimension of the output. (Default value = 0)\n            initialization (str, optional): Initialization method. (Default value = None)\n            bias (bool, optional): Using bias tensor or not. (Default value = True)\n            first_layer (bool, optional): Is this layer the first layer or not. (Default value = False)\n\n        \"\"\"\n\n        # It instantiates a linear operation\n        # f: y^l = f(x^(l-1)) = (W^l).dot(x^(l-1)) + b^l\n        layer = torch.nn.Linear(input_size, output_size, bias=bias)\n\n        if initialization == \"xavier\":\n            torch.nn.init.xavier_normal_(\n                layer.weight, gain=self._calculate_gain(self.activations_str[0])\n            )\n            return layer\n\n        # The Siren initialization requires some special consideration\n        elif initialization == \"siren\":\n            assert (\n                self.c is not None\n            ), \"When using siren, the parameter c must be defined.\"\n            assert (\n                self.omega_0 is not None\n            ), \"When using siren, the parameter omega_0 must be defined.\"\n\n            if first_layer == True:\n                m = 1 / input_size\n            else:\n                m = np.sqrt(self.c / input_size) / self.omega_0\n\n            torch.nn.init.trunc_normal_(layer.weight, a=-m, b=m)\n            b = np.sqrt(1 / input_size)\n            torch.nn.init.trunc_normal_(layer.bias, a=-b, b=b)\n            return layer\n\n        elif initialization == \"kaiming\":\n            return layer  # Kaiming is the default initialization in PyTorch\n\n        else:\n            print(\n                \"Initialization method still not implemented.\\\n                  Using Kaiming instead\"\n            )\n\n            return layer\n\n    # The forward step of the network\n    @as_tensor\n    def forward(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n\n        \"\"\"It executes the forward step for the DenseNetwork.\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): The input tensor to be processed by DenseNetwork. (Default value = None)\n\n        \"\"\"\n\n        input_tensor_ = input_data\n\n        # TODO It can be done using the PyTorch Sequential object\n        for layer_id in range(len(self.layers)):\n            output_tensor_ = self.layers[layer_id](input_tensor_)\n            _output_tensor_ = self.activations[layer_id](output_tensor_)\n            input_tensor_ = _output_tensor_\n\n        output_tensor = input_tensor_\n\n        return output_tensor\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.DenseNetwork.__init__","title":"<code>__init__(layers_units=None, activations=None, input_size=None, output_size=None, normalization='bypass', name='', last_bias=True, last_activation='identity', **kwargs)</code>","text":"<p>Dense (fully-connected) neural network written in PyTorch</p> <p>Parameters:</p> Name Type Description Default <code>layers_units</code> <code>list</code> <p>List with the number of neurons for each layer. (Default value = None)</p> <code>None</code> <code>activations</code> <code>Union[list, str]</code> <p>List of activations for each layer or a single string</p> <code>None</code> <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>normalization</code> <code>str</code> <p>Kind of normalization used between two layers. (Default value = \"bypass\")</p> <code>'bypass'</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = \"\")</p> <code>''</code> <code>last_bias</code> <code>bool</code> <p>Using bias in the last layer or not. (Default value = True)</p> <code>True</code> <code>last_activation</code> <code>str</code> <p>Activation for the last layer (default is 'identity').</p> <code>'identity'</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    layers_units: list = None,\n    activations: Union[list, str] = None,\n    input_size: int = None,\n    output_size: int = None,\n    normalization: str = \"bypass\",\n    name: str = \"\",\n    last_bias: bool = True,\n    last_activation: str = \"identity\",\n    **kwargs,\n) -&gt; None:\n\n    \"\"\"Dense (fully-connected) neural network written in PyTorch\n\n    Args:\n        layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n        activations (Union[list, str], optional): List of activations for each layer or a single string\n        informing the activation used for all of them. (Default value = None)\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n        name (str, optional): A name for identifying the model. (Default value = \"\")\n        last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n        last_activation (str, optional): Activation for the last layer (default is 'identity').\n        **kwargs \n\n    \"\"\"\n\n    super(DenseNetwork, self).__init__()\n\n    assert layers_units, \"Please, set a list of units for each layer\"\n\n    assert activations, (\n        \"Please, set a list of activation functions\" \"or a string for all of them.\"\n    )\n\n    # These activations support gain evaluation for the initial state\n    self.gain_supported_activations = [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"]\n\n    # Default attributes\n    self.layers_units = layers_units\n    self.input_size = input_size\n    self.output_size = output_size\n    self.normalization = normalization\n    self.name = name\n    self.last_bias = last_bias\n\n    # For extra and not ever required parameters\n    for k, v in kwargs.items():\n        setattr(self, k, v)\n\n    # Getting up parameters from host\n    self._get_from_guest(activation=activations)\n\n    self.weights = list()\n\n    # The total number of layers includes the output layer\n    self.n_layers = len(self.layers_units) + 1\n\n    self.default_last_activation = last_activation\n\n    self.activations, self.activations_str = self._setup_activations(\n        activation=activations\n    )\n\n    self.initializations = [\n        self._determine_initialization(activation)\n        for activation in self.activations_str\n    ]\n\n    self.layers = self._setup_hidden_layers(last_bias=last_bias)\n\n    array_layers = self._numpy_layers()\n    n_layers = len(self.layers)\n\n    self.shapes = [item.shape for item in list(sum(array_layers, []))]\n\n    self.stitch_idx = self._make_stitch_idx()\n\n    self.layers_map = [[ll, ll + 1] for ll in range(0, 2 * n_layers, 2)]\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.DenseNetwork.forward","title":"<code>forward(input_data=None)</code>","text":"<p>It executes the forward step for the DenseNetwork.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>The input tensor to be processed by DenseNetwork. (Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>@as_tensor\ndef forward(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n\n    \"\"\"It executes the forward step for the DenseNetwork.\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): The input tensor to be processed by DenseNetwork. (Default value = None)\n\n    \"\"\"\n\n    input_tensor_ = input_data\n\n    # TODO It can be done using the PyTorch Sequential object\n    for layer_id in range(len(self.layers)):\n        output_tensor_ = self.layers[layer_id](input_tensor_)\n        _output_tensor_ = self.activations[layer_id](output_tensor_)\n        input_tensor_ = _output_tensor_\n\n    output_tensor = input_tensor_\n\n    return output_tensor\n</code></pre>"},{"location":"simulai_regression/#resdensenetwork","title":"ResDenseNetwork","text":"<p>             Bases: <code>DenseNetwork</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class ResDenseNetwork(DenseNetwork):\n    name = \"residualdense\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        layers_units: list = None,\n        activations: Union[list, str] = None,\n        input_size: int = None,\n        output_size: int = None,\n        normalization: str = \"bypass\",\n        name: str = \"\",\n        last_bias: bool = True,\n        last_activation: str = \"identity\",\n        residual_size: int = 1,\n        **kwargs,\n    ) -&gt; None:\n\n        \"\"\"Residual Dense (fully-connected) neural network written in PyTorch\n\n        Args:\n            layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n            activations (Union[list, str], optional): List of activations for each layer or a single string\n            informing the activation used for all of them. (Default value = None)\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n            name (str, optional): A name for identifying the model. (Default value = \"\")\n            last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n            last_activation (str, optional): Activation for the last layer (default is 'identity').\n            residual_size (int, optional): Size of the residual block. (Default value = 1)\n            **kwargs \n\n        \"\"\"\n\n        super().__init__(\n            layers_units=layers_units,\n            activations=activations,\n            input_size=input_size,\n            output_size=output_size,\n            normalization=normalization,\n            name=name,\n            last_bias=last_bias,\n            last_activation=last_activation,\n            **kwargs,\n        )\n\n        # Considering the activations layers\n        self.residual_size = 2 * residual_size\n        self.ratio = 0.5\n\n        # Excluding the input and output layers\n        merged_layers = self._merge(layer=self.layers, act=self.activations)\n\n        assert len(merged_layers[2:-2]) % self.residual_size == 0, (\n            \"The number of layers must be divisible\"\n            \" by the residual block size,\"\n            f\" but received {len(merged_layers)} and {residual_size}\"\n        )\n\n        self.n_residual_blocks = int(len(merged_layers[2:-2]) / self.residual_size)\n\n        sub_layers = [\n            item.tolist()\n            for item in np.split(np.array(merged_layers[2:-2]), self.n_residual_blocks)\n        ]\n\n        self.input_block = torch.nn.Sequential(*merged_layers[:2])\n        self.hidden_blocks = [torch.nn.Sequential(*item) for item in sub_layers]\n        self.output_block = torch.nn.Sequential(*merged_layers[-2:])\n\n    # Merging the layers into a reasonable sequence\n    def _merge(self, layer: list = None, act: list = None) -&gt; list:\n\n        \"\"\"It merges the dense layers and the activations into a single block.\n\n        Args:\n            layer (list, optional): List of dense layers. (Default value = None)\n            act (list, optional): List of activation functions. (Default value = None)\n\n        \"\"\"\n\n        merged_list = list()\n\n        for i, j in zip(layer, act):\n            merged_list.append(i)\n            merged_list.append(j)\n\n        return merged_list\n\n    def summary(self):\n\n        \"\"\"It prints a summary of the network.\"\"\"\n\n        super().summary()\n\n        print(\"Residual Blocks:\\n\")\n\n        print(self.input_block)\n        print(self.hidden_blocks)\n        print(self.output_block)\n\n    @as_tensor\n    def forward(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n\n        \"\"\"\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional):  (Default value = None)\n\n        \"\"\"\n\n        input_tensor_ = input_data\n\n        input_tensor_ = self.input_block(input_tensor_)\n\n        for block in self.hidden_blocks:\n            output_tensor_ = self.ratio * (input_tensor_ + block(input_tensor_))\n\n            input_tensor_ = output_tensor_\n\n        output_tensor = self.output_block(input_tensor_)\n\n        return output_tensor\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ResDenseNetwork.__init__","title":"<code>__init__(layers_units=None, activations=None, input_size=None, output_size=None, normalization='bypass', name='', last_bias=True, last_activation='identity', residual_size=1, **kwargs)</code>","text":"<p>Residual Dense (fully-connected) neural network written in PyTorch</p> <p>Parameters:</p> Name Type Description Default <code>layers_units</code> <code>list</code> <p>List with the number of neurons for each layer. (Default value = None)</p> <code>None</code> <code>activations</code> <code>Union[list, str]</code> <p>List of activations for each layer or a single string</p> <code>None</code> <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>normalization</code> <code>str</code> <p>Kind of normalization used between two layers. (Default value = \"bypass\")</p> <code>'bypass'</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = \"\")</p> <code>''</code> <code>last_bias</code> <code>bool</code> <p>Using bias in the last layer or not. (Default value = True)</p> <code>True</code> <code>last_activation</code> <code>str</code> <p>Activation for the last layer (default is 'identity').</p> <code>'identity'</code> <code>residual_size</code> <code>int</code> <p>Size of the residual block. (Default value = 1)</p> <code>1</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    layers_units: list = None,\n    activations: Union[list, str] = None,\n    input_size: int = None,\n    output_size: int = None,\n    normalization: str = \"bypass\",\n    name: str = \"\",\n    last_bias: bool = True,\n    last_activation: str = \"identity\",\n    residual_size: int = 1,\n    **kwargs,\n) -&gt; None:\n\n    \"\"\"Residual Dense (fully-connected) neural network written in PyTorch\n\n    Args:\n        layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n        activations (Union[list, str], optional): List of activations for each layer or a single string\n        informing the activation used for all of them. (Default value = None)\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n        name (str, optional): A name for identifying the model. (Default value = \"\")\n        last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n        last_activation (str, optional): Activation for the last layer (default is 'identity').\n        residual_size (int, optional): Size of the residual block. (Default value = 1)\n        **kwargs \n\n    \"\"\"\n\n    super().__init__(\n        layers_units=layers_units,\n        activations=activations,\n        input_size=input_size,\n        output_size=output_size,\n        normalization=normalization,\n        name=name,\n        last_bias=last_bias,\n        last_activation=last_activation,\n        **kwargs,\n    )\n\n    # Considering the activations layers\n    self.residual_size = 2 * residual_size\n    self.ratio = 0.5\n\n    # Excluding the input and output layers\n    merged_layers = self._merge(layer=self.layers, act=self.activations)\n\n    assert len(merged_layers[2:-2]) % self.residual_size == 0, (\n        \"The number of layers must be divisible\"\n        \" by the residual block size,\"\n        f\" but received {len(merged_layers)} and {residual_size}\"\n    )\n\n    self.n_residual_blocks = int(len(merged_layers[2:-2]) / self.residual_size)\n\n    sub_layers = [\n        item.tolist()\n        for item in np.split(np.array(merged_layers[2:-2]), self.n_residual_blocks)\n    ]\n\n    self.input_block = torch.nn.Sequential(*merged_layers[:2])\n    self.hidden_blocks = [torch.nn.Sequential(*item) for item in sub_layers]\n    self.output_block = torch.nn.Sequential(*merged_layers[-2:])\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ResDenseNetwork.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>(Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>@as_tensor\ndef forward(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n\n    \"\"\"\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional):  (Default value = None)\n\n    \"\"\"\n\n    input_tensor_ = input_data\n\n    input_tensor_ = self.input_block(input_tensor_)\n\n    for block in self.hidden_blocks:\n        output_tensor_ = self.ratio * (input_tensor_ + block(input_tensor_))\n\n        input_tensor_ = output_tensor_\n\n    output_tensor = self.output_block(input_tensor_)\n\n    return output_tensor\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ResDenseNetwork.summary","title":"<code>summary()</code>","text":"<p>It prints a summary of the network.</p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def summary(self):\n\n    \"\"\"It prints a summary of the network.\"\"\"\n\n    super().summary()\n\n    print(\"Residual Blocks:\\n\")\n\n    print(self.input_block)\n    print(self.hidden_blocks)\n    print(self.output_block)\n</code></pre>"},{"location":"simulai_regression/#convexdensenetwork","title":"ConvexDenseNetwork","text":"<p>             Bases: <code>DenseNetwork</code></p> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>class ConvexDenseNetwork(DenseNetwork):\n    name = \"convexdense\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        layers_units: list = None,\n        activations: Union[list, str] = None,\n        input_size: int = None,\n        output_size: int = None,\n        normalization: str = \"bypass\",\n        name: str = \"\",\n        last_bias: bool = True,\n        last_activation: str = \"identity\",\n        **kwargs,\n    ) -&gt; None:\n\n        \"\"\"Dense network with convex combinations in the hidden layers.\n        This architecture is useful when combined to the Improved Version of DeepONets\n\n        Args:\n            layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n            activations (Union[list, str], optional): List of activations for each layer or a single string\n            informing the activation used for all of them. (Default value = None)\n            input_size (int, optional): Dimension of the input. (Default value = None)\n            output_size (int, optional): Dimension of the output. (Default value = None)\n            normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n            name (str, optional): A name for identifying the model. (Default value = \"\")\n            last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n            last_activation (str, optional): Activation for the last layer (default is 'identity').\n            **kwargs \n\n        \"\"\"\n\n        self.hidden_size = None\n        assert self._check_regular_net(layers_units=layers_units), (\n            \"All the hidden layers must be equal in\" \"a Convex Dense Network.\"\n        )\n\n        super().__init__(\n            layers_units=layers_units,\n            activations=activations,\n            input_size=input_size,\n            output_size=output_size,\n            normalization=normalization,\n            name=name,\n            last_bias=last_bias,\n            last_activation=last_activation,\n            **kwargs,\n        )\n\n    def _check_regular_net(self, layers_units: list) -&gt; bool:\n\n        \"\"\"It checks if all the layers has the same number of neurons.\n\n        Args:\n            layers_units (list): \n\n        \"\"\"\n\n        mean = int(sum(layers_units) / len(layers_units))\n        self.hidden_size = mean\n\n        if len([True for j in layers_units if j == mean]) == len(layers_units):\n            return True\n        else:\n            return False\n\n    @as_tensor\n    def forward(\n        self,\n        input_data: Union[torch.Tensor, np.ndarray] = None,\n        u: Union[torch.Tensor, np.ndarray] = None,\n        v: Union[torch.Tensor, np.ndarray] = None,\n    ) -&gt; torch.Tensor:\n\n        \"\"\"\n\n        Args:\n            input_data (Union[torch.Tensor, np.ndarray], optional): Input data to be processed using ConvexDenseNetwork. (Default value = None)\n            u (Union[torch.Tensor, np.ndarray], optional): Input generated by the first auxiliar encoder (external model). (Default value = None)\n            v (Union[torch.Tensor, np.ndarray], optional): Input generated by the second auxiliar encoder (external model). (Default value = None)\n\n        \"\"\"\n\n        input_tensor_ = input_data\n\n        # The first layer operation has no difference from the Vanilla one\n        first_output = self.activations[0](self.layers[0](input_tensor_))\n\n        input_tensor_ = first_output\n\n        layers_hidden = self.layers[1:-1]\n        activations_hidden = self.activations[1:-1]\n\n        for layer_id in range(len(layers_hidden)):\n            output_tensor_ = layers_hidden[layer_id](input_tensor_)\n            z = activations_hidden[layer_id](output_tensor_)\n            _output_tensor_ = (1 - z) * u + z * v\n\n            input_tensor_ = _output_tensor_\n\n        # The last layer operation too\n        last_output = self.activations[-1](self.layers[-1](input_tensor_))\n        output_tensor = last_output\n\n        return output_tensor\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ConvexDenseNetwork.__init__","title":"<code>__init__(layers_units=None, activations=None, input_size=None, output_size=None, normalization='bypass', name='', last_bias=True, last_activation='identity', **kwargs)</code>","text":"<p>Dense network with convex combinations in the hidden layers. This architecture is useful when combined to the Improved Version of DeepONets</p> <p>Parameters:</p> Name Type Description Default <code>layers_units</code> <code>list</code> <p>List with the number of neurons for each layer. (Default value = None)</p> <code>None</code> <code>activations</code> <code>Union[list, str]</code> <p>List of activations for each layer or a single string</p> <code>None</code> <code>input_size</code> <code>int</code> <p>Dimension of the input. (Default value = None)</p> <code>None</code> <code>output_size</code> <code>int</code> <p>Dimension of the output. (Default value = None)</p> <code>None</code> <code>normalization</code> <code>str</code> <p>Kind of normalization used between two layers. (Default value = \"bypass\")</p> <code>'bypass'</code> <code>name</code> <code>str</code> <p>A name for identifying the model. (Default value = \"\")</p> <code>''</code> <code>last_bias</code> <code>bool</code> <p>Using bias in the last layer or not. (Default value = True)</p> <code>True</code> <code>last_activation</code> <code>str</code> <p>Activation for the last layer (default is 'identity').</p> <code>'identity'</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>def __init__(\n    self,\n    layers_units: list = None,\n    activations: Union[list, str] = None,\n    input_size: int = None,\n    output_size: int = None,\n    normalization: str = \"bypass\",\n    name: str = \"\",\n    last_bias: bool = True,\n    last_activation: str = \"identity\",\n    **kwargs,\n) -&gt; None:\n\n    \"\"\"Dense network with convex combinations in the hidden layers.\n    This architecture is useful when combined to the Improved Version of DeepONets\n\n    Args:\n        layers_units (list, optional): List with the number of neurons for each layer. (Default value = None)\n        activations (Union[list, str], optional): List of activations for each layer or a single string\n        informing the activation used for all of them. (Default value = None)\n        input_size (int, optional): Dimension of the input. (Default value = None)\n        output_size (int, optional): Dimension of the output. (Default value = None)\n        normalization (str, optional): Kind of normalization used between two layers. (Default value = \"bypass\")\n        name (str, optional): A name for identifying the model. (Default value = \"\")\n        last_bias (bool, optional): Using bias in the last layer or not. (Default value = True)\n        last_activation (str, optional): Activation for the last layer (default is 'identity').\n        **kwargs \n\n    \"\"\"\n\n    self.hidden_size = None\n    assert self._check_regular_net(layers_units=layers_units), (\n        \"All the hidden layers must be equal in\" \"a Convex Dense Network.\"\n    )\n\n    super().__init__(\n        layers_units=layers_units,\n        activations=activations,\n        input_size=input_size,\n        output_size=output_size,\n        normalization=normalization,\n        name=name,\n        last_bias=last_bias,\n        last_activation=last_activation,\n        **kwargs,\n    )\n</code></pre>"},{"location":"simulai_regression/#simulai.regression.ConvexDenseNetwork.forward","title":"<code>forward(input_data=None, u=None, v=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[Tensor, ndarray]</code> <p>Input data to be processed using ConvexDenseNetwork. (Default value = None)</p> <code>None</code> <code>u</code> <code>Union[Tensor, ndarray]</code> <p>Input generated by the first auxiliar encoder (external model). (Default value = None)</p> <code>None</code> <code>v</code> <code>Union[Tensor, ndarray]</code> <p>Input generated by the second auxiliar encoder (external model). (Default value = None)</p> <code>None</code> Source code in <code>simulai/regression/_pytorch/_dense.py</code> <pre><code>@as_tensor\ndef forward(\n    self,\n    input_data: Union[torch.Tensor, np.ndarray] = None,\n    u: Union[torch.Tensor, np.ndarray] = None,\n    v: Union[torch.Tensor, np.ndarray] = None,\n) -&gt; torch.Tensor:\n\n    \"\"\"\n\n    Args:\n        input_data (Union[torch.Tensor, np.ndarray], optional): Input data to be processed using ConvexDenseNetwork. (Default value = None)\n        u (Union[torch.Tensor, np.ndarray], optional): Input generated by the first auxiliar encoder (external model). (Default value = None)\n        v (Union[torch.Tensor, np.ndarray], optional): Input generated by the second auxiliar encoder (external model). (Default value = None)\n\n    \"\"\"\n\n    input_tensor_ = input_data\n\n    # The first layer operation has no difference from the Vanilla one\n    first_output = self.activations[0](self.layers[0](input_tensor_))\n\n    input_tensor_ = first_output\n\n    layers_hidden = self.layers[1:-1]\n    activations_hidden = self.activations[1:-1]\n\n    for layer_id in range(len(layers_hidden)):\n        output_tensor_ = layers_hidden[layer_id](input_tensor_)\n        z = activations_hidden[layer_id](output_tensor_)\n        _output_tensor_ = (1 - z) * u + z * v\n\n        input_tensor_ = _output_tensor_\n\n    # The last layer operation too\n    last_output = self.activations[-1](self.layers[-1](input_tensor_))\n    output_tensor = last_output\n\n    return output_tensor\n</code></pre>"},{"location":"simulai_residuals/","title":"simulai.residuals","text":"<p>             Bases: <code>Module</code></p> <p>The SymbolicOperatorClass is a class that constructs tensor operators using symbolic expressions written in PyTorch.</p> <p>Returns:</p> Name Type Description <code>object</code> <p>An instance of the SymbolicOperatorClass.</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>class SymbolicOperator(torch.nn.Module):\n    \"\"\"The SymbolicOperatorClass is a class that constructs tensor operators using symbolic expressions written in PyTorch.\n\n\n    Returns:\n        object: An instance of the SymbolicOperatorClass.\n    \"\"\"\n\n    def __init__(\n        self,\n        expressions: List[Union[sympy.Expr, str]] = None,\n        input_vars: List[Union[sympy.Symbol, str]] = None,\n        output_vars: List[Union[sympy.Symbol, str]] = None,\n        function: callable = None,\n        gradient: callable = None,\n        keys: str = None,\n        inputs_key=None,\n        constants: dict = None,\n        trainable_parameters: dict = None,\n        external_functions: dict = dict(),\n        processing: str = \"serial\",\n        device: str = \"cpu\",\n        engine: str = \"torch\",\n        auxiliary_expressions: list = None,\n    ) -&gt; None:\n        if engine == \"torch\":\n            super(SymbolicOperator, self).__init__()\n        else:\n            pass\n\n        self.engine = importlib.import_module(engine)\n\n        self.constants = constants\n\n        if trainable_parameters is not None:\n            self.trainable_parameters = trainable_parameters\n\n        else:\n            self.trainable_parameters = dict()\n\n        self.external_functions = external_functions\n        self.processing = processing\n        self.periodic_bc_protected_key = \"periodic\"\n\n        self.protected_funcs = [\"cos\", \"sin\", \"sqrt\", \"exp\"]\n        self.protected_operators = [\"L\", \"Div\", \"Identity\", \"Kronecker\"]\n\n        self.protected_funcs_subs = self._construct_protected_functions()\n        self.protected_operators_subs = self._construct_implict_operators()\n\n        # Configuring the device to be used during the fitting process\n        if device == \"gpu\":\n            if not torch.cuda.is_available():\n                print(\"Warning: There is no GPU available, using CPU instead.\")\n                device = \"cpu\"\n            else:\n                device = \"cuda\"\n                print(\"Using GPU.\")\n        elif device == \"cpu\":\n            print(\"Using CPU.\")\n        else:\n            raise Exception(f\"The device must be cpu or gpu, but received: {device}\")\n\n        self.device = device\n\n        self.expressions = [self._parse_expression(expr=expr) for expr in expressions]\n\n        if isinstance(auxiliary_expressions, dict):\n            self.auxiliary_expressions = {\n                key: self._parse_expression(expr=expr)\n                for key, expr in auxiliary_expressions.items()\n            }\n        else:\n            self.auxiliary_expressions = auxiliary_expressions\n\n        self.input_vars = [self._parse_variable(var=var) for var in input_vars]\n        self.output_vars = [self._parse_variable(var=var) for var in output_vars]\n\n        self.input_names = [var.name for var in self.input_vars]\n        self.output_names = [var.name for var in self.output_vars]\n        self.keys = keys\n\n        if inputs_key != None:\n            self.inputs_key = self._parse_inputs_key(inputs_key=inputs_key)\n        else:\n            self.inputs_key = inputs_key\n\n        self.all_vars = self.input_vars + self.output_vars\n\n        if self.inputs_key is not None:\n            self.forward = self._forward_dict\n        else:\n            self.forward = self._forward_tensor\n\n        self.function = function\n        self.diff_symbol = D\n\n        self.output = None\n\n        self.f_expressions = list()\n        self.g_expressions = dict()\n\n        self.feed_vars = None\n\n        for name in self.output_names:\n            setattr(self, name, None)\n\n        # Defining functions for returning each variable of the regression\n        # function\n        for index, name in enumerate(self.output_names):\n            setattr(\n                self,\n                name,\n                lambda data: self.function.forward(input_data=data)[..., index][\n                    ..., None\n                ],\n            )\n\n        # If no external gradient is provided, use the core gradient evaluator\n        if gradient is None:\n            gradient_function = self.gradient\n        else:\n            gradient_function = gradient\n\n        subs = {self.diff_symbol.name: gradient_function}\n        subs.update(self.external_functions)\n        subs.update(self.protected_funcs_subs)\n\n        for expr in self.expressions:\n            if not callable(expr):\n                f_expr = sympy.lambdify(self.all_vars, expr, subs)\n            else:\n                f_expr = expr\n\n            self.f_expressions.append(f_expr)\n\n        if self.auxiliary_expressions is not None:\n            for key, expr in self.auxiliary_expressions.items():\n                if not callable(expr):\n                    g_expr = sympy.lambdify(self.all_vars, expr, subs)\n                else:\n                    g_expr = expr\n\n                self.g_expressions[key] = g_expr\n\n        # Method for executing the expressions evaluation\n        if self.processing == \"serial\":\n            self.process_expression = self._process_expression_serial\n        else:\n            raise Exception(f\"Processing case {self.processing} not supported.\")\n\n    def _construct_protected_functions(self):\n        \"\"\"This function creates a dictionary of protected functions from the engine object attribute.\n\n\n        Returns:\n            dict: A dictionary of function names and their corresponding function objects.\n        \"\"\"\n        protected_funcs = {\n            func: getattr(self.engine, func) for func in self.protected_funcs\n        }\n\n        return protected_funcs\n\n    def _construct_implict_operators(self):\n        \"\"\"This function creates a dictionary of protected operators from the operators engine module.\n\n\n        Returns:\n            dict: A dictionary of operator names and their corresponding function objects.\n        \"\"\"\n        operators_engine = importlib.import_module(\"simulai.tokens\")\n\n        protected_operators = {\n            func: getattr(operators_engine, func) for func in self.protected_operators\n        }\n\n        return protected_operators\n\n    def _parse_key_interval(self, intv:str) -&gt; List:\n\n        begin, end = intv.split(\",\")\n\n        end = int(end[:-1])\n        begin = int(begin)\n        end = int(end + 1)\n\n        return np.arange(begin, end).astype(int).tolist()\n\n    def _parse_inputs_key(self, inputs_key: str = None) -&gt; dict:\n        # Sentences separator: '|'\n        sep = \"|\"\n        # Index identifier: ':'\n        inx = \":\"\n        # Interval identifier\n        intv = \"[\"\n\n        # Removing possible spaces in the inputs_key string\n        inputs_key = inputs_key.replace(\" \", \"\")\n\n        try:\n            split_components = inputs_key.split(sep)\n        except ValueError:\n            split_components = inputs_key\n\n        keys_dict = dict()\n        for s in split_components:\n            try:\n                if len(s.split(inx)) &gt; 1:\n                    key, index = s.split(inx)\n\n                    if not key in keys_dict:\n                        keys_dict[key] = list()\n                        keys_dict[key].append(int(index))\n\n                    else:\n                        keys_dict[key].append(int(index))\n\n                elif len(s.split(intv)) &gt; 1:\n\n                    key, interval_str = s.split(intv)\n                    interval = self._parse_key_interval(interval_str)\n                    keys_dict[key] = interval\n\n                else:\n                    raise ValueError\n\n            except ValueError:\n                keys_dict[s] = -1\n\n        return keys_dict\n\n    def _collect_data_from_inputs_list(self, inputs_list: dict = None) -&gt; list:\n        data = list()\n        for k, v in self.inputs_key.items():\n            if v == -1:\n                if inputs_list[k].shape[1] == 1:\n                    data_ = [inputs_list[k]]\n                else:\n                    data_ = list(torch.split(inputs_list[k], 1, dim=1))\n            else:\n                data_ = [inputs_list[k][:, i : i + 1] for i in v]\n\n            data += data_\n\n        return data\n\n    def _parse_expression(self, expr=Union[sympy.Expr, str]) -&gt; sympy.Expr:\n        \"\"\"Parses the input expression and returns a SymPy expression.\n\n        Args:\n            expr (Union[sympy.Expr, str], optional, optional): The expression to parse, by default None. It can either be a SymPy expression or a string.\n\n        Returns:\n            sympy.Expr: The parsed SymPy expression.\n\n        Raises:\n            Exception: If the `constants` attribute is not defined, and the input expression is a string.\n\n\n        \"\"\"\n        if isinstance(expr, str):\n            try:\n                expr_ = sympify(\n                    expr, locals=self.protected_operators_subs, evaluate=False\n                )\n\n                if self.constants is not None:\n                    expr_ = expr_.subs(self.constants)\n                if self.trainable_parameters is not None:\n                    expr_ = expr_.subs(self.trainable_parameters)\n            except ValueError:\n                if self.constants is not None:\n                    _expr = expr\n                    for key, value in self.constants.items():\n                        _expr = _expr.replace(key, str(value))\n\n                    expr_ = parse_expr(_expr, evaluate=0)\n                else:\n                    raise Exception(\"It is necessary to define a constants dict.\")\n        elif callable(expr):\n            expr_ = expr\n        else:\n            if self.constants is not None:\n                expr_ = expr.subs(self.constants)\n            else:\n                expr_ = expr\n\n        return expr_\n\n    def _parse_variable(self, var=Union[sympy.Symbol, str]) -&gt; sympy.Symbol:\n        \"\"\"Parse the input variable and return a SymPy Symbol.\n\n        Args:\n            var (Union[sympy.Symbol, str], optional, optional): The input variable, either a SymPy Symbol or a string. (Default value = Union[sympy.Symbol, str])\n\n        Returns:\n            sympy.Symbol: A SymPy Symbol representing the input variable.\n\n        \"\"\"\n        if isinstance(var, str):\n            return sympy.Symbol(var)\n        else:\n            return var\n\n    def _forward_tensor(self, input_data: torch.Tensor = None) -&gt; torch.Tensor:\n        \"\"\"Forward the input tensor through the function.\n\n        Args:\n            input_data (torch.Tensor, optional): The input tensor. (Default value = None)\n\n        Returns:\n            torch.Tensor: The output tensor after forward pass.\n\n        \"\"\"\n        return self.function.forward(input_data=input_data)\n\n    def _forward_dict(self, input_data: dict = None) -&gt; torch.Tensor:\n        \"\"\"Forward the input dictionary through the function.\n\n        Args:\n            input_data (dict, optional): The input dictionary. (Default value = None)\n\n        Returns:\n            torch.Tensor: The output tensor after forward pass.\n\n        \"\"\"\n        return self.function.forward(**input_data)\n\n    def _process_expression_serial(self, feed_vars: dict = None) -&gt; List[torch.Tensor]:\n        \"\"\"Process the expression list serially using the given feed variables.\n\n        Args:\n            feed_vars (dict, optional): The feed variables. (Default value = None)\n\n        Returns:\n            List[torch.Tensor]: A list of tensors after evaluating the expressions serially.\n\n        \"\"\"\n        return [f(**feed_vars).to(self.device) for f in self.f_expressions]\n\n    def _process_expression_individual(\n        self, index: int = None, feed_vars: dict = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates a single expression specified by index from the f_expressions list with given feed variables.\n\n        Args:\n            index (int, optional): Index of the expression to be evaluated, by default None\n            feed_vars (dict, optional): Dictionary of feed variables, by default None\n\n        Returns:\n            torch.Tensor: Result of evaluating the specified expression with given feed variables\n\n        \"\"\"\n        return self.f_expressions[index](**feed_vars).to(self.device)\n\n    def __call__(\n        self, inputs_data: Union[np.ndarray, dict] = None\n    ) -&gt; List[torch.Tensor]:\n        \"\"\"Evaluate the symbolic expression.\n\n        This function takes either a numpy array or a dictionary of numpy arrays as input.\n\n        Args:\n            inputs_data (Union[np.ndarray, dict], optional): Union (Default value = None)\n\n        Returns:\n            List[torch.Tensor]: List[torch.Tensor]: A list of tensors containing the evaluated expressions.\n\n            Raises:\n\n        Raises:\n            does: not match with the inputs_key attribute\n\n        \"\"\"\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        inputs_list = constructor(input_data=inputs_data, device=self.device)\n\n        output = self.forward(input_data=inputs_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs = {key: value for key, value in zip(self.output_names, outputs_list)}\n\n        if type(inputs_list) is list:\n            inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n\n        elif type(inputs_list) is dict:\n            assert (\n                self.inputs_key is not None\n            ), \"If inputs_list is dict, \\\n                it is necessary to provide\\\n                a key.\"\n\n            inputs_list = self._collect_data_from_inputs_list(inputs_list=inputs_list)\n\n            inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n        else:\n            raise Exception(\n                f\"Format {type(inputs_list)} not supported \\\n                            for inputs_list\"\n            )\n\n        feed_vars = {**outputs, **inputs}\n\n        # It returns a list of tensors containing the expressions\n        # evaluated over a domain\n        return self.process_expression(feed_vars=feed_vars)\n\n    def eval_expression(self, key, inputs_list):\n        \"\"\"This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.\n\n        Args:\n            key (str): the key used to retrieve the expression from the 'g_expressions' attribute\n            inputs_list (list): either a list of arrays, an np.ndarray, or a dict containing the inputs to the function\n\n        Returns:\n            the result of evaluating the expression using the inputs.: \n\n        \"\"\"\n\n        try:\n            g = self.g_expressions.get(key)\n        except:\n            raise Exception(f\"The expression {key} does not exist.\")\n\n        # Periodic boundary conditions\n        if self.periodic_bc_protected_key in key:\n            assert isinstance(inputs_list, list), (\n                \"When a periodic boundary expression is used,\"\n                \" the input must be a list of arrays.\"\n            )\n\n            # Lower bound\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            tensors_list = constructor(input_data=inputs_list[0], device=self.device)\n\n            inputs_L = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n            output = self.function.forward(input_data=tensors_list)\n\n            output = output.to(self.device)  # TODO Check if it is necessary\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs_L = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            feed_vars_L = {**inputs_L, **outputs_L}\n\n            # Upper bound\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            tensors_list = constructor(input_data=inputs_list[-1], device=self.device)\n\n            inputs_U = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n            output = self.function.forward(input_data=tensors_list)\n\n            output = output.to(self.device)  # TODO Check if it is necessary\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs_U = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            feed_vars_U = {**inputs_U, **outputs_U}\n\n            # Evaluating the boundaries equality\n            return g(**feed_vars_L) - g(**feed_vars_U)\n\n        # The non-periodic cases\n        else:\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            inputs_list = constructor(input_data=inputs_list, device=self.device)\n\n            output = self.function.forward(input_data=inputs_list)\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            if type(inputs_list) is list:\n                inputs = {\n                    key: value for key, value in zip(self.input_names, inputs_list)\n                }\n\n            elif type(inputs_list) is np.ndarray:\n                arrays_list = np.split(inputs_list, inputs_list.shape[1], axis=1)\n                tensors_list = [torch.from_numpy(arr) for arr in arrays_list]\n\n                for t in tensors_list:\n                    t.requires_grad = True\n\n                inputs = {\n                    key: value for key, value in zip(self.input_names, tensors_list)\n                }\n\n            elif type(inputs_list) is dict:\n                assert (\n                    self.inputs_key is not None\n                ), \"If inputs_list is dict, \\\n                                                     it is necessary to provide\\\n                                                     a key.\"\n\n                inputs = {\n                    key: value\n                    for key, value in zip(\n                        self.input_names, inputs_list[self.inputs_key]\n                    )\n                }\n\n            else:\n                raise Exception(\n                    f\"Format {type(inputs_list)} not supported \\\n                                for inputs_list\"\n                )\n\n            feed_vars = {**inputs, **outputs}\n\n            return g(**feed_vars)\n\n    @staticmethod\n    def gradient(feature, param):\n        \"\"\"Calculates the gradient of the given feature with respect to the given parameter.\n\n        Args:\n            feature (torch.Tensor): Tensor with the input feature.\n            param (torch.Tensor): Tensor with the parameter to calculate the gradient with respect to.\n\n        Returns:\n            torch.Tensor: Tensor with the gradient of the feature with respect to the given parameter.\n        Example::\n\n            &gt;&gt;&gt; feature = torch.tensor([1, 2, 3], dtype=torch.float32)\n            &gt;&gt;&gt; param = torch.tensor([2, 3, 4], dtype=torch.float32)\n            &gt;&gt;&gt; gradient(feature, param)\n            tensor([1., 1., 1.], grad_fn=&lt;AddBackward0&gt;)\n        \"\"\"\n        grad_ = grad(\n            feature,\n            param,\n            grad_outputs=torch.ones_like(feature),\n            create_graph=True,\n            allow_unused=True,\n            retain_graph=True,\n        )\n\n        return grad_[0]\n\n    def jac(self, inputs):\n        \"\"\"Calculates the Jacobian of the forward function of the model with respect to its inputs.\n\n        Args:\n            inputs (torch.Tensor): Tensor with the input data to the forward function.\n\n        Returns:\n            torch.Tensor: Tensor with the Jacobian of the forward function with respect to its inputs.\n        Example::\n\n            &gt;&gt;&gt; inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32)\n            &gt;&gt;&gt; jac(inputs)\n            tensor([[1., 1., 1.],\n                    [1., 1., 1.]], grad_fn=&lt;MulBackward0&gt;)\n        \"\"\"\n\n        def inner(inputs):\n            return self.forward(input_data=inputs)\n\n        return jacobian(inner, inputs)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.__call__","title":"<code>__call__(inputs_data=None)</code>","text":"<p>Evaluate the symbolic expression.</p> <p>This function takes either a numpy array or a dictionary of numpy arrays as input.</p> <p>Parameters:</p> Name Type Description Default <code>inputs_data</code> <code>Union[ndarray, dict]</code> <p>Union (Default value = None)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>List[Tensor]</code> <p>List[torch.Tensor]: List[torch.Tensor]: A list of tensors containing the evaluated expressions.</p> <code>Raises</code> <code>List[Tensor]</code> <p>Raises:</p> Type Description <code>does</code> <p>not match with the inputs_key attribute</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def __call__(\n    self, inputs_data: Union[np.ndarray, dict] = None\n) -&gt; List[torch.Tensor]:\n    \"\"\"Evaluate the symbolic expression.\n\n    This function takes either a numpy array or a dictionary of numpy arrays as input.\n\n    Args:\n        inputs_data (Union[np.ndarray, dict], optional): Union (Default value = None)\n\n    Returns:\n        List[torch.Tensor]: List[torch.Tensor]: A list of tensors containing the evaluated expressions.\n\n        Raises:\n\n    Raises:\n        does: not match with the inputs_key attribute\n\n    \"\"\"\n    constructor = MakeTensor(\n        input_names=self.input_names, output_names=self.output_names\n    )\n\n    inputs_list = constructor(input_data=inputs_data, device=self.device)\n\n    output = self.forward(input_data=inputs_list)\n\n    output = output.to(self.device)  # TODO Check if it is necessary\n\n    outputs_list = torch.split(output, 1, dim=-1)\n\n    outputs = {key: value for key, value in zip(self.output_names, outputs_list)}\n\n    if type(inputs_list) is list:\n        inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n\n    elif type(inputs_list) is dict:\n        assert (\n            self.inputs_key is not None\n        ), \"If inputs_list is dict, \\\n            it is necessary to provide\\\n            a key.\"\n\n        inputs_list = self._collect_data_from_inputs_list(inputs_list=inputs_list)\n\n        inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n    else:\n        raise Exception(\n            f\"Format {type(inputs_list)} not supported \\\n                        for inputs_list\"\n        )\n\n    feed_vars = {**outputs, **inputs}\n\n    # It returns a list of tensors containing the expressions\n    # evaluated over a domain\n    return self.process_expression(feed_vars=feed_vars)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.eval_expression","title":"<code>eval_expression(key, inputs_list)</code>","text":"<p>This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>the key used to retrieve the expression from the 'g_expressions' attribute</p> required <code>inputs_list</code> <code>list</code> <p>either a list of arrays, an np.ndarray, or a dict containing the inputs to the function</p> required <p>Returns:</p> Type Description <p>the result of evaluating the expression using the inputs.:</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def eval_expression(self, key, inputs_list):\n    \"\"\"This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.\n\n    Args:\n        key (str): the key used to retrieve the expression from the 'g_expressions' attribute\n        inputs_list (list): either a list of arrays, an np.ndarray, or a dict containing the inputs to the function\n\n    Returns:\n        the result of evaluating the expression using the inputs.: \n\n    \"\"\"\n\n    try:\n        g = self.g_expressions.get(key)\n    except:\n        raise Exception(f\"The expression {key} does not exist.\")\n\n    # Periodic boundary conditions\n    if self.periodic_bc_protected_key in key:\n        assert isinstance(inputs_list, list), (\n            \"When a periodic boundary expression is used,\"\n            \" the input must be a list of arrays.\"\n        )\n\n        # Lower bound\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        tensors_list = constructor(input_data=inputs_list[0], device=self.device)\n\n        inputs_L = {\n            key: value for key, value in zip(self.input_names, tensors_list)\n        }\n\n        output = self.function.forward(input_data=tensors_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs_L = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        feed_vars_L = {**inputs_L, **outputs_L}\n\n        # Upper bound\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        tensors_list = constructor(input_data=inputs_list[-1], device=self.device)\n\n        inputs_U = {\n            key: value for key, value in zip(self.input_names, tensors_list)\n        }\n\n        output = self.function.forward(input_data=tensors_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs_U = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        feed_vars_U = {**inputs_U, **outputs_U}\n\n        # Evaluating the boundaries equality\n        return g(**feed_vars_L) - g(**feed_vars_U)\n\n    # The non-periodic cases\n    else:\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        inputs_list = constructor(input_data=inputs_list, device=self.device)\n\n        output = self.function.forward(input_data=inputs_list)\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        if type(inputs_list) is list:\n            inputs = {\n                key: value for key, value in zip(self.input_names, inputs_list)\n            }\n\n        elif type(inputs_list) is np.ndarray:\n            arrays_list = np.split(inputs_list, inputs_list.shape[1], axis=1)\n            tensors_list = [torch.from_numpy(arr) for arr in arrays_list]\n\n            for t in tensors_list:\n                t.requires_grad = True\n\n            inputs = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n        elif type(inputs_list) is dict:\n            assert (\n                self.inputs_key is not None\n            ), \"If inputs_list is dict, \\\n                                                 it is necessary to provide\\\n                                                 a key.\"\n\n            inputs = {\n                key: value\n                for key, value in zip(\n                    self.input_names, inputs_list[self.inputs_key]\n                )\n            }\n\n        else:\n            raise Exception(\n                f\"Format {type(inputs_list)} not supported \\\n                            for inputs_list\"\n            )\n\n        feed_vars = {**inputs, **outputs}\n\n        return g(**feed_vars)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.gradient","title":"<code>gradient(feature, param)</code>  <code>staticmethod</code>","text":"<p>Calculates the gradient of the given feature with respect to the given parameter.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Tensor</code> <p>Tensor with the input feature.</p> required <code>param</code> <code>Tensor</code> <p>Tensor with the parameter to calculate the gradient with respect to.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Tensor with the gradient of the feature with respect to the given parameter.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; feature = torch.tensor([1, 2, 3], dtype=torch.float32)\n&gt;&gt;&gt; param = torch.tensor([2, 3, 4], dtype=torch.float32)\n&gt;&gt;&gt; gradient(feature, param)\ntensor([1., 1., 1.], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>@staticmethod\ndef gradient(feature, param):\n    \"\"\"Calculates the gradient of the given feature with respect to the given parameter.\n\n    Args:\n        feature (torch.Tensor): Tensor with the input feature.\n        param (torch.Tensor): Tensor with the parameter to calculate the gradient with respect to.\n\n    Returns:\n        torch.Tensor: Tensor with the gradient of the feature with respect to the given parameter.\n    Example::\n\n        &gt;&gt;&gt; feature = torch.tensor([1, 2, 3], dtype=torch.float32)\n        &gt;&gt;&gt; param = torch.tensor([2, 3, 4], dtype=torch.float32)\n        &gt;&gt;&gt; gradient(feature, param)\n        tensor([1., 1., 1.], grad_fn=&lt;AddBackward0&gt;)\n    \"\"\"\n    grad_ = grad(\n        feature,\n        param,\n        grad_outputs=torch.ones_like(feature),\n        create_graph=True,\n        allow_unused=True,\n        retain_graph=True,\n    )\n\n    return grad_[0]\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.jac","title":"<code>jac(inputs)</code>","text":"<p>Calculates the Jacobian of the forward function of the model with respect to its inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Tensor with the input data to the forward function.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Tensor with the Jacobian of the forward function with respect to its inputs.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32)\n&gt;&gt;&gt; jac(inputs)\ntensor([[1., 1., 1.],\n        [1., 1., 1.]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def jac(self, inputs):\n    \"\"\"Calculates the Jacobian of the forward function of the model with respect to its inputs.\n\n    Args:\n        inputs (torch.Tensor): Tensor with the input data to the forward function.\n\n    Returns:\n        torch.Tensor: Tensor with the Jacobian of the forward function with respect to its inputs.\n    Example::\n\n        &gt;&gt;&gt; inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32)\n        &gt;&gt;&gt; jac(inputs)\n        tensor([[1., 1., 1.],\n                [1., 1., 1.]], grad_fn=&lt;MulBackward0&gt;)\n    \"\"\"\n\n    def inner(inputs):\n        return self.forward(input_data=inputs)\n\n    return jacobian(inner, inputs)\n</code></pre>"}]}