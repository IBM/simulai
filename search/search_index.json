{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SimulAI","text":"<p>{width=\"500px\"}</p> <p>An extensible Python package with data-driven pipelines for physics-informed machine learning.</p> <p>The SimulAI toolkit provides easy access to state-of-the-art models and algorithms for physics-informed machine learning. Currently, it includes the following methods described in the literature:</p> <ul> <li>Physics-Informed Neural Networks (PINNs)</li> <li>Deep Operator Networks (DeepONets)</li> <li>Variational Encoder-Decoders (VED)</li> <li>Operator Inference (OpInf)</li> <li>Koopman Autoencoders (experimental)</li> <li>Echo State Networks (experimental GPU support)</li> <li>Transformers</li> <li>U-Nets</li> </ul> <p>In addition to the methods above, many more techniques for model reduction and regularization are included in SimulAI. See documentation.</p>"},{"location":"#installing","title":"Installing","text":"<p>Python version requirements: 3.9 \\&lt;= python \\&lt;= 3.11</p>"},{"location":"#using-pip","title":"Using pip","text":"<p>For installing the most recent stable version from PyPI:</p> <pre><code>pip install simulai-toolkit\n</code></pre> <p>For installing from the latest commit sent to GitHub (just for testing and developing purposes):</p> <pre><code>pip uninstall simulai_toolkit\npip install -U git+https://github.com/IBM/simulai@$(git ls-remote git@github.com:IBM/simulai.git  | head -1 | awk '{print $1;}')#egg=simulai_toolkit\n</code></pre>"},{"location":"#contributing-code-to-simulai","title":"Contributing code to SimulAI","text":"<p>If you are interested in directly contributing to this project, please see CONTRIBUTING.</p>"},{"location":"#using-mpi","title":"Using MPI","text":"<p>Some methods implemented on SimulAI support multiprocessing with MPI.</p> <p>In order to use it, you will need a valid MPI distribution, e.g. MPICH, OpenMPI. As an example, you can use <code>conda</code> to install MPICH as follows:</p> <pre><code>conda install -c conda-forge mpich gcc\n</code></pre>"},{"location":"#issues-with-macos","title":"Issues with macOS","text":"<p>If you have problems installing <code>gcc</code> using the command above, we recommend you to install it using Homebrew.</p>"},{"location":"#using-tensorboard","title":"Using Tensorboard","text":"<p>Tensorboard is supported for monitoring neural network training tasks. For a tutorial about how to set it see this example.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Please, refer to the SimulAI API documentation before using the toolkit.</p>"},{"location":"#examples","title":"Examples","text":"<p>Additionally, you can refer to examples in the respective folder.</p>"},{"location":"#license","title":"License","text":"<p>This software is licensed under Apache license 2.0. See LICENSE.</p>"},{"location":"#contributing-code-to-simulai_1","title":"Contributing code to SimulAI","text":"<p>If you are interested in directly contributing to this project, please see CONTRIBUTING.</p>"},{"location":"#how-to-cite-simulai-in-your-publications","title":"How to cite SimulAI in your publications","text":"<p>If you find SimulAI to be useful, please consider citing it in your published work:</p> <pre><code>@misc{simulai,\n  author = {IBM},\n  title = {SimulAI Toolkit},\n  subtitle = {A Python package with data-driven pipelines for physics-informed machine learning},\n  note = \"https://github.com/IBM/simulai\",\n  doi = {10.5281/zenodo.7351516},\n  year = {2022},\n}\n</code></pre> <p>or, via Zenodo:</p> <pre><code>@software{joao_lucas_de_sousa_almeida_2023_7566603,\n      author       = {Jo\u00e3o Lucas de Sousa Almeida and\n                      Leonardo Martins and\n                      Tar\u0131k Kaan Ko\u00e7},\n      title        = {IBM/simulai: 0.99.13},\n      month        = jan,\n      year         = 2023,\n      publisher    = {Zenodo},\n      version      = {0.99.25},\n      doi          = {10.5281/zenodo.7566603},\n      url          = {https://doi.org/10.5281/zenodo.7566603}\n    }\n</code></pre>"},{"location":"#publications","title":"Publications","text":"<p>Jo\u00e3o Lucas de Sousa Almeida, Pedro Roberto Barbosa Rocha, Allan Moreira de Carvalho and Alberto Costa Nogueira Jr. A coupled Variational Encoder-Decoder - DeepONet surrogate model for the Rayleigh-B\u00e9nard convection problem. In When Machine Learning meets Dynamical Systems: Theory and Applications, AAAI, 2023.</p> <p>Jo\u00e3o Lucas S. Almeida, Arthur C. Pires, Klaus F. V. Cid, and Alberto C. Nogueira Jr. Non-intrusive operator inference for chaotic systems. IEEE Transactions on Artificial Intelligence, pages 1--14, 2022.</p> <p>Pedro Roberto Barbosa Rocha, Marcos Sebasti\u00e3o de Paula Gomes, Allan Moreira de Carvalho, Jo\u00e3o Lucas de Sousa Almeida and Alberto Costa Nogueira Jr. Data-driven reduced-order model for atmospheric CO2 dispersion. In AAAI 2022 Fall Symposium: The Role of AI in Responding to Climate Challenges, 2022.</p> <p>Pedro Roberto Barbosa Rocha, Jo\u00e3o Lucas de Sousa Almeida, Marcos Sebasti\u00e3o de Paula Gomes, Alberto Costa Nogueira, Reduced-order modeling of the two-dimensional Rayleigh--B\u00e9nard convection flow through a non-intrusive operator inference, Engineering Applications of Artificial Intelligence, Volume 126, Part B, 2023, 106923, ISSN 0952-1976, https://doi.org/10.1016/j.engappai.2023.106923. (https://www.sciencedirect.com/science/article/pii/S0952197623011077)</p>"},{"location":"#references","title":"References","text":"<p>Jaeger, H., Haas, H. (2004). \\\"Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication,\\\" Science, 304 (5667): 78--80.  \\&lt;https://doi.org/10.1126/science.1091277&gt;`_. <p>Lu, L., Jin, P., Pang, G., Zhang, Z., Karniadakis, G. E. (2021). \\\"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,\\\" Nature Machine Intelligence, 3 (1): 218--229. ISSN: 2522-5839.  \\&lt;https://doi.org/10.1038/s42256-021-00302-5&gt;`_. <p>Eivazi, H., Le Clainche, S., Hoyas, S., Vinuesa, R. (2022) \\\"Towards extraction of orthogonal and parsimonious non-linear modes from turbulent flows\\\" Expert Systems with Applications, 202. ISSN: 0957-4174.  \\&lt;https://doi.org/10.1016/j.eswa.2022.117038&gt;`_. <p>Raissi, M., Perdikaris, P., Karniadakis, G. E. (2019). \\\"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,\\\" Journal of Computational Physics, 378 (1): 686-707. ISSN: 0021-9991.  \\&lt;https://doi.org/10.1016/j.jcp.2018.10.045&gt;`_. <p>Lusch, B., Kutz, J. N., Brunton, S.L. (2018). \\\"Deep learning for universal linear embeddings of nonlinear dynamics,\\\" Nature Communications, 9: 4950. ISSN: 2041-1723.  \\&lt;https://doi.org/10.1038/s41467-018-07210-0&gt;`_. <p>McQuarrie, S., Huang, C. and Willcox, K. (2021). \\\"Data-driven reduced-order models via regularized operator inference for a single-injector combustion process,\\\" Journal of the Royal Society of New Zealand, 51(2): 194-211. ISSN: 0303-6758.  \\&lt;https://doi.org/10.1080/03036758.2020.1863237&gt;`_."},{"location":"simulai_models/","title":"simulai.models","text":""},{"location":"simulai_models/#transformer","title":"Transformer","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>class Transformer(NetworkTemplate):\n\n    def __init__(self, num_heads_encoder:int=1,\n                       num_heads_decoder:int=1,\n                       embed_dim_encoder:int=Union[int, Tuple],\n                       embed_dim_decoder:int=Union[int, Tuple],\n                       encoder_activation: Union[str, torch.nn.Module]='relu',\n                       decoder_activation: Union[str, torch.nn.Module]='relu',\n                       encoder_mlp_layer_config:dict=None,\n                       decoder_mlp_layer_config:dict=None,\n                       number_of_encoders:int=1,\n                       number_of_decoders:int=1) -&gt; None:\n        \"\"\"\n        A classical encoder-decoder transformer:\n\n            U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n            (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n\n        Parameters\n        ----------\n        num_heads_encoder : int\n            The number of heads for the self-attention layer of the encoder. \n        num_heads_decoder :int\n            The number of heads for the self-attention layer of the decoder.\n        embed_dim_encoder : int\n            The dimension of the embedding for the encoder.\n        embed_dim_decoder : int \n            The dimension of the embedding for the decoder. \n        encoder_activation : Union[str, torch.nn.Module]\n            The activation to be used in all the encoder layers.\n        decoder_activation : Union[str, torch.nn.Module]\n            The activation to be used in all the decoder layers.\n        encoder_mlp_layer_config : dict\n            A configuration dictionary to instantiate the encoder MLP layer.weights\n        decoder_mlp_layer_config : dict\n            A configuration dictionary to instantiate the encoder MLP layer.weights\n        number_of_encoders : int\n            The number of encoders to be used.\n        number_of_decoders : int\n            The number of decoders to be used.\n\n        \"\"\"\n\n        super(Transformer, self).__init__()\n\n        self.num_heads_encoder = num_heads_encoder\n        self.num_heads_decoder = num_heads_decoder\n\n        self.embed_dim_encoder = embed_dim_encoder\n        self.embed_dim_decoder = embed_dim_decoder\n\n        self.encoder_mlp_layer_dict = encoder_mlp_layer_config\n        self.decoder_mlp_layer_dict = decoder_mlp_layer_config\n\n        self.number_of_encoders = number_of_encoders\n        self.number_of_decoders = number_of_encoders\n\n        self.encoder_activation = encoder_activation\n        self.decoder_activation = decoder_activation\n\n        self.encoder_mlp_layers_list = list()\n        self.decoder_mlp_layers_list = list()\n\n        # Creating independent copies for the MLP layers which will be used \n        # by the multiple encoders/decoders.\n        for e in range(self.number_of_encoders):\n            self.encoder_mlp_layers_list.append(\n                                                    DenseNetwork(**self.encoder_mlp_layer_dict)\n                                               )\n\n        for d in range(self.number_of_decoders):\n            self.decoder_mlp_layers_list.append(\n\n                                                    DenseNetwork(**self.decoder_mlp_layer_dict)\n                                               )\n\n        # Defining the encoder architecture\n        self.EncoderStage = torch.nn.Sequential(\n                                *[BasicEncoder(num_heads=self.num_heads_encoder,\n                                              activation=self.encoder_activation,\n                                              mlp_layer=self.encoder_mlp_layers_list[e],\n                                              embed_dim=self.embed_dim_encoder) for e in range(self.number_of_encoders)]\n                            )\n\n        # Defining the decoder architecture\n        self.DecoderStage =  torch.nn.ModuleList([BasicDecoder(num_heads=self.num_heads_decoder,\n                                                               activation=self.decoder_activation,\n                                                               mlp_layer=self.decoder_mlp_layers_list[d],\n                                                               embed_dim=self.embed_dim_decoder) for d in range(self.number_of_decoders)\n                              ])\n\n\n        self.weights = list()\n\n        for e, encoder_e in enumerate(self.EncoderStage):\n            self.weights += encoder_e.weights\n            self.add_module(f\"encoder_{e}\", encoder_e)\n\n        for d, decoder_d in enumerate(self.DecoderStage):\n            self.weights += decoder_d.weights\n            self.add_module(f\"decoder_{d}\", decoder_d)\n\n    @as_tensor\n    def forward(self, input_data: Union[torch.Tensor, np.ndarray] = None) -&gt; torch.Tensor:\n\n        \"\"\"\n\n        Parameters\n        ----------\n        input_data : Union[torch.Tensor, np.ndarray] \n            The input dataset.\n\n        Returns\n        -------\n        torch.Tensor\n            The transformer output.\n        \"\"\"\n\n        encoder_output = self.EncoderStage(input_data)\n\n        current_input = input_data\n        for decoder in self.DecoderStage:\n            output = decoder(input_data=current_input, encoder_output=encoder_output)\n            current_input = output\n\n        return output\n\n    def summary(self):\n        \"\"\"\n         It prints a general view of the architecture.\n        \"\"\"\n\n        print(self)\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.__init__","title":"<code>__init__(num_heads_encoder=1, num_heads_decoder=1, embed_dim_encoder=Union[int, Tuple], embed_dim_decoder=Union[int, Tuple], encoder_activation='relu', decoder_activation='relu', encoder_mlp_layer_config=None, decoder_mlp_layer_config=None, number_of_encoders=1, number_of_decoders=1)</code>","text":"<p>A classical encoder-decoder transformer:</p> <pre><code>U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n(u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.__init__--parameters","title":"Parameters","text":"<p>num_heads_encoder : int     The number of heads for the self-attention layer of the encoder.  num_heads_decoder :int     The number of heads for the self-attention layer of the decoder. embed_dim_encoder : int     The dimension of the embedding for the encoder. embed_dim_decoder : int      The dimension of the embedding for the decoder.  encoder_activation : Union[str, torch.nn.Module]     The activation to be used in all the encoder layers. decoder_activation : Union[str, torch.nn.Module]     The activation to be used in all the decoder layers. encoder_mlp_layer_config : dict     A configuration dictionary to instantiate the encoder MLP layer.weights decoder_mlp_layer_config : dict     A configuration dictionary to instantiate the encoder MLP layer.weights number_of_encoders : int     The number of encoders to be used. number_of_decoders : int     The number of decoders to be used.</p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>def __init__(self, num_heads_encoder:int=1,\n                   num_heads_decoder:int=1,\n                   embed_dim_encoder:int=Union[int, Tuple],\n                   embed_dim_decoder:int=Union[int, Tuple],\n                   encoder_activation: Union[str, torch.nn.Module]='relu',\n                   decoder_activation: Union[str, torch.nn.Module]='relu',\n                   encoder_mlp_layer_config:dict=None,\n                   decoder_mlp_layer_config:dict=None,\n                   number_of_encoders:int=1,\n                   number_of_decoders:int=1) -&gt; None:\n    \"\"\"\n    A classical encoder-decoder transformer:\n\n        U -&gt; ( Encoder_1 -&gt; Encoder_2 -&gt; ... -&gt; Encoder_N ) -&gt; u_e\n\n        (u_e, U) -&gt; ( Decoder_1 -&gt; Decoder_2 -&gt; ... Decoder_N ) -&gt; V\n\n    Parameters\n    ----------\n    num_heads_encoder : int\n        The number of heads for the self-attention layer of the encoder. \n    num_heads_decoder :int\n        The number of heads for the self-attention layer of the decoder.\n    embed_dim_encoder : int\n        The dimension of the embedding for the encoder.\n    embed_dim_decoder : int \n        The dimension of the embedding for the decoder. \n    encoder_activation : Union[str, torch.nn.Module]\n        The activation to be used in all the encoder layers.\n    decoder_activation : Union[str, torch.nn.Module]\n        The activation to be used in all the decoder layers.\n    encoder_mlp_layer_config : dict\n        A configuration dictionary to instantiate the encoder MLP layer.weights\n    decoder_mlp_layer_config : dict\n        A configuration dictionary to instantiate the encoder MLP layer.weights\n    number_of_encoders : int\n        The number of encoders to be used.\n    number_of_decoders : int\n        The number of decoders to be used.\n\n    \"\"\"\n\n    super(Transformer, self).__init__()\n\n    self.num_heads_encoder = num_heads_encoder\n    self.num_heads_decoder = num_heads_decoder\n\n    self.embed_dim_encoder = embed_dim_encoder\n    self.embed_dim_decoder = embed_dim_decoder\n\n    self.encoder_mlp_layer_dict = encoder_mlp_layer_config\n    self.decoder_mlp_layer_dict = decoder_mlp_layer_config\n\n    self.number_of_encoders = number_of_encoders\n    self.number_of_decoders = number_of_encoders\n\n    self.encoder_activation = encoder_activation\n    self.decoder_activation = decoder_activation\n\n    self.encoder_mlp_layers_list = list()\n    self.decoder_mlp_layers_list = list()\n\n    # Creating independent copies for the MLP layers which will be used \n    # by the multiple encoders/decoders.\n    for e in range(self.number_of_encoders):\n        self.encoder_mlp_layers_list.append(\n                                                DenseNetwork(**self.encoder_mlp_layer_dict)\n                                           )\n\n    for d in range(self.number_of_decoders):\n        self.decoder_mlp_layers_list.append(\n\n                                                DenseNetwork(**self.decoder_mlp_layer_dict)\n                                           )\n\n    # Defining the encoder architecture\n    self.EncoderStage = torch.nn.Sequential(\n                            *[BasicEncoder(num_heads=self.num_heads_encoder,\n                                          activation=self.encoder_activation,\n                                          mlp_layer=self.encoder_mlp_layers_list[e],\n                                          embed_dim=self.embed_dim_encoder) for e in range(self.number_of_encoders)]\n                        )\n\n    # Defining the decoder architecture\n    self.DecoderStage =  torch.nn.ModuleList([BasicDecoder(num_heads=self.num_heads_decoder,\n                                                           activation=self.decoder_activation,\n                                                           mlp_layer=self.decoder_mlp_layers_list[d],\n                                                           embed_dim=self.embed_dim_decoder) for d in range(self.number_of_decoders)\n                          ])\n\n\n    self.weights = list()\n\n    for e, encoder_e in enumerate(self.EncoderStage):\n        self.weights += encoder_e.weights\n        self.add_module(f\"encoder_{e}\", encoder_e)\n\n    for d, decoder_d in enumerate(self.DecoderStage):\n        self.weights += decoder_d.weights\n        self.add_module(f\"decoder_{d}\", decoder_d)\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.forward","title":"<code>forward(input_data=None)</code>","text":""},{"location":"simulai_models/#simulai.models.Transformer.forward--parameters","title":"Parameters","text":"<p>input_data : Union[torch.Tensor, np.ndarray]      The input dataset.</p>"},{"location":"simulai_models/#simulai.models.Transformer.forward--returns","title":"Returns","text":"<p>torch.Tensor     The transformer output.</p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>@as_tensor\ndef forward(self, input_data: Union[torch.Tensor, np.ndarray] = None) -&gt; torch.Tensor:\n\n    \"\"\"\n\n    Parameters\n    ----------\n    input_data : Union[torch.Tensor, np.ndarray] \n        The input dataset.\n\n    Returns\n    -------\n    torch.Tensor\n        The transformer output.\n    \"\"\"\n\n    encoder_output = self.EncoderStage(input_data)\n\n    current_input = input_data\n    for decoder in self.DecoderStage:\n        output = decoder(input_data=current_input, encoder_output=encoder_output)\n        current_input = output\n\n    return output\n</code></pre>"},{"location":"simulai_models/#simulai.models.Transformer.summary","title":"<code>summary()</code>","text":"<p>It prints a general view of the architecture.</p> Source code in <code>simulai/models/_pytorch_models/_transformer.py</code> <pre><code>def summary(self):\n    \"\"\"\n     It prints a general view of the architecture.\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"simulai_models/#u-net","title":"U-Net","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>class UNet(NetworkTemplate):\n\n\n    def __init__(self, layers_config:Dict=None,\n                 intermediary_outputs_indices:List[int]=None,\n                 intermediary_inputs_indices:List[int]=None,\n                 encoder_extra_args:Dict=dict(),\n                 decoder_extra_args:Dict=dict()) -&gt; None:\n        \"\"\"\n        U-Net. \n\n        Parameters\n        ----------\n        layers_config : Dict\n            A dictionary containing the complete configuration for the \n            U-Net encoder and decoder.\n        intermediary_outputs_indices : List[int]\n            A list of indices for indicating the encoder outputs.\n        intermediary_inputs_indices : List[int]\n            A list of indices for indicating the decoder inputs.\n        encoder_extra_args : Dict\n            A dictionary containing extra arguments for the encoder.\n        decoder_extra_args : Dict\n            A dictionary containing extra arguments for the decoder. \n\n        \"\"\"\n\n        super(UNet, self).__init__()\n\n        self.layers_config = layers_config\n        self.intermediary_outputs_indices = intermediary_outputs_indices\n        self.intermediary_inputs_indices = intermediary_inputs_indices\n\n        self.layers_config_encoder = self.layers_config[\"encoder\"] \n        self.layers_config_decoder = self.layers_config[\"decoder\"] \n\n        self.encoder_activations = self.layers_config[\"encoder_activations\"]\n        self.decoder_activations = self.layers_config[\"decoder_activations\"]\n\n        self.encoder_horizontal_outputs = dict()\n\n        # Configuring the encoder\n        encoder_type = self.layers_config_encoder.get(\"type\")\n        layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n        if encoder_type == \"cnn\":\n            self.encoder = CNNUnetEncoder(layers=self.layers_config_encoder[\"architecture\"],\n                                          activations=self.encoder_activations,\n                                          intermediary_outputs_indices=self.intermediary_outputs_indices,\n                                          case=\"2d\", name=\"encoder\",\n                                          **encoder_extra_args)\n        else:\n            raise Exception(f\"Option {encoder_type} is not available.\")\n\n         # Configuring the decoder\n        decoder_type = self.layers_config_decoder.get(\"type\")\n        layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n        if encoder_type == \"cnn\":\n            self.decoder = CNNUnetDecoder(layers=self.layers_config_decoder[\"architecture\"],\n                                          activations=self.decoder_activations,\n                                          intermediary_inputs_indices=self.intermediary_inputs_indices,\n                                          case=\"2d\", name=\"decoder\",\n                                          **decoder_extra_args)\n        else:\n            raise Exception(f\"Option {encoder_type} is not available.\")\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n    @as_tensor\n    def forward(self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        The U-Net forward method. \n\n        Parameters\n        ----------\n        input_data : Union[torch.Tensor, np.ndarray],\n            A dataset to be inputted in the CNN U-Net encoder.\n\n        Returns\n        -------\n        torch.Tensor\n            The U-Net output. \n        \"\"\"\n\n        encoder_main_output, encoder_intermediary_outputs = self.encoder(input_data=input_data)\n        output = self.decoder(input_data = encoder_main_output,\n                              intermediary_encoder_outputs=encoder_intermediary_outputs)\n\n        return output\n\n    def summary(self):\n        \"\"\"\n        It shows a general view of the architecture.\n        \"\"\"\n\n        print(self)\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.__init__","title":"<code>__init__(layers_config=None, intermediary_outputs_indices=None, intermediary_inputs_indices=None, encoder_extra_args=dict(), decoder_extra_args=dict())</code>","text":"<p>U-Net. </p>"},{"location":"simulai_models/#simulai.models.UNet.__init__--parameters","title":"Parameters","text":"<p>layers_config : Dict     A dictionary containing the complete configuration for the      U-Net encoder and decoder. intermediary_outputs_indices : List[int]     A list of indices for indicating the encoder outputs. intermediary_inputs_indices : List[int]     A list of indices for indicating the decoder inputs. encoder_extra_args : Dict     A dictionary containing extra arguments for the encoder. decoder_extra_args : Dict     A dictionary containing extra arguments for the decoder.</p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>def __init__(self, layers_config:Dict=None,\n             intermediary_outputs_indices:List[int]=None,\n             intermediary_inputs_indices:List[int]=None,\n             encoder_extra_args:Dict=dict(),\n             decoder_extra_args:Dict=dict()) -&gt; None:\n    \"\"\"\n    U-Net. \n\n    Parameters\n    ----------\n    layers_config : Dict\n        A dictionary containing the complete configuration for the \n        U-Net encoder and decoder.\n    intermediary_outputs_indices : List[int]\n        A list of indices for indicating the encoder outputs.\n    intermediary_inputs_indices : List[int]\n        A list of indices for indicating the decoder inputs.\n    encoder_extra_args : Dict\n        A dictionary containing extra arguments for the encoder.\n    decoder_extra_args : Dict\n        A dictionary containing extra arguments for the decoder. \n\n    \"\"\"\n\n    super(UNet, self).__init__()\n\n    self.layers_config = layers_config\n    self.intermediary_outputs_indices = intermediary_outputs_indices\n    self.intermediary_inputs_indices = intermediary_inputs_indices\n\n    self.layers_config_encoder = self.layers_config[\"encoder\"] \n    self.layers_config_decoder = self.layers_config[\"decoder\"] \n\n    self.encoder_activations = self.layers_config[\"encoder_activations\"]\n    self.decoder_activations = self.layers_config[\"decoder_activations\"]\n\n    self.encoder_horizontal_outputs = dict()\n\n    # Configuring the encoder\n    encoder_type = self.layers_config_encoder.get(\"type\")\n    layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n    if encoder_type == \"cnn\":\n        self.encoder = CNNUnetEncoder(layers=self.layers_config_encoder[\"architecture\"],\n                                      activations=self.encoder_activations,\n                                      intermediary_outputs_indices=self.intermediary_outputs_indices,\n                                      case=\"2d\", name=\"encoder\",\n                                      **encoder_extra_args)\n    else:\n        raise Exception(f\"Option {encoder_type} is not available.\")\n\n     # Configuring the decoder\n    decoder_type = self.layers_config_decoder.get(\"type\")\n    layers_config_encoder = self.layers_config_encoder.get(\"architecture\")\n\n    if encoder_type == \"cnn\":\n        self.decoder = CNNUnetDecoder(layers=self.layers_config_decoder[\"architecture\"],\n                                      activations=self.decoder_activations,\n                                      intermediary_inputs_indices=self.intermediary_inputs_indices,\n                                      case=\"2d\", name=\"decoder\",\n                                      **decoder_extra_args)\n    else:\n        raise Exception(f\"Option {encoder_type} is not available.\")\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.forward","title":"<code>forward(input_data=None)</code>","text":"<p>The U-Net forward method. </p>"},{"location":"simulai_models/#simulai.models.UNet.forward--parameters","title":"Parameters","text":"<p>input_data : Union[torch.Tensor, np.ndarray],     A dataset to be inputted in the CNN U-Net encoder.</p>"},{"location":"simulai_models/#simulai.models.UNet.forward--returns","title":"Returns","text":"<p>torch.Tensor     The U-Net output.</p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>@as_tensor\ndef forward(self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    The U-Net forward method. \n\n    Parameters\n    ----------\n    input_data : Union[torch.Tensor, np.ndarray],\n        A dataset to be inputted in the CNN U-Net encoder.\n\n    Returns\n    -------\n    torch.Tensor\n        The U-Net output. \n    \"\"\"\n\n    encoder_main_output, encoder_intermediary_outputs = self.encoder(input_data=input_data)\n    output = self.decoder(input_data = encoder_main_output,\n                          intermediary_encoder_outputs=encoder_intermediary_outputs)\n\n    return output\n</code></pre>"},{"location":"simulai_models/#simulai.models.UNet.summary","title":"<code>summary()</code>","text":"<p>It shows a general view of the architecture.</p> Source code in <code>simulai/models/_pytorch_models/_unet.py</code> <pre><code>def summary(self):\n    \"\"\"\n    It shows a general view of the architecture.\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"simulai_models/#deeponet","title":"DeepONet","text":"<p>             Bases: <code>NetworkTemplate</code></p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>class DeepONet(NetworkTemplate):\n    name = \"deeponet\"\n    engine = \"torch\"\n\n    def __init__(\n        self,\n        trunk_network: NetworkTemplate = None,\n        branch_network: NetworkTemplate = None,\n        decoder_network: NetworkTemplate = None,  # The decoder network is optional and considered\n        var_dim: int = 1,  # less effective than the output reshaping alternative\n        devices: Union[str, list] = \"cpu\",\n        product_type: str = None,\n        rescale_factors: np.ndarray = None,\n        model_id:str=None,\n        use_bias:bool=False,\n    ) -&gt; None:\n        \"\"\"\n\n        Classical Deep Operator Network (DeepONet), a deep learning version\n        of the Universal Approximation Theorem.\n\n        Parameters\n        ----------\n\n        trunk_network : NetworkTemplate\n            Subnetwork for processing the coordinates inputs.\n        branch_network : NetworkTemplate\n            Subnetwork for processing the forcing/conditioning inputs.\n        decoder_network : NetworkTemplate\n            Subnetworks for converting the embedding to the output (optional).\n        var_dim: int\n            Number of output variables.\n        devices: Union[str, list]\n            Devices in which the model will be executed.\n        product_type: str\n            Type of product to execute in the embedding space.\n        rescale_factors: np.ndarray\n            Values used for rescaling the network outputs for a given order of magnitude.\n        model_id: str\n            Name for the model\n\n        \"\"\"\n\n        super(DeepONet, self).__init__(devices=devices)\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks used in the DeepONet model\n        self.device = self._set_device(devices=devices)\n        self.use_bias = use_bias\n\n        self.trunk_network = self.to_wrap(entity=trunk_network, device=self.device)\n        self.branch_network = self.to_wrap(entity=branch_network, device=self.device)\n\n        self.add_module(\"trunk_network\", self.trunk_network)\n        self.add_module(\"branch_network\", self.branch_network)\n\n        if decoder_network is not None:\n            self.decoder_network = self.to_wrap(entity=decoder_network, device=self.device)\n            self.add_module(\"decoder_network\", self.decoder_network)\n        else:\n            self.decoder_network = decoder_network\n\n        self.product_type = product_type\n        self.model_id = model_id\n        self.var_dim = var_dim\n\n        # Rescaling factors for the output\n        if rescale_factors is not None:\n            assert (\n                len(rescale_factors) == var_dim\n            ), \"The number of rescaling factors must be equal to var_dim.\"\n            rescale_factors = torch.from_numpy(rescale_factors.astype(\"float32\"))\n            self.rescale_factors = self.to_wrap(entity=rescale_factors, device=self.device)\n        else:\n            self.rescale_factors = None\n\n        # Checking up whether the output of each subnetwork are in correct shape\n        assert self._latent_dimension_is_correct(self.trunk_network.output_size), (\n            \"The trunk network must have\"\n            \" one-dimensional output , \"\n            \"but received\"\n            f\"{self.trunk_network.output_size}\"\n        )\n\n        assert self._latent_dimension_is_correct(self.branch_network.output_size), (\n            \"The branch network must have\"\n            \" one-dimensional output,\"\n            \" but received\"\n            f\"{self.branch_network.output_size}\"\n        )\n\n        # If bias is being used, check whether the network outputs are compatible.\n        if self.use_bias:\n            print(\"Bias is being used.\")\n            self._bias_compatibility_is_correct(dim_trunk=self.trunk_network.output_size,\n                                                dim_branch=self.branch_network.output_size)\n            self.bias_wrapper = self._wrapper_bias_active\n        else:\n            self.bias_wrapper = self._wrapper_bias_inactive\n\n        # Using a decoder on top of the model or not\n        if self.decoder_network is not None:\n            self.decoder_wrapper = self._wrapper_decoder_active\n        else:\n            self.decoder_wrapper = self._wrapper_decoder_inactive\n\n        # Using rescaling factors or not\n        if rescale_factors is not None:\n            self.rescale_wrapper = self._wrapper_rescale_active\n        else:\n            self.rescale_wrapper = self._wrapper_rescale_inactive\n\n        # Checking the compatibility of the subnetworks outputs for each kind of product being employed.\n        if self.product_type != \"dense\":\n            output_branch = self.branch_network.output_size\n            output_trunk = self.trunk_network.output_size\n\n            # It checks if the inner product operation can be performed.\n            if not self.use_bias:\n                assert output_branch == output_trunk, (\n                    f\"The output dimensions for the sub-networks\"\n                    f\" trunk and branch must be equal but are\"\n                    f\" {output_branch}\"\n                    f\" and {output_trunk}\"\n                )\n            else:\n                print(\"Bias compatibility was already verified.\")\n        else:\n            output_branch = self.branch_network.output_size\n\n            assert not output_branch % self.var_dim, (\n                f\"The number of branch latent outputs must\"\n                f\" be divisible by the number of variables,\"\n                f\" but received {output_branch}\"\n                f\" and {self.var_dim}\"\n            )\n\n        self.subnetworks = [\n            net\n            for net in [self.trunk_network, self.branch_network, self.decoder_network]\n            if net is not None\n        ]\n\n        self.input_trunk = None\n        self.input_branch = None\n\n        self.output = None\n        self.var_map = dict()\n\n        #TODO Checking up if the input of the decoder network has the correct dimension\n        if self.decoder_network is not None:\n            print(\"Decoder is being used.\")\n        else:\n            pass\n\n        # Selecting the correct forward approach to be used\n        self._forward = self._forward_selector_()\n\n        self.subnetworks_names = [\"trunk\", \"branch\"]\n\n    def _latent_dimension_is_correct(self, dim: Union[int, tuple]) -&gt; bool:\n        \"\"\"\n\n        It checks if the latent dimension is consistent.\n\n        Parameters\n        ----------\n\n        dim : Union[int, tuple]\n            Latent_space_dimension.\n\n        Returns\n        -------\n\n        bool\n            The confirmation about the dimensionality correctness.\n\n        \"\"\"\n\n        if type(dim) == int:\n            return True\n        elif type(dim) == tuple:\n            if len(tuple) == 1:\n                return True\n            else:\n                return False\n\n    def _bias_compatibility_is_correct(self, dim_trunk: Union[int, tuple],\n                                       dim_branch: Union[int, tuple]) -&gt; bool:\n\n        assert dim_branch == dim_trunk + self.var_dim, (\"When using bias, the dimension\"+\n                                                        \"of the branch output should be\" +\n                                                        \"trunk output + var_dim.\")\n\n    def _forward_dense(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n\n        Forward method used when the embeddings are multiplied using a matrix-like product, it means, the trunk\n        network outputs serve as \"interpolation basis\" for the branch outputs.\n\n        Parameters\n        ----------\n\n        output_trunk: torch.Tensor\n            The embedding generated by the trunk network.\n        output_branch: torch.Tensor\n            The embedding generated by the branch network.\n\n        Returns\n        -------\n        torch.Tensor\n            The product between the two embeddings.\n\n        \"\"\"\n\n        latent_dim = int(output_branch.shape[-1] / self.var_dim)\n        output_branch_reshaped = torch.reshape(\n            output_branch, (-1, self.var_dim, latent_dim)\n        )\n\n        output = torch.matmul(output_branch_reshaped, output_trunk[..., None])\n        output = torch.squeeze(output)\n\n        return output\n\n    def _forward_pointwise(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n\n        Forward method used when the embeddings are multiplied using a simple point-wise product, after that a\n        reshaping is applied in order to produce multiple outputs.\n\n        Parameters\n        ----------\n\n        output_trunk: torch.Tensor\n            The embedding generated by the trunk network.\n        output_branch: torch.Tensor\n            The embedding generated by the branch network.\n\n        Returns\n        -------\n        torch.Tensor\n            The product between the two embeddings.\n\n        \"\"\"\n\n        latent_dim = int(output_trunk.shape[-1] / self.var_dim)\n        output_trunk_reshaped = torch.reshape(\n            output_trunk, (-1, latent_dim, self.var_dim)\n        )\n        output_branch_reshaped = torch.reshape(\n            output_branch, (-1, latent_dim, self.var_dim)\n        )\n        output = torch.sum(\n            output_trunk_reshaped * output_branch_reshaped, dim=-2, keepdim=False\n        )\n\n        return output\n\n    def _forward_vanilla(\n        self, output_trunk: torch.Tensor = None, output_branch: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n\n        Forward method used when the embeddings are multiplied using a simple point-wise product.\n\n        Parameters\n        ----------\n\n        output_trunk: torch.Tensor\n            The embedding generated by the trunk network.\n        output_branch: torch.Tensor\n            The embedding generated by the branch network.\n\n        Returns\n        -------\n        torch.Tensor\n            The product between the two embeddings.\n\n        \"\"\"\n\n        output = torch.sum(output_trunk * output_branch, dim=-1, keepdim=True)\n\n        return output\n\n    def _forward_selector_(self) -&gt; callable:\n        \"\"\"\n\n        It selects the forward method to be used.\n\n        Returns\n        -------\n\n        callable:\n            The callable corresponding to the required forward method.\n\n        \"\"\"\n\n        if self.var_dim &gt; 1:\n\n            # It operates as a typical dense layer\n            if self.product_type == \"dense\":\n                return self._forward_dense\n            # It executes an inner product by parts between the outputs\n            # of the subnetworks branch and trunk\n            else:\n                return self._forward_pointwise\n        else:\n            return self._forward_vanilla\n\n    @property\n    def _var_map(self) -&gt; dict:\n        # It checks all the data arrays in self.var_map have the same\n        # batches dimension\n        batches_dimensions = set([value.shape[0] for value in self.var_map.values()])\n\n        assert (\n            len(batches_dimensions) == 1\n        ), \"This dataset is not proper to apply shuffling\"\n\n        dim = list(batches_dimensions)[0]\n\n        indices = np.arange(dim)\n\n        np.random.shuffle(indices)\n\n        var_map_shuffled = {key: value[indices] for key, value in self.var_map.items()}\n\n        return var_map_shuffled\n\n    @property\n    def weights(self) -&gt; list:\n        return sum([net.weights for net in self.subnetworks], [])\n\n    # Now, a sequence of wrappers\n    def _wrapper_bias_inactive(\n        self,\n        output_trunk: Union[np.ndarray, torch.Tensor] = None,\n        output_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        output = self._forward(output_trunk=output_trunk, output_branch=output_branch)\n\n        return output\n\n    def _wrapper_bias_active(\n        self,\n        output_trunk: Union[np.ndarray, torch.Tensor] = None,\n        output_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        output_branch_ = output_branch[:, :-self.var_dim]\n        bias = output_branch[:, -self.var_dim:]\n\n        output = self._forward(output_trunk=output_trunk, output_branch=output_branch_) + bias\n\n        return output\n\n    def _wrapper_decoder_active(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return self.decoder_network.forward(input_data=input_data)\n\n    def _wrapper_decoder_inactive(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data\n\n    def _wrapper_rescale_active(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data * self.rescale_factors\n\n    def _wrapper_rescale_inactive(\n        self, \n        input_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n        return input_data\n\n    def forward(\n        self,\n        input_trunk: Union[np.ndarray, torch.Tensor] = None,\n        input_branch: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n\n        Wrapper forward method.\n\n        Parameters\n        ----------\n\n        input_trunk : Union[np.ndarray, torch.Tensor]\n        input_branch : Union[np.ndarray, torch.Tensor]\n\n        Returns\n        -------\n\n        torch.Tensor\n            The result of all the hidden operations in the network.\n\n        \"\"\"\n\n        # Forward method execution\n        output_trunk = self.to_wrap(entity=self.trunk_network.forward(input_trunk),\n                                    device=self.device)\n\n        output_branch = self.to_wrap(entity=self.branch_network.forward(input_branch),\n                                     device=self.device)\n\n        # Wrappers are applied to execute user-defined operations.\n        # When those operations are not selected, these wrappers simply\n        # bypass the inputs. \n        output = self.bias_wrapper(output_trunk=output_trunk, output_branch=output_branch)\n\n        return self.rescale_wrapper(input_data=self.decoder_wrapper(input_data=output))\n\n    @guarantee_device\n    def eval(\n        self,\n        trunk_data: Union[np.ndarray, torch.Tensor] = None,\n        branch_data: Union[np.ndarray, torch.Tensor] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n\n        It uses the network to make evaluations.\n\n        Parameters\n        ----------\n\n        trunk_data : Union[np.ndarray, torch.Tensor]\n        branch_data : Union[np.ndarray, torch.Tensor]\n\n        Returns\n        -------\n\n        np.ndarray\n            The result of all the hidden operations in the network.\n\n        \"\"\"\n\n        output_tensor = self.forward(input_trunk=trunk_data, input_branch=branch_data)\n\n        return output_tensor.cpu().detach().numpy()\n\n    @guarantee_device\n    def eval_subnetwork(\n        self, name: str = None, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        It evaluates the output of DeepONet subnetworks.\n\n        Parameters\n        ----------\n\n        name : str\n            Name of the subnetwork.\n        input_data : Union[np.ndarray, torch.Tensor]\n            The data used as input for the subnetwork.\n\n        Returns\n        -------\n\n        np.ndarray\n            The evaluation performed by the subnetwork.\n\n        \"\"\"\n\n        assert (\n            name in self.subnetworks_names\n        ), f\"The name {name} is not a subnetwork of {self}.\"\n\n        network_to_be_used = getattr(self, name + \"_network\")\n\n        return network_to_be_used.forward(input_data).cpu().detach().numpy()\n\n    def summary(self) -&gt; None:\n        print(\"Trunk Network:\")\n        self.trunk_network.summary()\n        print(\"Branch Network:\")\n        self.branch_network.summary()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.__init__","title":"<code>__init__(trunk_network=None, branch_network=None, decoder_network=None, var_dim=1, devices='cpu', product_type=None, rescale_factors=None, model_id=None, use_bias=False)</code>","text":"<p>Classical Deep Operator Network (DeepONet), a deep learning version of the Universal Approximation Theorem.</p>"},{"location":"simulai_models/#simulai.models.DeepONet.__init__--parameters","title":"Parameters","text":"NetworkTemplate <p>Subnetwork for processing the coordinates inputs.</p> <p>branch_network : NetworkTemplate     Subnetwork for processing the forcing/conditioning inputs. decoder_network : NetworkTemplate     Subnetworks for converting the embedding to the output (optional). var_dim: int     Number of output variables. devices: Union[str, list]     Devices in which the model will be executed. product_type: str     Type of product to execute in the embedding space. rescale_factors: np.ndarray     Values used for rescaling the network outputs for a given order of magnitude. model_id: str     Name for the model</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>def __init__(\n    self,\n    trunk_network: NetworkTemplate = None,\n    branch_network: NetworkTemplate = None,\n    decoder_network: NetworkTemplate = None,  # The decoder network is optional and considered\n    var_dim: int = 1,  # less effective than the output reshaping alternative\n    devices: Union[str, list] = \"cpu\",\n    product_type: str = None,\n    rescale_factors: np.ndarray = None,\n    model_id:str=None,\n    use_bias:bool=False,\n) -&gt; None:\n    \"\"\"\n\n    Classical Deep Operator Network (DeepONet), a deep learning version\n    of the Universal Approximation Theorem.\n\n    Parameters\n    ----------\n\n    trunk_network : NetworkTemplate\n        Subnetwork for processing the coordinates inputs.\n    branch_network : NetworkTemplate\n        Subnetwork for processing the forcing/conditioning inputs.\n    decoder_network : NetworkTemplate\n        Subnetworks for converting the embedding to the output (optional).\n    var_dim: int\n        Number of output variables.\n    devices: Union[str, list]\n        Devices in which the model will be executed.\n    product_type: str\n        Type of product to execute in the embedding space.\n    rescale_factors: np.ndarray\n        Values used for rescaling the network outputs for a given order of magnitude.\n    model_id: str\n        Name for the model\n\n    \"\"\"\n\n    super(DeepONet, self).__init__(devices=devices)\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks used in the DeepONet model\n    self.device = self._set_device(devices=devices)\n    self.use_bias = use_bias\n\n    self.trunk_network = self.to_wrap(entity=trunk_network, device=self.device)\n    self.branch_network = self.to_wrap(entity=branch_network, device=self.device)\n\n    self.add_module(\"trunk_network\", self.trunk_network)\n    self.add_module(\"branch_network\", self.branch_network)\n\n    if decoder_network is not None:\n        self.decoder_network = self.to_wrap(entity=decoder_network, device=self.device)\n        self.add_module(\"decoder_network\", self.decoder_network)\n    else:\n        self.decoder_network = decoder_network\n\n    self.product_type = product_type\n    self.model_id = model_id\n    self.var_dim = var_dim\n\n    # Rescaling factors for the output\n    if rescale_factors is not None:\n        assert (\n            len(rescale_factors) == var_dim\n        ), \"The number of rescaling factors must be equal to var_dim.\"\n        rescale_factors = torch.from_numpy(rescale_factors.astype(\"float32\"))\n        self.rescale_factors = self.to_wrap(entity=rescale_factors, device=self.device)\n    else:\n        self.rescale_factors = None\n\n    # Checking up whether the output of each subnetwork are in correct shape\n    assert self._latent_dimension_is_correct(self.trunk_network.output_size), (\n        \"The trunk network must have\"\n        \" one-dimensional output , \"\n        \"but received\"\n        f\"{self.trunk_network.output_size}\"\n    )\n\n    assert self._latent_dimension_is_correct(self.branch_network.output_size), (\n        \"The branch network must have\"\n        \" one-dimensional output,\"\n        \" but received\"\n        f\"{self.branch_network.output_size}\"\n    )\n\n    # If bias is being used, check whether the network outputs are compatible.\n    if self.use_bias:\n        print(\"Bias is being used.\")\n        self._bias_compatibility_is_correct(dim_trunk=self.trunk_network.output_size,\n                                            dim_branch=self.branch_network.output_size)\n        self.bias_wrapper = self._wrapper_bias_active\n    else:\n        self.bias_wrapper = self._wrapper_bias_inactive\n\n    # Using a decoder on top of the model or not\n    if self.decoder_network is not None:\n        self.decoder_wrapper = self._wrapper_decoder_active\n    else:\n        self.decoder_wrapper = self._wrapper_decoder_inactive\n\n    # Using rescaling factors or not\n    if rescale_factors is not None:\n        self.rescale_wrapper = self._wrapper_rescale_active\n    else:\n        self.rescale_wrapper = self._wrapper_rescale_inactive\n\n    # Checking the compatibility of the subnetworks outputs for each kind of product being employed.\n    if self.product_type != \"dense\":\n        output_branch = self.branch_network.output_size\n        output_trunk = self.trunk_network.output_size\n\n        # It checks if the inner product operation can be performed.\n        if not self.use_bias:\n            assert output_branch == output_trunk, (\n                f\"The output dimensions for the sub-networks\"\n                f\" trunk and branch must be equal but are\"\n                f\" {output_branch}\"\n                f\" and {output_trunk}\"\n            )\n        else:\n            print(\"Bias compatibility was already verified.\")\n    else:\n        output_branch = self.branch_network.output_size\n\n        assert not output_branch % self.var_dim, (\n            f\"The number of branch latent outputs must\"\n            f\" be divisible by the number of variables,\"\n            f\" but received {output_branch}\"\n            f\" and {self.var_dim}\"\n        )\n\n    self.subnetworks = [\n        net\n        for net in [self.trunk_network, self.branch_network, self.decoder_network]\n        if net is not None\n    ]\n\n    self.input_trunk = None\n    self.input_branch = None\n\n    self.output = None\n    self.var_map = dict()\n\n    #TODO Checking up if the input of the decoder network has the correct dimension\n    if self.decoder_network is not None:\n        print(\"Decoder is being used.\")\n    else:\n        pass\n\n    # Selecting the correct forward approach to be used\n    self._forward = self._forward_selector_()\n\n    self.subnetworks_names = [\"trunk\", \"branch\"]\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.eval","title":"<code>eval(trunk_data=None, branch_data=None)</code>","text":"<p>It uses the network to make evaluations.</p>"},{"location":"simulai_models/#simulai.models.DeepONet.eval--parameters","title":"Parameters","text":"<p>trunk_data : Union[np.ndarray, torch.Tensor] branch_data : Union[np.ndarray, torch.Tensor]</p>"},{"location":"simulai_models/#simulai.models.DeepONet.eval--returns","title":"Returns","text":"<p>np.ndarray     The result of all the hidden operations in the network.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>@guarantee_device\ndef eval(\n    self,\n    trunk_data: Union[np.ndarray, torch.Tensor] = None,\n    branch_data: Union[np.ndarray, torch.Tensor] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n\n    It uses the network to make evaluations.\n\n    Parameters\n    ----------\n\n    trunk_data : Union[np.ndarray, torch.Tensor]\n    branch_data : Union[np.ndarray, torch.Tensor]\n\n    Returns\n    -------\n\n    np.ndarray\n        The result of all the hidden operations in the network.\n\n    \"\"\"\n\n    output_tensor = self.forward(input_trunk=trunk_data, input_branch=branch_data)\n\n    return output_tensor.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.eval_subnetwork","title":"<code>eval_subnetwork(name=None, input_data=None)</code>","text":"<p>It evaluates the output of DeepONet subnetworks.</p>"},{"location":"simulai_models/#simulai.models.DeepONet.eval_subnetwork--parameters","title":"Parameters","text":"str <p>Name of the subnetwork.</p> <p>input_data : Union[np.ndarray, torch.Tensor]     The data used as input for the subnetwork.</p>"},{"location":"simulai_models/#simulai.models.DeepONet.eval_subnetwork--returns","title":"Returns","text":"<p>np.ndarray     The evaluation performed by the subnetwork.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>@guarantee_device\ndef eval_subnetwork(\n    self, name: str = None, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    It evaluates the output of DeepONet subnetworks.\n\n    Parameters\n    ----------\n\n    name : str\n        Name of the subnetwork.\n    input_data : Union[np.ndarray, torch.Tensor]\n        The data used as input for the subnetwork.\n\n    Returns\n    -------\n\n    np.ndarray\n        The evaluation performed by the subnetwork.\n\n    \"\"\"\n\n    assert (\n        name in self.subnetworks_names\n    ), f\"The name {name} is not a subnetwork of {self}.\"\n\n    network_to_be_used = getattr(self, name + \"_network\")\n\n    return network_to_be_used.forward(input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.DeepONet.forward","title":"<code>forward(input_trunk=None, input_branch=None)</code>","text":"<p>Wrapper forward method.</p>"},{"location":"simulai_models/#simulai.models.DeepONet.forward--parameters","title":"Parameters","text":"<p>input_trunk : Union[np.ndarray, torch.Tensor] input_branch : Union[np.ndarray, torch.Tensor]</p>"},{"location":"simulai_models/#simulai.models.DeepONet.forward--returns","title":"Returns","text":"<p>torch.Tensor     The result of all the hidden operations in the network.</p> Source code in <code>simulai/models/_pytorch_models/_deeponet.py</code> <pre><code>def forward(\n    self,\n    input_trunk: Union[np.ndarray, torch.Tensor] = None,\n    input_branch: Union[np.ndarray, torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n\n    Wrapper forward method.\n\n    Parameters\n    ----------\n\n    input_trunk : Union[np.ndarray, torch.Tensor]\n    input_branch : Union[np.ndarray, torch.Tensor]\n\n    Returns\n    -------\n\n    torch.Tensor\n        The result of all the hidden operations in the network.\n\n    \"\"\"\n\n    # Forward method execution\n    output_trunk = self.to_wrap(entity=self.trunk_network.forward(input_trunk),\n                                device=self.device)\n\n    output_branch = self.to_wrap(entity=self.branch_network.forward(input_branch),\n                                 device=self.device)\n\n    # Wrappers are applied to execute user-defined operations.\n    # When those operations are not selected, these wrappers simply\n    # bypass the inputs. \n    output = self.bias_wrapper(output_trunk=output_trunk, output_branch=output_branch)\n\n    return self.rescale_wrapper(input_data=self.decoder_wrapper(input_data=output))\n</code></pre>"},{"location":"simulai_models/#autoencodermlp","title":"AutoencoderMLP","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Fully-connected AutoEncoder as Reduced Order Model;</p> <p>A MLP autoencoder architecture consists of two stages: --&gt; Fully-connected encoder --&gt; Fully connected decoder</p> SCHEME <p>|         | |  |   |  |</p> <p>Z -&gt;    |  | | |  |  -&gt; Z_til         |  |   |  |         |         |</p> <p>ENCODER       DECODER</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderMLP(NetworkTemplate):\n    \"\"\"\n    This is an implementation of a Fully-connected AutoEncoder as\n    Reduced Order Model;\n\n    A MLP autoencoder architecture consists of two stages:\n    --&gt; Fully-connected encoder\n    --&gt; Fully connected decoder\n\n    SCHEME:\n            |         |\n            |  |   |  |\n    Z -&gt;    |  | | |  |  -&gt; Z_til\n            |  |   |  |\n            |         |\n\n    ENCODER       DECODER\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: DenseNetwork = None,\n        decoder: DenseNetwork = None,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        shallow: Optional[bool] = False,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the AutoencoderMLP network\n\n        Parameters\n        ----------\n        encoder : DenseNetwork\n            The encoder network architecture.\n        decoder : DenseNetwork\n            The decoder network architecture.\n        input_dim : int, optional\n            The input dimensions of the data, by default None.\n        output_dim : int, optional\n            The output dimensions of the data, by default None.\n        latent_dim : int, optional\n            The dimensions of the latent space, by default None.\n        activation : Union[list, str], optional\n            The activation functions used by the network, by default None.\n        shallow : bool, optional\n            Whether the network should be shallow or not, by default False.\n        devices : Union[str, list], optional\n            The device(s) to be used for allocating subnetworks, by default \"cpu\".\n        name : str, optional\n            The name of the network, by default None.\n        \"\"\"\n\n        super(AutoencoderMLP, self).__init__(name=name)\n\n        self.weights = list()\n\n        # This option is used when no network is provided\n        # and it uses default choices for the architectures\n        if encoder == None and decoder == None:\n            encoder, decoder = mlp_autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                shallow=shallow,\n            )\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks used in the DeepONet model\n        self.device = self._set_device(devices=devices)\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        self.last_encoder_channels = None\n\n        self.shapes_dict = dict()\n\n    def summary(self) -&gt; None:\n        \"\"\"\n        Prints the summary of the network architecture\n        \"\"\"\n        self.encoder.summary()\n        self.decoder.summary()\n\n    def projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Project the input dataset into the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be projected, by default None.\n\n        Returns\n        -------\n        torch.Tensor\n            The dataset projected over the latent space.\n\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    def reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Reconstruct the latent dataset to the original one.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be reconstructed, by default None.\n\n        Returns\n        -------\n        torch.Tensor\n            The dataset reconstructed.\n\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Execute the complete projection/reconstruction pipeline.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input dataset, by default None.\n\n        Returns\n        -------\n        torch.Tensor\n            The dataset reconstructed.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def eval_projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate the projection of the input dataset into the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be projected, by default None.\n\n        Returns\n        -------\n        np.ndarray\n            The dataset projected over the latent space.\n\n        \"\"\"\n        return self.projection(input_data=input_data).detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.__init__","title":"<code>__init__(encoder=None, decoder=None, input_dim=None, output_dim=None, latent_dim=None, activation=None, shallow=False, devices='cpu', name=None)</code>","text":"<p>Initialize the AutoencoderMLP network</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.__init__--parameters","title":"Parameters","text":"<p>encoder : DenseNetwork     The encoder network architecture. decoder : DenseNetwork     The decoder network architecture. input_dim : int, optional     The input dimensions of the data, by default None. output_dim : int, optional     The output dimensions of the data, by default None. latent_dim : int, optional     The dimensions of the latent space, by default None. activation : Union[list, str], optional     The activation functions used by the network, by default None. shallow : bool, optional     Whether the network should be shallow or not, by default False. devices : Union[str, list], optional     The device(s) to be used for allocating subnetworks, by default \"cpu\". name : str, optional     The name of the network, by default None.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: DenseNetwork = None,\n    decoder: DenseNetwork = None,\n    input_dim: Optional[int] = None,\n    output_dim: Optional[int] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    shallow: Optional[bool] = False,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the AutoencoderMLP network\n\n    Parameters\n    ----------\n    encoder : DenseNetwork\n        The encoder network architecture.\n    decoder : DenseNetwork\n        The decoder network architecture.\n    input_dim : int, optional\n        The input dimensions of the data, by default None.\n    output_dim : int, optional\n        The output dimensions of the data, by default None.\n    latent_dim : int, optional\n        The dimensions of the latent space, by default None.\n    activation : Union[list, str], optional\n        The activation functions used by the network, by default None.\n    shallow : bool, optional\n        Whether the network should be shallow or not, by default False.\n    devices : Union[str, list], optional\n        The device(s) to be used for allocating subnetworks, by default \"cpu\".\n    name : str, optional\n        The name of the network, by default None.\n    \"\"\"\n\n    super(AutoencoderMLP, self).__init__(name=name)\n\n    self.weights = list()\n\n    # This option is used when no network is provided\n    # and it uses default choices for the architectures\n    if encoder == None and decoder == None:\n        encoder, decoder = mlp_autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            shallow=shallow,\n        )\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks used in the DeepONet model\n    self.device = self._set_device(devices=devices)\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    self.last_encoder_channels = None\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.eval_projection","title":"<code>eval_projection(input_data=None)</code>","text":"<p>Evaluate the projection of the input dataset into the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.eval_projection--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be projected, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.eval_projection--returns","title":"Returns","text":"<p>np.ndarray     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval_projection(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate the projection of the input dataset into the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be projected, by default None.\n\n    Returns\n    -------\n    np.ndarray\n        The dataset projected over the latent space.\n\n    \"\"\"\n    return self.projection(input_data=input_data).detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.forward","title":"<code>forward(input_data=None)</code>","text":"<p>Execute the complete projection/reconstruction pipeline.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.forward--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input dataset, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.forward--returns","title":"Returns","text":"<p>torch.Tensor     The dataset reconstructed.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Execute the complete projection/reconstruction pipeline.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input dataset, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        The dataset reconstructed.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.projection","title":"<code>projection(input_data=None)</code>","text":"<p>Project the input dataset into the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.projection--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be projected, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.projection--returns","title":"Returns","text":"<p>torch.Tensor     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def projection(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Project the input dataset into the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be projected, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        The dataset projected over the latent space.\n\n    \"\"\"\n    latent = self.encoder.forward(input_data=input_data)\n\n    return latent\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.reconstruction","title":"<code>reconstruction(input_data=None)</code>","text":"<p>Reconstruct the latent dataset to the original one.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.reconstruction--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be reconstructed, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.reconstruction--returns","title":"Returns","text":"<p>torch.Tensor     The dataset reconstructed.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction(\n    self, input_data: Union[torch.Tensor, np.ndarray] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Reconstruct the latent dataset to the original one.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be reconstructed, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        The dataset reconstructed.\n\n    \"\"\"\n    reconstructed = self.decoder.forward(input_data=input_data)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderMLP.summary","title":"<code>summary()</code>","text":"<p>Prints the summary of the network architecture</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Prints the summary of the network architecture\n    \"\"\"\n    self.encoder.summary()\n    self.decoder.summary()\n</code></pre>"},{"location":"simulai_models/#autoencodercnn","title":"AutoencoderCNN","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a convolutional autoencoder as Reduced Order Model. An autoencoder architecture consists of three stages:</p> <ul> <li>The convolutional encoder</li> </ul> <p>The bottleneck stage, subdivided in:     * Fully-connected encoder     * Fully connected decoder     * The convolutional decoder</p> <p>SCHEME:</p> <p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til</p> <pre><code>            ENCODER               DENSE BOTTLENECK           DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderCNN(NetworkTemplate):\n    \"\"\"\n    This is an implementation of a convolutional autoencoder as Reduced Order Model.\n    An autoencoder architecture consists of three stages:\n\n    * The convolutional encoder\n\n    The bottleneck stage, subdivided in:\n        * Fully-connected encoder\n        * Fully connected decoder\n        * The convolutional decoder\n\n    SCHEME:\n\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n\n\n                    ENCODER               DENSE BOTTLENECK           DECODER\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: ConvolutionalNetwork = None,\n        bottleneck_encoder: Linear = None,\n        bottleneck_decoder: Linear = None,\n        decoder: ConvolutionalNetwork = None,\n        encoder_activation: str = \"relu\",\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        kernel_size: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        case: Optional[str] = None,\n        shallow: Optional[bool] = False,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the AutoencoderCNN network.\n\n        Parameters\n        ----------\n        encoder : ConvolutionalNetwork, optional\n            The encoder network architecture, by default None.\n        bottleneck_encoder : Linear, optional\n            The bottleneck encoder network architecture, by default None.\n        bottleneck_decoder : Linear, optional\n            The bottleneck decoder network architecture, by default None.\n        decoder : ConvolutionalNetwork, optional\n            The decoder network architecture, by default None.\n        encoder_activation : str, optional\n            The activation function used by the encoder network, by default 'relu'.\n        input_dim : Tuple[int, ...], optional\n            The input dimensions of the data, by default None.\n        output_dim : Tuple[int, ...], optional\n            The output dimensions of the data, by default None.\n        latent_dim : int, optional\n            The dimensions of the latent space, by default None.\n        activation : Union[list, str], optional\n            The activation functions used by the network, by default None.\n        channels : int, optional\n            The number of channels of the convolutional layers, by default None.\n        case : str, optional\n            The type of convolutional encoder and decoder to be used, by default None.\n        shallow : bool, optional\n            Whether the network should be shallow or not, by default False.\n        devices : Union[str, list], optional\n            The device(s) to be used for allocating subnetworks, by default 'cpu'.\n        name : str, optional\n            The name of the network, by default None.\n        \"\"\"\n\n        super(AutoencoderCNN, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            (\n                encoder,\n                decoder,\n                bottleneck_encoder,\n                bottleneck_decoder,\n            ) = cnn_autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                kernel_size=kernel_size,\n                channels=channels,\n                case=case,\n                shallow=shallow,\n            )\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n        self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n        self.weights += self.decoder.weights\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Prints the summary of the network architecture.\n\n        Parameters\n        ----------\n        input_data : np.ndarray or torch.Tensor\n            The input dataset.\n        input_shape : list, optional\n            The shape of the input data.\n\n        Returns\n        -------\n        torch.Tensor\n            The dataset projected over the latent space.\n        \"\"\"\n\n        if verbose == True:\n            if self.input_dim != None:\n                input_shape = self.input_dim\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device\n            )\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n                input_shape = self.encoder.input_size\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary()\n            self.bottleneck_decoder.summary()\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def projection(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"\n        Project input dataset into the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor]\n            The dataset to be projected.\n\n        Returns\n        -------\n        torch.Tensor\n            The dataset projected over the latent space.\n\n        \"\"\"\n\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Reconstruct the latent dataset to the original one.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor]\n            The dataset to be reconstructed.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed dataset.\n\n        \"\"\"\n\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=input_data)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    def forward(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"\n        Execute the complete projection/reconstruction pipeline.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor]\n            The input dataset.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed dataset.\n        \"\"\"\n\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate the autoencoder on the given dataset.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be evaluated, by default None.\n\n        Returns\n        -------\n        np.ndarray\n            The dataset projected over the latent space.\n        \"\"\"\n\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        return super().eval(input_data=input_data)\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"\n        Project the input dataset into the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be projected, by default None.\n\n        Returns\n        -------\n        np.ndarray\n            The dataset projected over the latent space.\n        \"\"\"\n\n        projected_data = self.projection(input_data=input_data)\n\n        return projected_data.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Reconstructs the latent dataset to the original one.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The dataset to be reconstructed. If not provided, uses the original input data, by default None.\n\n        Returns\n        -------\n        np.ndarray\n            The reconstructed dataset.\n\n        \"\"\"\n        reconstructed_data = self.reconstruction(input_data=input_data)\n        return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, encoder_activation='relu', input_dim=None, output_dim=None, latent_dim=None, kernel_size=None, activation=None, channels=None, case=None, shallow=False, devices='cpu', name=None, **kwargs)</code>","text":"<p>Initialize the AutoencoderCNN network.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.__init__--parameters","title":"Parameters","text":"<p>encoder : ConvolutionalNetwork, optional     The encoder network architecture, by default None. bottleneck_encoder : Linear, optional     The bottleneck encoder network architecture, by default None. bottleneck_decoder : Linear, optional     The bottleneck decoder network architecture, by default None. decoder : ConvolutionalNetwork, optional     The decoder network architecture, by default None. encoder_activation : str, optional     The activation function used by the encoder network, by default 'relu'. input_dim : Tuple[int, ...], optional     The input dimensions of the data, by default None. output_dim : Tuple[int, ...], optional     The output dimensions of the data, by default None. latent_dim : int, optional     The dimensions of the latent space, by default None. activation : Union[list, str], optional     The activation functions used by the network, by default None. channels : int, optional     The number of channels of the convolutional layers, by default None. case : str, optional     The type of convolutional encoder and decoder to be used, by default None. shallow : bool, optional     Whether the network should be shallow or not, by default False. devices : Union[str, list], optional     The device(s) to be used for allocating subnetworks, by default 'cpu'. name : str, optional     The name of the network, by default None.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: ConvolutionalNetwork = None,\n    bottleneck_encoder: Linear = None,\n    bottleneck_decoder: Linear = None,\n    decoder: ConvolutionalNetwork = None,\n    encoder_activation: str = \"relu\",\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    kernel_size: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    case: Optional[str] = None,\n    shallow: Optional[bool] = False,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the AutoencoderCNN network.\n\n    Parameters\n    ----------\n    encoder : ConvolutionalNetwork, optional\n        The encoder network architecture, by default None.\n    bottleneck_encoder : Linear, optional\n        The bottleneck encoder network architecture, by default None.\n    bottleneck_decoder : Linear, optional\n        The bottleneck decoder network architecture, by default None.\n    decoder : ConvolutionalNetwork, optional\n        The decoder network architecture, by default None.\n    encoder_activation : str, optional\n        The activation function used by the encoder network, by default 'relu'.\n    input_dim : Tuple[int, ...], optional\n        The input dimensions of the data, by default None.\n    output_dim : Tuple[int, ...], optional\n        The output dimensions of the data, by default None.\n    latent_dim : int, optional\n        The dimensions of the latent space, by default None.\n    activation : Union[list, str], optional\n        The activation functions used by the network, by default None.\n    channels : int, optional\n        The number of channels of the convolutional layers, by default None.\n    case : str, optional\n        The type of convolutional encoder and decoder to be used, by default None.\n    shallow : bool, optional\n        Whether the network should be shallow or not, by default False.\n    devices : Union[str, list], optional\n        The device(s) to be used for allocating subnetworks, by default 'cpu'.\n    name : str, optional\n        The name of the network, by default None.\n    \"\"\"\n\n    super(AutoencoderCNN, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        (\n            encoder,\n            decoder,\n            bottleneck_encoder,\n            bottleneck_decoder,\n        ) = cnn_autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            kernel_size=kernel_size,\n            channels=channels,\n            case=case,\n            shallow=shallow,\n        )\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n    self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n    self.decoder = self.to_wrap(entity=decoder, device=self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n    self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.bottleneck_encoder.weights\n    self.weights += self.bottleneck_decoder.weights\n    self.weights += self.decoder.weights\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.eval","title":"<code>eval(input_data=None)</code>","text":"<p>Evaluate the autoencoder on the given dataset.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.eval--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be evaluated, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.eval--returns","title":"Returns","text":"<p>np.ndarray     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate the autoencoder on the given dataset.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be evaluated, by default None.\n\n    Returns\n    -------\n    np.ndarray\n        The dataset projected over the latent space.\n    \"\"\"\n\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    return super().eval(input_data=input_data)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.forward","title":"<code>forward(input_data)</code>","text":"<p>Execute the complete projection/reconstruction pipeline.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.forward--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor]     The input dataset.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.forward--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def forward(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Execute the complete projection/reconstruction pipeline.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor]\n        The input dataset.\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed dataset.\n    \"\"\"\n\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.project","title":"<code>project(input_data=None)</code>","text":"<p>Project the input dataset into the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.project--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be projected, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.project--returns","title":"Returns","text":"<p>np.ndarray     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"\n    Project the input dataset into the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be projected, by default None.\n\n    Returns\n    -------\n    np.ndarray\n        The dataset projected over the latent space.\n    \"\"\"\n\n    projected_data = self.projection(input_data=input_data)\n\n    return projected_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.projection","title":"<code>projection(input_data)</code>","text":"<p>Project input dataset into the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.projection--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor]     The dataset to be projected.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.projection--returns","title":"Returns","text":"<p>torch.Tensor     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>@as_tensor\ndef projection(self, input_data: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Project input dataset into the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor]\n        The dataset to be projected.\n\n    Returns\n    -------\n    torch.Tensor\n        The dataset projected over the latent space.\n\n    \"\"\"\n\n    btnk_input = self.encoder.forward(input_data=input_data)\n\n    self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n    btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n    latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n    return latent\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the latent dataset to the original one.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruct--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The dataset to be reconstructed. If not provided, uses the original input data, by default None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruct--returns","title":"Returns","text":"<p>np.ndarray     The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reconstructs the latent dataset to the original one.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The dataset to be reconstructed. If not provided, uses the original input data, by default None.\n\n    Returns\n    -------\n    np.ndarray\n        The reconstructed dataset.\n\n    \"\"\"\n    reconstructed_data = self.reconstruction(input_data=input_data)\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruction","title":"<code>reconstruction(input_data)</code>","text":"<p>Reconstruct the latent dataset to the original one.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruction--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor]     The dataset to be reconstructed.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.reconstruction--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed dataset.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>@as_tensor\ndef reconstruction(\n    self, input_data: Union[torch.Tensor, np.ndarray]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Reconstruct the latent dataset to the original one.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor]\n        The dataset to be reconstructed.\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed dataset.\n\n    \"\"\"\n\n    bottleneck_output = self.encoder_activation(\n        self.bottleneck_decoder.forward(input_data=input_data)\n    )\n\n    bottleneck_output = bottleneck_output.reshape(\n        (-1,) + self.before_flatten_dimension\n    )\n\n    reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.summary","title":"<code>summary(input_data=None, input_shape=None, verbose=True)</code>","text":"<p>Prints the summary of the network architecture.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.summary--parameters","title":"Parameters","text":"<p>input_data : np.ndarray or torch.Tensor     The input dataset. input_shape : list, optional     The shape of the input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderCNN.summary--returns","title":"Returns","text":"<p>torch.Tensor     The dataset projected over the latent space.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    input_shape: list = None,\n    verbose: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Prints the summary of the network architecture.\n\n    Parameters\n    ----------\n    input_data : np.ndarray or torch.Tensor\n        The input dataset.\n    input_shape : list, optional\n        The shape of the input data.\n\n    Returns\n    -------\n    torch.Tensor\n        The dataset projected over the latent space.\n    \"\"\"\n\n    if verbose == True:\n        if self.input_dim != None:\n            input_shape = self.input_dim\n        else:\n            pass\n\n        self.encoder.summary(\n            input_data=input_data, input_shape=input_shape, device=self.device\n        )\n\n        if isinstance(input_data, np.ndarray):\n            btnk_input = self.encoder.forward(input_data=input_data)\n        else:\n            assert (\n                input_shape\n            ), \"It is necessary to have input_shape when input_data is None.\"\n            input_shape = self.encoder.input_size\n            input_shape[0] = 1\n\n            input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n            btnk_input = self.encoder.forward(input_data=input_data)\n\n        before_flatten_dimension = tuple(btnk_input.shape[1:])\n        btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        self.bottleneck_encoder.summary()\n        self.bottleneck_decoder.summary()\n\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=latent)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1, *before_flatten_dimension)\n        )\n\n        self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n        # Saving the content of the subnetworks to the overall architecture dictionary\n        self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n        self.shapes_dict.update(\n            {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n        )\n        self.shapes_dict.update(\n            {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n        )\n        self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n    else:\n        print(self)\n</code></pre>"},{"location":"simulai_models/#autoencoderkoopman","title":"AutoencoderKoopman","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Koopman autoencoder as a Reduced Order Model.</p> <p>A Koopman autoencoder architecture consists of five stages:</p> <ul> <li>The convolutional encoder [Optional]</li> <li>Fully-connected encoder</li> <li>Koopman operator</li> <li>Fully connected decoder</li> <li>The convolutional decoder [Optional]</li> </ul> SCHEME <p>(Koopman OPERATOR)          ^   |      |      |   |  |   |   |  |</p> <p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til                                   |  |       |  |                                   |             |</p> <pre><code>            ENCODER          DENSE BOTTLENECK        DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderKoopman(NetworkTemplate):\n    \"\"\"\n    This is an implementation of a Koopman autoencoder as a Reduced Order Model.\n\n    A Koopman autoencoder architecture consists of five stages:\n\n    * The convolutional encoder [Optional]\n    * Fully-connected encoder\n    * Koopman operator\n    * Fully connected decoder\n    * The convolutional decoder [Optional]\n\n    SCHEME:\n                                    (Koopman OPERATOR)\n                                             ^\n                                      |      |      |\n                                      |  |   |   |  |\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n                                      |  |       |  |\n                                      |             |\n\n                    ENCODER          DENSE BOTTLENECK        DECODER\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n        bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n        decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        case: Optional[str] = None,\n        architecture: Optional[str] = None,\n        shallow: Optional[bool] = False,\n        use_batch_norm: Optional[bool] = False,\n        encoder_activation: str = \"relu\",\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Constructs a new instance of the Autoencoder\n\n        Parameters\n        ----------\n        encoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n            The encoder network. Defaults to None.\n        bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional\n            The bottleneck encoder network. Defaults to None.\n        bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional\n            The bottleneck decoder network. Defaults to None.\n        decoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n            The decoder network. Defaults to None.\n        input_dim : Optional[Tuple[int, ...]], optional\n            The input dimensions. Used for automatic network generation. Defaults to None.\n        output_dim : Optional[Tuple[int, ...]], optional\n            The output dimensions. Used for automatic network generation. Defaults to None.\n        latent_dim : Optional[int], optional\n            The latent dimensions. Used for automatic network generation. Defaults to None.\n        activation : Optional[Union[list, str]], optional\n            The activation functions for each layer. Used for automatic network generation. Defaults to None.\n        channels : Optional[int], optional\n            The number of channels. Used for automatic network generation. Defaults to None.\n        case : Optional[str], optional\n            The type of problem. Used for automatic network generation. Defaults to None.\n        architecture : Optional[str], optional\n            The network architecture. Used for automatic network generation. Defaults to None.\n        shallow : Optional[bool], optional\n            Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.\n        encoder_activation : str, optional\n            The activation function for the encoder. Defaults to \"relu\".\n        devices : Union[str, list], optional\n            The devices to use. Defaults to \"cpu\".\n        name : str, optional\n            The name of the autoencoder. Defaults to None.\n        \"\"\"\n        super(AutoencoderKoopman, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                channels=channels,\n                architecture=architecture,\n                case=case,\n                shallow=shallow,\n                use_batch_norm=use_batch_norm,\n            )\n\n        self.encoder = encoder.to(self.device)\n        self.decoder = decoder.to(self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.projection = self._projection_with_bottleneck\n            self.reconstruction = self._reconstruction_with_bottleneck\n        else:\n            self.projection = self._projection\n            self.reconstruction = self._reconstruction\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.latent_dimension = None\n\n        if bottleneck_encoder is not None:\n            self.latent_dimension = bottleneck_encoder.output_size\n        else:\n            self.latent_dimension = self.encoder.output_size\n\n        self.K_op = self.to_wrap(entity=torch.nn.Linear(\n            self.latent_dimension, self.latent_dimension, bias=False\n        ).weight, device=self.device)\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n    ) -&gt; torch.Tensor:\n        if verbose == True:\n            if self.input_dim != None:\n                input_shape = list(self.input_dim)\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device\n            )\n\n            self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n                input_shape = self.encoder.input_size\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary()\n\n            print(f\"The Koopman Operator has shape: {self.K_op.shape} \")\n\n            self.bottleneck_decoder.summary()\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def _projection_with_bottleneck(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the projection of the input data onto the bottleneck encoder.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The projected latent representation.\n\n        \"\"\"\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(btnk_input.shape[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def _projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the projection of the input data onto the encoder.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The projected latent representation.\n\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    @as_tensor\n    def _reconstruction_with_bottleneck(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Reconstructs the input data using the bottleneck decoder.\n\n        Parameters\n        ----------\n        input_data : Union[torch.Tensor, np.ndarray], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed data.\n\n        \"\"\"\n        bottleneck_output = self.encoder_activation(\n            self.bottleneck_decoder.forward(input_data=input_data)\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    @as_tensor\n    def _reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Reconstructs the input data using the decoder.\n\n        Parameters\n        ----------\n        input_data : Union[torch.Tensor, np.ndarray], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed data.\n\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def latent_forward_m(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the operation u^{u+m} = K^m u^{i}\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n        m : int, optional\n            The number of Koopman iterations. Defaults to 1.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed latent representation.\n\n        \"\"\"\n        return torch.matmul(input_data, torch.pow(self.K_op.T, m))\n\n    def latent_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the operation u^{u+1} = K u^{i}\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed latent representation.\n\n        \"\"\"\n        return torch.matmul(input_data, self.K_op.T)\n\n    def reconstruction_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the operation \u0168 = D(E(U))\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed data.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def reconstruction_forward_m(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the operation \u0168_m = D(K^m E(U))\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n        m : int, optional\n            The number of Koopman iterations. Defaults to 1.\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed data.\n\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        latent_m = self.latent_forward_m(input_data=latent, m=m)\n        reconstructed_m = self.reconstruction(input_data=latent_m)\n\n        return reconstructed_m\n\n    def predict(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, n_steps: int = 1\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n        n_steps : int, optional\n            The number of extrapolations to perform. Defaults to 1.\n\n        Returns\n        -------\n        np.ndarray\n            The predicted reconstructed data.\n\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        predictions = list()\n        latent = self.projection(input_data=input_data)\n        init_latent = latent\n\n        # Extrapolating in the latent space over n_steps steps\n        for s in range(n_steps):\n            latent_s = self.latent_forward(input_data=init_latent)\n            init_latent = latent_s\n            predictions.append(latent_s)\n\n        predictions = torch.vstack(predictions)\n\n        reconstructed_predictions = self.reconstruction(input_data=predictions)\n\n        return reconstructed_predictions.detach().numpy()\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"\n        Projects the input data into the latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        np.ndarray\n            The projected data.\n\n        \"\"\"\n        projected_data = self.projection(input_data=input_data)\n\n        return projected_data.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Reconstructs the input data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data. Defaults to None.\n\n        Returns\n        -------\n        np.ndarray\n            The reconstructed data.\n\n        \"\"\"\n        reconstructed_data = self.reconstruction(input_data=input_data)\n\n        return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, input_dim=None, output_dim=None, latent_dim=None, activation=None, channels=None, case=None, architecture=None, shallow=False, use_batch_norm=False, encoder_activation='relu', devices='cpu', name=None)</code>","text":"<p>Constructs a new instance of the Autoencoder</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.__init__--parameters","title":"Parameters","text":"<p>encoder : Union[ConvolutionalNetwork, DenseNetwork], optional     The encoder network. Defaults to None. bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional     The bottleneck encoder network. Defaults to None. bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional     The bottleneck decoder network. Defaults to None. decoder : Union[ConvolutionalNetwork, DenseNetwork], optional     The decoder network. Defaults to None. input_dim : Optional[Tuple[int, ...]], optional     The input dimensions. Used for automatic network generation. Defaults to None. output_dim : Optional[Tuple[int, ...]], optional     The output dimensions. Used for automatic network generation. Defaults to None. latent_dim : Optional[int], optional     The latent dimensions. Used for automatic network generation. Defaults to None. activation : Optional[Union[list, str]], optional     The activation functions for each layer. Used for automatic network generation. Defaults to None. channels : Optional[int], optional     The number of channels. Used for automatic network generation. Defaults to None. case : Optional[str], optional     The type of problem. Used for automatic network generation. Defaults to None. architecture : Optional[str], optional     The network architecture. Used for automatic network generation. Defaults to None. shallow : Optional[bool], optional     Whether to use shallow or deep network. Used for automatic network generation. Defaults to False. encoder_activation : str, optional     The activation function for the encoder. Defaults to \"relu\". devices : Union[str, list], optional     The devices to use. Defaults to \"cpu\". name : str, optional     The name of the autoencoder. Defaults to None.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n    bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n    decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    case: Optional[str] = None,\n    architecture: Optional[str] = None,\n    shallow: Optional[bool] = False,\n    use_batch_norm: Optional[bool] = False,\n    encoder_activation: str = \"relu\",\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n) -&gt; None:\n    \"\"\"\n    Constructs a new instance of the Autoencoder\n\n    Parameters\n    ----------\n    encoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n        The encoder network. Defaults to None.\n    bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional\n        The bottleneck encoder network. Defaults to None.\n    bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional\n        The bottleneck decoder network. Defaults to None.\n    decoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n        The decoder network. Defaults to None.\n    input_dim : Optional[Tuple[int, ...]], optional\n        The input dimensions. Used for automatic network generation. Defaults to None.\n    output_dim : Optional[Tuple[int, ...]], optional\n        The output dimensions. Used for automatic network generation. Defaults to None.\n    latent_dim : Optional[int], optional\n        The latent dimensions. Used for automatic network generation. Defaults to None.\n    activation : Optional[Union[list, str]], optional\n        The activation functions for each layer. Used for automatic network generation. Defaults to None.\n    channels : Optional[int], optional\n        The number of channels. Used for automatic network generation. Defaults to None.\n    case : Optional[str], optional\n        The type of problem. Used for automatic network generation. Defaults to None.\n    architecture : Optional[str], optional\n        The network architecture. Used for automatic network generation. Defaults to None.\n    shallow : Optional[bool], optional\n        Whether to use shallow or deep network. Used for automatic network generation. Defaults to False.\n    encoder_activation : str, optional\n        The activation function for the encoder. Defaults to \"relu\".\n    devices : Union[str, list], optional\n        The devices to use. Defaults to \"cpu\".\n    name : str, optional\n        The name of the autoencoder. Defaults to None.\n    \"\"\"\n    super(AutoencoderKoopman, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            channels=channels,\n            architecture=architecture,\n            case=case,\n            shallow=shallow,\n            use_batch_norm=use_batch_norm,\n        )\n\n    self.encoder = encoder.to(self.device)\n    self.decoder = decoder.to(self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.projection = self._projection_with_bottleneck\n        self.reconstruction = self._reconstruction_with_bottleneck\n    else:\n        self.projection = self._projection\n        self.reconstruction = self._reconstruction\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.latent_dimension = None\n\n    if bottleneck_encoder is not None:\n        self.latent_dimension = bottleneck_encoder.output_size\n    else:\n        self.latent_dimension = self.encoder.output_size\n\n    self.K_op = self.to_wrap(entity=torch.nn.Linear(\n        self.latent_dimension, self.latent_dimension, bias=False\n    ).weight, device=self.device)\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward","title":"<code>latent_forward(input_data=None)</code>","text":"<p>Evaluates the operation u^{u+1} = K u^{i}</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward--returns","title":"Returns","text":"<p>torch.Tensor     The computed latent representation.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the operation u^{u+1} = K u^{i}\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed latent representation.\n\n    \"\"\"\n    return torch.matmul(input_data, self.K_op.T)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward_m","title":"<code>latent_forward_m(input_data=None, m=1)</code>","text":"<p>Evaluates the operation u^{u+m} = K^m u^{i}</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward_m--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None. m : int, optional     The number of Koopman iterations. Defaults to 1.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.latent_forward_m--returns","title":"Returns","text":"<p>torch.Tensor     The computed latent representation.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_forward_m(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the operation u^{u+m} = K^m u^{i}\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n    m : int, optional\n        The number of Koopman iterations. Defaults to 1.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed latent representation.\n\n    \"\"\"\n    return torch.matmul(input_data, torch.pow(self.K_op.T, m))\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.predict","title":"<code>predict(input_data=None, n_steps=1)</code>","text":"<p>Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.predict--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None. n_steps : int, optional     The number of extrapolations to perform. Defaults to 1.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.predict--returns","title":"Returns","text":"<p>np.ndarray     The predicted reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def predict(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, n_steps: int = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the reconstructed data for the input data after n_steps extrapolation in the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n    n_steps : int, optional\n        The number of extrapolations to perform. Defaults to 1.\n\n    Returns\n    -------\n    np.ndarray\n        The predicted reconstructed data.\n\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    predictions = list()\n    latent = self.projection(input_data=input_data)\n    init_latent = latent\n\n    # Extrapolating in the latent space over n_steps steps\n    for s in range(n_steps):\n        latent_s = self.latent_forward(input_data=init_latent)\n        init_latent = latent_s\n        predictions.append(latent_s)\n\n    predictions = torch.vstack(predictions)\n\n    reconstructed_predictions = self.reconstruction(input_data=predictions)\n\n    return reconstructed_predictions.detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.project","title":"<code>project(input_data=None)</code>","text":"<p>Projects the input data into the latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.project--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.project--returns","title":"Returns","text":"<p>np.ndarray     The projected data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"\n    Projects the input data into the latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n\n    Returns\n    -------\n    np.ndarray\n        The projected data.\n\n    \"\"\"\n    projected_data = self.projection(input_data=input_data)\n\n    return projected_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruct--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruct--returns","title":"Returns","text":"<p>np.ndarray     The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reconstructs the input data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n\n    Returns\n    -------\n    np.ndarray\n        The reconstructed data.\n\n    \"\"\"\n    reconstructed_data = self.reconstruction(input_data=input_data)\n\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward","title":"<code>reconstruction_forward(input_data=None)</code>","text":"<p>Evaluates the operation \u0168 = D(E(U))</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the operation \u0168 = D(E(U))\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed data.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward_m","title":"<code>reconstruction_forward_m(input_data=None, m=1)</code>","text":"<p>Evaluates the operation \u0168_m = D(K^m E(U))</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward_m--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data. Defaults to None. m : int, optional     The number of Koopman iterations. Defaults to 1.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderKoopman.reconstruction_forward_m--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed data.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward_m(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, m: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the operation \u0168_m = D(K^m E(U))\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data. Defaults to None.\n    m : int, optional\n        The number of Koopman iterations. Defaults to 1.\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed data.\n\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    latent_m = self.latent_forward_m(input_data=latent, m=m)\n    reconstructed_m = self.reconstruction(input_data=latent_m)\n\n    return reconstructed_m\n</code></pre>"},{"location":"simulai_models/#autoencodervariational","title":"AutoencoderVariational","text":"<p>             Bases: <code>NetworkTemplate</code></p> <p>This is an implementation of a Koopman autoencoder as a reduced order model.</p> <p>A variational autoencoder architecture consists of five stages: --&gt; The convolutional encoder [Optional] --&gt; Fully-connected encoder --&gt; Gaussian noise --&gt; Fully connected decoder --&gt; The convolutional decoder [Optional]</p> SCHEME <p>Gaussian noise ^</p> <pre><code>                                   |      |      |\n                                   |  |   |   |  |\n</code></pre> <p>Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til                                        |  |       |  |                                        |             |</p> <pre><code>           ENCODER               DENSE BOTTLENECK           DECODER\n</code></pre> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>class AutoencoderVariational(NetworkTemplate):\n    r\"\"\"\n    This is an implementation of a Koopman autoencoder as a reduced order model.\n\n    A variational autoencoder architecture consists of five stages:\n    --&gt; The convolutional encoder [Optional]\n    --&gt; Fully-connected encoder\n    --&gt; Gaussian noise\n    --&gt; Fully connected decoder\n    --&gt; The convolutional decoder [Optional]\n\n    SCHEME:\n                                                  Gaussian noise\n                                                  ^\n                                           |      |      |\n                                           |  |   |   |  |\n    Z -&gt; [Conv] -&gt; [Conv] -&gt; ... [Conv] -&gt; |  | | - | |  | -&gt; [Conv.T] -&gt; [Conv.T] -&gt; ... [Conv.T] -&gt; Z_til\n                                           |  |       |  |\n                                           |             |\n\n                   ENCODER               DENSE BOTTLENECK           DECODER\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n        bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n        decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n        encoder_activation: str = \"relu\",\n        input_dim: Optional[Tuple[int, ...]] = None,\n        output_dim: Optional[Tuple[int, ...]] = None,\n        latent_dim: Optional[int] = None,\n        activation: Optional[Union[list, str]] = None,\n        channels: Optional[int] = None,\n        kernel_size: Optional[int] = None,\n        case: Optional[str] = None,\n        architecture: Optional[str] = None,\n        use_batch_norm: Optional[bool] = False,\n        shallow: Optional[bool] = False,\n        scale: float = 1e-3,\n        devices: Union[str, list] = \"cpu\",\n        name: str = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Constructor method.\n\n        Parameters\n        ----------\n        encoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n            The encoder network. Defaults to None.\n        bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional\n            The bottleneck encoder network. Defaults to None.\n        bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional\n            The bottleneck decoder network. Defaults to None.\n        decoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n            The decoder network. Defaults to None.\n        encoder_activation : str, optional\n            The activation function to use in the encoder. Defaults to \"relu\".\n        input_dim : Optional[Tuple[int, ...]], optional\n            The input dimension of the data. Defaults to None.\n        output_dim : Optional[Tuple[int, ...]], optional\n            The output dimension of the data. Defaults to None.\n        latent_dim : Optional[int], optional\n            The size of the bottleneck layer. Defaults to None.\n        activation : Optional[Union[list, str]], optional\n            The activation function to use in the networks. Defaults to None.\n        channels : Optional[int], optional\n            The number of channels in the input data. Defaults to None.\n        kernel_size : Optional[int]\n            Convolutional kernel size.\n        case : Optional[str], optional\n            The name of the autoencoder variant. Defaults to None.\n        architecture : Optional[str], optional\n            The architecture of the networks. Defaults to None.\n        shallow : Optional[bool], optional\n            Whether to use a shallow network architecture. Defaults to False.\n        scale : float, optional\n            The scale of the initialization. Defaults to 1e-3.\n        devices : Union[str, list], optional\n            The device(s) to use for computation. Defaults to \"cpu\".\n        name : str, optional\n            The name of the autoencoder. Defaults to None.\n\n        \"\"\"\n        super(AutoencoderVariational, self).__init__(name=name)\n\n        self.weights = list()\n\n        # Determining the kind of device to be used for allocating the\n        # subnetworks\n        self.device = self._set_device(devices=devices)\n\n        self.input_dim = None\n\n        # If not network is provided, the automatic generation\n        # pipeline is activated.\n        if all(\n            [\n                isn == None\n                for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n            ]\n        ):\n            self.input_dim = input_dim\n\n            encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n                input_dim=input_dim,\n                latent_dim=latent_dim,\n                output_dim=output_dim,\n                activation=activation,\n                channels=channels,\n                kernel_size=kernel_size,\n                architecture=architecture,\n                case=case,\n                shallow=shallow,\n                use_batch_norm=use_batch_norm,\n                name=self.name,\n                **kwargs\n            )\n\n        self.encoder = self.to_wrap(entity=encoder, device=self.device)\n        self.decoder = decoder.to(self.device)\n\n        self.add_module(\"encoder\", self.encoder)\n        self.add_module(\"decoder\", self.decoder)\n\n        self.weights += self.encoder.weights\n        self.weights += self.decoder.weights\n\n        self.there_is_bottleneck = False\n\n        # These subnetworks are optional\n        if bottleneck_encoder is not None and bottleneck_decoder is not None:\n            self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n            self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n            self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n            self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n            self.weights += self.bottleneck_encoder.weights\n            self.weights += self.bottleneck_decoder.weights\n\n            self.projection = self._projection_with_bottleneck\n            self.reconstruction = self._reconstruction_with_bottleneck\n\n            self.there_is_bottleneck = True\n\n        else:\n            self.projection = self._projection\n            self.reconstruction = self._reconstruction\n\n        self.last_encoder_channels = None\n        self.before_flatten_dimension = None\n\n        self.latent_dimension = None\n\n        if bottleneck_encoder is not None:\n            self.latent_dimension = bottleneck_encoder.output_size\n        else:\n            self.latent_dimension = self.encoder.output_size\n\n        self.z_mean = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                          self.latent_dimension),\n            device=self.device\n        )\n\n        self.z_log_var = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                            self.latent_dimension),\n            device=self.device\n        )\n\n        self.add_module(\"z_mean\", self.z_mean)\n        self.add_module(\"z_log_var\", self.z_log_var)\n\n        self.weights += [self.z_mean.weight]\n        self.weights += [self.z_log_var.weight]\n\n        self.mu = None\n        self.log_v = None\n        self.scale = scale\n\n        self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n        self.shapes_dict = dict()\n\n    def summary(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        input_shape: list = None,\n        verbose: bool = True,\n        display: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            Input data to pass through the encoder, by default None\n        input_shape : list, optional\n            The shape of the input data if input_data is None, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The output of the autoencoder's decoder applied to the input data.\n\n        Raises\n        ------\n        Exception\n            If self.input_dim is not a tuple or an integer.\n\n        AssertionError\n            If input_shape is None when input_data is None.\n\n        Notes\n        -----\n        The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)\n        \"\"\"\n\n        if verbose == True:\n            if self.input_dim != None:\n                if type(self.input_dim) == tuple:\n                    input_shape = list(self.input_dim)\n                elif type(self.input_dim) == int:\n                    input_shape = [None, self.input_dim]\n                else:\n                    raise Exception(\n                        f\"input_dim is expected to be tuple or int, but received {type(self.input_dim)}\"\n                    )\n            else:\n                pass\n\n            self.encoder.summary(\n                input_data=input_data, input_shape=input_shape, device=self.device, display=display\n            )\n\n            if type(self.encoder.output_size) == tuple:\n                self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n                input_shape = self.encoder.input_size\n            elif type(self.encoder.output_size) == int:\n                input_shape = [None, self.encoder.input_size]\n            else:\n                pass\n\n            if isinstance(input_data, np.ndarray):\n                btnk_input = self.encoder.forward(input_data=input_data)\n            else:\n                assert (\n                    input_shape\n                ), \"It is necessary to have input_shape when input_data is None.\"\n\n                input_shape[0] = 1\n\n                input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n                btnk_input = self.encoder.forward(input_data=input_data)\n\n            before_flatten_dimension = tuple(btnk_input.shape[1:])\n            btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n            # Bottleneck networks is are optional\n            if self.there_is_bottleneck:\n                latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n                self.bottleneck_encoder.summary(display=display)\n                self.bottleneck_decoder.summary(display=display)\n\n                bottleneck_output = self.encoder_activation(\n                    self.bottleneck_decoder.forward(input_data=latent)\n                )\n\n                bottleneck_output = bottleneck_output.reshape(\n                    (-1, *before_flatten_dimension)\n                )\n            else:\n                bottleneck_output = btnk_input\n\n            self.decoder.summary(input_data=bottleneck_output, device=self.device, display=display)\n\n            # Saving the content of the subnetworks to the overall architecture dictionary\n            self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n\n            # Bottleneck networks is are optional\n            if self.there_is_bottleneck:\n                self.shapes_dict.update(\n                    {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n                )\n                self.shapes_dict.update(\n                    {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n                )\n\n            self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n        else:\n            print(self)\n\n    @as_tensor\n    def _projection_with_bottleneck(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the encoder and bottleneck encoder to input data and returns the output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the encoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The output of the bottleneck encoder applied to the input data.\n\n        Notes\n        -----\n        This function is used for projection of the input data into the bottleneck space.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; output_data = autoencoder._projection_with_bottleneck(input_data=input_data)\n        \"\"\"\n        btnk_input = self.encoder.forward(input_data=input_data)\n\n        self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n\n        btnk_input = btnk_input.reshape((-1, np.prod(self.before_flatten_dimension)))\n\n        latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n        return latent\n\n    @as_tensor\n    def _projection(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the encoder to input data and returns the output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the encoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The output of the encoder applied to the input data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; output_data = autoencoder._projection(input_data=input_data)\n        \"\"\"\n        latent = self.encoder.forward(input_data=input_data)\n\n        return latent\n\n    @as_tensor\n    def _reconstruction_with_bottleneck(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the bottleneck decoder and decoder to input data and returns the output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the bottleneck decoder and decoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The output of the decoder applied to the bottleneck decoder's output.\n\n        Notes\n        -----\n        This function is used for reconstruction of the input data from the bottleneck space.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; bottleneck_output = autoencoder._projection_with_bottleneck(input_data=input_data)\n        &gt;&gt;&gt; output_data = autoencoder._reconstruction_with_bottleneck(input_data=bottleneck_output)\n        \"\"\"\n        bottleneck_output = self.encoder_activation(\n            (self.bottleneck_decoder.forward(input_data=input_data))\n        )\n\n        bottleneck_output = bottleneck_output.reshape(\n            (-1,) + self.before_flatten_dimension\n        )\n\n        reconstructed = self.decoder.forward(input_data=bottleneck_output)\n\n        return reconstructed\n\n    @as_tensor\n    def _reconstruction(\n        self, input_data: Union[torch.Tensor, np.ndarray] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the decoder to input data and returns the output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the decoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The output of the decoder applied to the input data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; output_data = autoencoder._reconstruction(input_data=input_data)\n        \"\"\"\n        reconstructed = self.decoder.forward(input_data=input_data)\n\n        return reconstructed\n\n    def Mu(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Computes the mean of the encoded input data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to encode and compute the mean, by default None\n        to_numpy : bool, optional\n            If True, returns the result as a NumPy array, by default False\n\n        Returns\n        -------\n        Union[np.ndarray, torch.Tensor]\n            The mean of the encoded input data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n\n        if to_numpy == True:\n            return self.z_mean(latent).detach().numpy()\n        else:\n            return self.z_mean(latent)\n\n    def Sigma(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Computes the standard deviation of the encoded input data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to encode and compute the standard deviation, by default None\n        to_numpy : bool, optional\n            If True, returns the result as a NumPy array, by default False\n\n        Returns\n        -------\n        Union[np.ndarray, torch.Tensor]\n            The standard deviation of the encoded input data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n\n        if to_numpy == True:\n            return torch.exp(self.z_log_var(latent) / 2).detach().numpy()\n        else:\n            return torch.exp(self.z_log_var(latent) / 2)\n\n    def CoVariance(\n        self,\n        input_data: Union[np.ndarray, torch.Tensor] = None,\n        inv: bool = False,\n        to_numpy: bool = False,\n    ) -&gt; Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Computes the covariance matrix of the encoded input data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to encode and compute the covariance matrix, by default None\n        inv : bool, optional\n            If True, returns the inverse of the covariance matrix, by default False\n        to_numpy : bool, optional\n            If True, returns the result as a NumPy array, by default False\n\n        Returns\n        -------\n        Union[np.ndarray, torch.Tensor]\n            The covariance matrix (or its inverse) of the encoded input data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)\n        \"\"\"\n        if inv == False:\n            Sigma_inv = 1 / self.Sigma(input_data=input_data)\n            covariance = torch.diag_embed(Sigma_inv)\n\n        else:\n            Sigma = self.Sigma(input_data=input_data)\n            covariance = torch.diag_embed(Sigma)\n\n        if to_numpy == True:\n            return covariance.detach().numpy()\n        else:\n            return covariance\n\n    def latent_gaussian_noisy(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Generates a noisy latent representation of the input data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to encode and generate a noisy latent representation, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            A noisy latent representation of the input data.\n\n        Notes\n        -----\n        This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)\n        \"\"\"\n        self.mu = self.z_mean(input_data)\n        self.log_v = self.z_log_var(input_data)\n        eps = self.scale * torch.autograd.Variable(\n            torch.randn(*self.log_v.size())\n        ).type_as(self.log_v)\n\n        return self.mu + torch.exp(self.log_v / 2.0) * eps\n\n    def reconstruction_forward(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the autoencoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed output of the autoencoder.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)\n        \"\"\"\n        latent = self.projection(input_data=input_data)\n        latent_noisy = self.latent_gaussian_noisy(input_data=latent)\n        reconstructed = self.reconstruction(input_data=latent_noisy)\n\n        return reconstructed\n\n    def reconstruction_eval(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to pass through the autoencoder, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            The reconstructed output of the autoencoder.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)\n        \"\"\"\n        encoder_output = self.projection(input_data=input_data)\n        latent = self.z_mean(encoder_output)\n        reconstructed = self.reconstruction(input_data=latent)\n\n        return reconstructed\n\n    def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"\n        Projects the input data onto the autoencoder's latent space.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to project onto the autoencoder's latent space, by default None\n\n        Returns\n        -------\n        np.ndarray\n            The input data projected onto the autoencoder's latent space.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        projected_data_latent = self.Mu(input_data=input_data)\n\n        return projected_data_latent.cpu().detach().numpy()\n\n    def reconstruct(\n        self, input_data: Union[np.ndarray, torch.Tensor] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Reconstructs the input data using the trained autoencoder.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to reconstruct, by default None\n\n        Returns\n        -------\n        np.ndarray\n            The reconstructed data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        reconstructed_data = self.reconstruction(input_data=input_data)\n\n        return reconstructed_data.cpu().detach().numpy()\n\n    def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n        \"\"\"\n        Reconstructs the input data using the mean of the encoded data.\n\n        Parameters\n        ----------\n        input_data : Union[np.ndarray, torch.Tensor], optional\n            The input data to reconstruct, by default None\n\n        Returns\n        -------\n        np.ndarray\n            The reconstructed data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n        &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n        &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n        input_data = input_data.to(self.device)\n\n        return self.reconstruction_eval(input_data=input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.CoVariance","title":"<code>CoVariance(input_data=None, inv=False, to_numpy=False)</code>","text":"<p>Computes the covariance matrix of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.CoVariance--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to encode and compute the covariance matrix, by default None inv : bool, optional     If True, returns the inverse of the covariance matrix, by default False to_numpy : bool, optional     If True, returns the result as a NumPy array, by default False</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.CoVariance--returns","title":"Returns","text":"<p>Union[np.ndarray, torch.Tensor]     The covariance matrix (or its inverse) of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.CoVariance--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) covariance = autoencoder.CoVariance(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def CoVariance(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    inv: bool = False,\n    to_numpy: bool = False,\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Computes the covariance matrix of the encoded input data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to encode and compute the covariance matrix, by default None\n    inv : bool, optional\n        If True, returns the inverse of the covariance matrix, by default False\n    to_numpy : bool, optional\n        If True, returns the result as a NumPy array, by default False\n\n    Returns\n    -------\n    Union[np.ndarray, torch.Tensor]\n        The covariance matrix (or its inverse) of the encoded input data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; covariance = autoencoder.CoVariance(input_data=input_data)\n    \"\"\"\n    if inv == False:\n        Sigma_inv = 1 / self.Sigma(input_data=input_data)\n        covariance = torch.diag_embed(Sigma_inv)\n\n    else:\n        Sigma = self.Sigma(input_data=input_data)\n        covariance = torch.diag_embed(Sigma)\n\n    if to_numpy == True:\n        return covariance.detach().numpy()\n    else:\n        return covariance\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Mu","title":"<code>Mu(input_data=None, to_numpy=False)</code>","text":"<p>Computes the mean of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Mu--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to encode and compute the mean, by default None to_numpy : bool, optional     If True, returns the result as a NumPy array, by default False</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Mu--returns","title":"Returns","text":"<p>Union[np.ndarray, torch.Tensor]     The mean of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Mu--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) mu = autoencoder.Mu(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def Mu(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Computes the mean of the encoded input data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to encode and compute the mean, by default None\n    to_numpy : bool, optional\n        If True, returns the result as a NumPy array, by default False\n\n    Returns\n    -------\n    Union[np.ndarray, torch.Tensor]\n        The mean of the encoded input data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; mu = autoencoder.Mu(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n\n    if to_numpy == True:\n        return self.z_mean(latent).detach().numpy()\n    else:\n        return self.z_mean(latent)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Sigma","title":"<code>Sigma(input_data=None, to_numpy=False)</code>","text":"<p>Computes the standard deviation of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Sigma--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to encode and compute the standard deviation, by default None to_numpy : bool, optional     If True, returns the result as a NumPy array, by default False</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Sigma--returns","title":"Returns","text":"<p>Union[np.ndarray, torch.Tensor]     The standard deviation of the encoded input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.Sigma--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) sigma = autoencoder.Sigma(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def Sigma(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None, to_numpy: bool = False\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Computes the standard deviation of the encoded input data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to encode and compute the standard deviation, by default None\n    to_numpy : bool, optional\n        If True, returns the result as a NumPy array, by default False\n\n    Returns\n    -------\n    Union[np.ndarray, torch.Tensor]\n        The standard deviation of the encoded input data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; sigma = autoencoder.Sigma(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n\n    if to_numpy == True:\n        return torch.exp(self.z_log_var(latent) / 2).detach().numpy()\n    else:\n        return torch.exp(self.z_log_var(latent) / 2)\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.__init__","title":"<code>__init__(encoder=None, bottleneck_encoder=None, bottleneck_decoder=None, decoder=None, encoder_activation='relu', input_dim=None, output_dim=None, latent_dim=None, activation=None, channels=None, kernel_size=None, case=None, architecture=None, use_batch_norm=False, shallow=False, scale=0.001, devices='cpu', name=None, **kwargs)</code>","text":"<p>Constructor method.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.__init__--parameters","title":"Parameters","text":"<p>encoder : Union[ConvolutionalNetwork, DenseNetwork], optional     The encoder network. Defaults to None. bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional     The bottleneck encoder network. Defaults to None. bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional     The bottleneck decoder network. Defaults to None. decoder : Union[ConvolutionalNetwork, DenseNetwork], optional     The decoder network. Defaults to None. encoder_activation : str, optional     The activation function to use in the encoder. Defaults to \"relu\". input_dim : Optional[Tuple[int, ...]], optional     The input dimension of the data. Defaults to None. output_dim : Optional[Tuple[int, ...]], optional     The output dimension of the data. Defaults to None. latent_dim : Optional[int], optional     The size of the bottleneck layer. Defaults to None. activation : Optional[Union[list, str]], optional     The activation function to use in the networks. Defaults to None. channels : Optional[int], optional     The number of channels in the input data. Defaults to None. kernel_size : Optional[int]     Convolutional kernel size. case : Optional[str], optional     The name of the autoencoder variant. Defaults to None. architecture : Optional[str], optional     The architecture of the networks. Defaults to None. shallow : Optional[bool], optional     Whether to use a shallow network architecture. Defaults to False. scale : float, optional     The scale of the initialization. Defaults to 1e-3. devices : Union[str, list], optional     The device(s) to use for computation. Defaults to \"cpu\". name : str, optional     The name of the autoencoder. Defaults to None.</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    bottleneck_encoder: Optional[Union[Linear, DenseNetwork]] = None,\n    bottleneck_decoder: Optional[Union[Linear, DenseNetwork]] = None,\n    decoder: Union[ConvolutionalNetwork, DenseNetwork] = None,\n    encoder_activation: str = \"relu\",\n    input_dim: Optional[Tuple[int, ...]] = None,\n    output_dim: Optional[Tuple[int, ...]] = None,\n    latent_dim: Optional[int] = None,\n    activation: Optional[Union[list, str]] = None,\n    channels: Optional[int] = None,\n    kernel_size: Optional[int] = None,\n    case: Optional[str] = None,\n    architecture: Optional[str] = None,\n    use_batch_norm: Optional[bool] = False,\n    shallow: Optional[bool] = False,\n    scale: float = 1e-3,\n    devices: Union[str, list] = \"cpu\",\n    name: str = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Constructor method.\n\n    Parameters\n    ----------\n    encoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n        The encoder network. Defaults to None.\n    bottleneck_encoder : Optional[Union[Linear, DenseNetwork]], optional\n        The bottleneck encoder network. Defaults to None.\n    bottleneck_decoder : Optional[Union[Linear, DenseNetwork]], optional\n        The bottleneck decoder network. Defaults to None.\n    decoder : Union[ConvolutionalNetwork, DenseNetwork], optional\n        The decoder network. Defaults to None.\n    encoder_activation : str, optional\n        The activation function to use in the encoder. Defaults to \"relu\".\n    input_dim : Optional[Tuple[int, ...]], optional\n        The input dimension of the data. Defaults to None.\n    output_dim : Optional[Tuple[int, ...]], optional\n        The output dimension of the data. Defaults to None.\n    latent_dim : Optional[int], optional\n        The size of the bottleneck layer. Defaults to None.\n    activation : Optional[Union[list, str]], optional\n        The activation function to use in the networks. Defaults to None.\n    channels : Optional[int], optional\n        The number of channels in the input data. Defaults to None.\n    kernel_size : Optional[int]\n        Convolutional kernel size.\n    case : Optional[str], optional\n        The name of the autoencoder variant. Defaults to None.\n    architecture : Optional[str], optional\n        The architecture of the networks. Defaults to None.\n    shallow : Optional[bool], optional\n        Whether to use a shallow network architecture. Defaults to False.\n    scale : float, optional\n        The scale of the initialization. Defaults to 1e-3.\n    devices : Union[str, list], optional\n        The device(s) to use for computation. Defaults to \"cpu\".\n    name : str, optional\n        The name of the autoencoder. Defaults to None.\n\n    \"\"\"\n    super(AutoencoderVariational, self).__init__(name=name)\n\n    self.weights = list()\n\n    # Determining the kind of device to be used for allocating the\n    # subnetworks\n    self.device = self._set_device(devices=devices)\n\n    self.input_dim = None\n\n    # If not network is provided, the automatic generation\n    # pipeline is activated.\n    if all(\n        [\n            isn == None\n            for isn in [encoder, decoder, bottleneck_encoder, bottleneck_decoder]\n        ]\n    ):\n        self.input_dim = input_dim\n\n        encoder, decoder, bottleneck_encoder, bottleneck_decoder = autoencoder_auto(\n            input_dim=input_dim,\n            latent_dim=latent_dim,\n            output_dim=output_dim,\n            activation=activation,\n            channels=channels,\n            kernel_size=kernel_size,\n            architecture=architecture,\n            case=case,\n            shallow=shallow,\n            use_batch_norm=use_batch_norm,\n            name=self.name,\n            **kwargs\n        )\n\n    self.encoder = self.to_wrap(entity=encoder, device=self.device)\n    self.decoder = decoder.to(self.device)\n\n    self.add_module(\"encoder\", self.encoder)\n    self.add_module(\"decoder\", self.decoder)\n\n    self.weights += self.encoder.weights\n    self.weights += self.decoder.weights\n\n    self.there_is_bottleneck = False\n\n    # These subnetworks are optional\n    if bottleneck_encoder is not None and bottleneck_decoder is not None:\n        self.bottleneck_encoder = self.to_wrap(entity=bottleneck_encoder, device=self.device)\n        self.bottleneck_decoder = self.to_wrap(entity=bottleneck_decoder, device=self.device)\n\n        self.add_module(\"bottleneck_encoder\", self.bottleneck_encoder)\n        self.add_module(\"bottleneck_decoder\", self.bottleneck_decoder)\n\n        self.weights += self.bottleneck_encoder.weights\n        self.weights += self.bottleneck_decoder.weights\n\n        self.projection = self._projection_with_bottleneck\n        self.reconstruction = self._reconstruction_with_bottleneck\n\n        self.there_is_bottleneck = True\n\n    else:\n        self.projection = self._projection\n        self.reconstruction = self._reconstruction\n\n    self.last_encoder_channels = None\n    self.before_flatten_dimension = None\n\n    self.latent_dimension = None\n\n    if bottleneck_encoder is not None:\n        self.latent_dimension = bottleneck_encoder.output_size\n    else:\n        self.latent_dimension = self.encoder.output_size\n\n    self.z_mean = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                      self.latent_dimension),\n        device=self.device\n    )\n\n    self.z_log_var = self.to_wrap(entity=torch.nn.Linear(self.latent_dimension,\n                                                        self.latent_dimension),\n        device=self.device\n    )\n\n    self.add_module(\"z_mean\", self.z_mean)\n    self.add_module(\"z_log_var\", self.z_log_var)\n\n    self.weights += [self.z_mean.weight]\n    self.weights += [self.z_log_var.weight]\n\n    self.mu = None\n    self.log_v = None\n    self.scale = scale\n\n    self.encoder_activation = self._get_operation(operation=encoder_activation)\n\n    self.shapes_dict = dict()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.eval","title":"<code>eval(input_data=None)</code>","text":"<p>Reconstructs the input data using the mean of the encoded data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.eval--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to reconstruct, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.eval--returns","title":"Returns","text":"<p>np.ndarray     The reconstructed data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.eval--examples","title":"Examples","text":"<p>autoencoder = Autoencoder(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) reconstructed_data = autoencoder.eval(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def eval(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"\n    Reconstructs the input data using the mean of the encoded data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to reconstruct, by default None\n\n    Returns\n    -------\n    np.ndarray\n        The reconstructed data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; reconstructed_data = autoencoder.eval(input_data=input_data)\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    return self.reconstruction_eval(input_data=input_data).cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy","title":"<code>latent_gaussian_noisy(input_data=None)</code>","text":"<p>Generates a noisy latent representation of the input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to encode and generate a noisy latent representation, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy--returns","title":"Returns","text":"<p>torch.Tensor     A noisy latent representation of the input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy--notes","title":"Notes","text":"<p>This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.latent_gaussian_noisy--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def latent_gaussian_noisy(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generates a noisy latent representation of the input data.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to encode and generate a noisy latent representation, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        A noisy latent representation of the input data.\n\n    Notes\n    -----\n    This function adds Gaussian noise to the mean and standard deviation of the encoded input data to generate a noisy latent representation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; noisy_latent = autoencoder.latent_gaussian_noisy(input_data=input_data)\n    \"\"\"\n    self.mu = self.z_mean(input_data)\n    self.log_v = self.z_log_var(input_data)\n    eps = self.scale * torch.autograd.Variable(\n        torch.randn(*self.log_v.size())\n    ).type_as(self.log_v)\n\n    return self.mu + torch.exp(self.log_v / 2.0) * eps\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.project","title":"<code>project(input_data=None)</code>","text":"<p>Projects the input data onto the autoencoder's latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.project--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to project onto the autoencoder's latent space, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.project--returns","title":"Returns","text":"<p>np.ndarray     The input data projected onto the autoencoder's latent space.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.project--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) projected_data = autoencoder.project(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def project(self, input_data: Union[np.ndarray, torch.Tensor] = None) -&gt; np.ndarray:\n    \"\"\"\n    Projects the input data onto the autoencoder's latent space.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to project onto the autoencoder's latent space, by default None\n\n    Returns\n    -------\n    np.ndarray\n        The input data projected onto the autoencoder's latent space.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; projected_data = autoencoder.project(input_data=input_data)\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    projected_data_latent = self.Mu(input_data=input_data)\n\n    return projected_data_latent.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruct","title":"<code>reconstruct(input_data=None)</code>","text":"<p>Reconstructs the input data using the trained autoencoder.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruct--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to reconstruct, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruct--returns","title":"Returns","text":"<p>np.ndarray     The reconstructed data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruct--examples","title":"Examples","text":"<p>autoencoder = Autoencoder(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) reconstructed_data = autoencoder.reconstruct(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruct(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reconstructs the input data using the trained autoencoder.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to reconstruct, by default None\n\n    Returns\n    -------\n    np.ndarray\n        The reconstructed data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = Autoencoder(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruct(input_data=input_data)\n    \"\"\"\n    if isinstance(input_data, np.ndarray):\n        input_data = torch.from_numpy(input_data.astype(ARRAY_DTYPE))\n\n    input_data = input_data.to(self.device)\n\n    reconstructed_data = self.reconstruction(input_data=input_data)\n\n    return reconstructed_data.cpu().detach().numpy()\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_eval","title":"<code>reconstruction_eval(input_data=None)</code>","text":"<p>Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_eval--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to pass through the autoencoder, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_eval--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed output of the autoencoder.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_eval--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_eval(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the encoder, computes the mean of the encoded data, and then applies the decoder to generate a reconstructed output.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to pass through the autoencoder, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed output of the autoencoder.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_eval(input_data=input_data)\n    \"\"\"\n    encoder_output = self.projection(input_data=input_data)\n    latent = self.z_mean(encoder_output)\n    reconstructed = self.reconstruction(input_data=latent)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_forward","title":"<code>reconstruction_forward(input_data=None)</code>","text":"<p>Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_forward--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     The input data to pass through the autoencoder, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_forward--returns","title":"Returns","text":"<p>torch.Tensor     The reconstructed output of the autoencoder.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.reconstruction_forward--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def reconstruction_forward(\n    self, input_data: Union[np.ndarray, torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the encoder, adds Gaussian noise to the encoded data, and then applies the decoder to generate a reconstructed output.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        The input data to pass through the autoencoder, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed output of the autoencoder.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; reconstructed_data = autoencoder.reconstruction_forward(input_data=input_data)\n    \"\"\"\n    latent = self.projection(input_data=input_data)\n    latent_noisy = self.latent_gaussian_noisy(input_data=latent)\n    reconstructed = self.reconstruction(input_data=latent_noisy)\n\n    return reconstructed\n</code></pre>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary","title":"<code>summary(input_data=None, input_shape=None, verbose=True, display=True)</code>","text":"<p>Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary--parameters","title":"Parameters","text":"<p>input_data : Union[np.ndarray, torch.Tensor], optional     Input data to pass through the encoder, by default None input_shape : list, optional     The shape of the input data if input_data is None, by default None</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary--returns","title":"Returns","text":"<p>torch.Tensor     The output of the autoencoder's decoder applied to the input data.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary--raises","title":"Raises","text":"<p>Exception     If self.input_dim is not a tuple or an integer.</p> <p>AssertionError     If input_shape is None when input_data is None.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary--notes","title":"Notes","text":"<p>The summary method calls the <code>summary</code> method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.</p>"},{"location":"simulai_models/#simulai.models.AutoencoderVariational.summary--examples","title":"Examples","text":"<p>autoencoder = AutoencoderVariational(input_dim=(28, 28, 1)) input_data = np.random.rand(1, 28, 28, 1) output_data = autoencoder.summary(input_data=input_data)</p> Source code in <code>simulai/models/_pytorch_models/_autoencoder.py</code> <pre><code>def summary(\n    self,\n    input_data: Union[np.ndarray, torch.Tensor] = None,\n    input_shape: list = None,\n    verbose: bool = True,\n    display: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Summarizes the overall architecture of the autoencoder and saves the content of the subnetworks to a dictionary.\n\n    Parameters\n    ----------\n    input_data : Union[np.ndarray, torch.Tensor], optional\n        Input data to pass through the encoder, by default None\n    input_shape : list, optional\n        The shape of the input data if input_data is None, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        The output of the autoencoder's decoder applied to the input data.\n\n    Raises\n    ------\n    Exception\n        If self.input_dim is not a tuple or an integer.\n\n    AssertionError\n        If input_shape is None when input_data is None.\n\n    Notes\n    -----\n    The summary method calls the `summary` method of each of the subnetworks and saves the content of the subnetworks to the overall architecture dictionary. If there is a bottleneck network, it is also summarized and saved to the architecture dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; autoencoder = AutoencoderVariational(input_dim=(28, 28, 1))\n    &gt;&gt;&gt; input_data = np.random.rand(1, 28, 28, 1)\n    &gt;&gt;&gt; output_data = autoencoder.summary(input_data=input_data)\n    \"\"\"\n\n    if verbose == True:\n        if self.input_dim != None:\n            if type(self.input_dim) == tuple:\n                input_shape = list(self.input_dim)\n            elif type(self.input_dim) == int:\n                input_shape = [None, self.input_dim]\n            else:\n                raise Exception(\n                    f\"input_dim is expected to be tuple or int, but received {type(self.input_dim)}\"\n                )\n        else:\n            pass\n\n        self.encoder.summary(\n            input_data=input_data, input_shape=input_shape, device=self.device, display=display\n        )\n\n        if type(self.encoder.output_size) == tuple:\n            self.before_flatten_dimension = tuple(self.encoder.output_size[1:])\n            input_shape = self.encoder.input_size\n        elif type(self.encoder.output_size) == int:\n            input_shape = [None, self.encoder.input_size]\n        else:\n            pass\n\n        if isinstance(input_data, np.ndarray):\n            btnk_input = self.encoder.forward(input_data=input_data)\n        else:\n            assert (\n                input_shape\n            ), \"It is necessary to have input_shape when input_data is None.\"\n\n            input_shape[0] = 1\n\n            input_data = self.to_wrap(entity=torch.ones(input_shape), device=self.device)\n\n            btnk_input = self.encoder.forward(input_data=input_data)\n\n        before_flatten_dimension = tuple(btnk_input.shape[1:])\n        btnk_input = btnk_input.reshape((-1, np.prod(btnk_input.shape[1:])))\n\n        # Bottleneck networks is are optional\n        if self.there_is_bottleneck:\n            latent = self.bottleneck_encoder.forward(input_data=btnk_input)\n\n            self.bottleneck_encoder.summary(display=display)\n            self.bottleneck_decoder.summary(display=display)\n\n            bottleneck_output = self.encoder_activation(\n                self.bottleneck_decoder.forward(input_data=latent)\n            )\n\n            bottleneck_output = bottleneck_output.reshape(\n                (-1, *before_flatten_dimension)\n            )\n        else:\n            bottleneck_output = btnk_input\n\n        self.decoder.summary(input_data=bottleneck_output, device=self.device, display=display)\n\n        # Saving the content of the subnetworks to the overall architecture dictionary\n        self.shapes_dict.update({\"encoder\": self.encoder.shapes_dict})\n\n        # Bottleneck networks is are optional\n        if self.there_is_bottleneck:\n            self.shapes_dict.update(\n                {\"bottleneck_encoder\": self.bottleneck_encoder.shapes_dict}\n            )\n            self.shapes_dict.update(\n                {\"bottleneck_decoder\": self.bottleneck_decoder.shapes_dict}\n            )\n\n        self.shapes_dict.update({\"decoder\": self.decoder.shapes_dict})\n\n    else:\n        print(self)\n</code></pre>"},{"location":"simulai_residuals/","title":"simulai.residuals","text":"<p>             Bases: <code>Module</code></p> <p>The SymbolicOperatorClass is a class that constructs tensor operators using symbolic expressions written in PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>list</code> <p>List of expressions representing the operator. Defaults to None.</p> <code>None</code> <code>input_vars</code> <code>list</code> <p>List of input variables. Defaults to None.</p> <code>None</code> <code>output_vars</code> <code>list</code> <p>List of output variables. Defaults to None.</p> <code>None</code> <code>function</code> <code>callable</code> <p>A callable that represents the evaluation of the tensor operator. Defaults to None.</p> <code>None</code> <code>gradient</code> <code>callable</code> <p>A callable that represents the gradient evaluation of the tensor operator. Defaults to None.</p> <code>None</code> <code>keys</code> <code>str</code> <p>The keys for accessing the inputs in the input dictionary. Defaults to None.</p> <code>None</code> <code>inputs_key</code> <code>str</code> <p>The key for accessing the input data in the input dictionary. Defaults to None.</p> <code>None</code> <code>processing</code> <code>str</code> <p>The processing method to be used during the evaluation of the tensor operator. Defaults to 'serial'.</p> <code>'serial'</code> <code>device</code> <code>str</code> <p>The device to be used during the evaluation of the tensor operator. Can be either 'cpu' or 'gpu'. Defaults to 'cpu'.</p> <code>'cpu'</code> <code>auxiliary_expressions</code> <code>list</code> <p>Additional expressions that are needed to evaluate the tensor operator. Defaults to None.</p> <code>None</code> <code>constants</code> <code>dict</code> <p>Constants that are required during the evaluation of the tensor operator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>object</code> <p>An instance of the SymbolicOperatorClass.</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>class SymbolicOperator(torch.nn.Module):\n    \"\"\"\n    The SymbolicOperatorClass is a class that constructs tensor operators using symbolic expressions written in PyTorch.\n\n    Args:\n        expressions (list, optional): List of expressions representing the operator. Defaults to None.\n        input_vars (list, optional): List of input variables. Defaults to None.\n        output_vars (list, optional): List of output variables. Defaults to None.\n        function (callable, optional): A callable that represents the evaluation of the tensor operator. Defaults to None.\n        gradient (callable, optional): A callable that represents the gradient evaluation of the tensor operator. Defaults to None.\n        keys (str, optional): The keys for accessing the inputs in the input dictionary. Defaults to None.\n        inputs_key (str, optional): The key for accessing the input data in the input dictionary. Defaults to None.\n        processing (str, optional): The processing method to be used during the evaluation of the tensor operator. Defaults to 'serial'.\n        device (str, optional): The device to be used during the evaluation of the tensor operator. Can be either 'cpu' or 'gpu'. Defaults to 'cpu'.\n        auxiliary_expressions (list, optional): Additional expressions that are needed to evaluate the tensor operator. Defaults to None.\n        constants (dict, optional): Constants that are required during the evaluation of the tensor operator. Defaults to None.\n\n    Returns:\n        object: An instance of the SymbolicOperatorClass.\n    \"\"\"\n\n    def __init__(\n        self,\n        expressions: List[Union[sympy.Expr, str]] = None,\n        input_vars: List[Union[sympy.Symbol, str]] = None,\n        output_vars: List[Union[sympy.Symbol, str]] = None,\n        function: callable = None,\n        gradient: callable = None,\n        keys: str = None,\n        inputs_key=None,\n        constants: dict = None,\n        trainable_parameters: dict = None,\n        external_functions: dict = dict(),\n        processing: str = \"serial\",\n        device: str = \"cpu\",\n        engine: str = \"torch\",\n        auxiliary_expressions: list = None,\n    ) -&gt; None:\n        if engine == \"torch\":\n            super(SymbolicOperator, self).__init__()\n        else:\n            pass\n\n        self.engine = importlib.import_module(engine)\n\n        self.constants = constants\n\n        if trainable_parameters is not None:\n            self.trainable_parameters = trainable_parameters\n\n        else:\n            self.trainable_parameters = dict()\n\n        self.external_functions = external_functions\n        self.processing = processing\n        self.periodic_bc_protected_key = \"periodic\"\n\n        self.protected_funcs = [\"cos\", \"sin\", \"sqrt\", \"exp\"]\n        self.protected_operators = [\"L\", \"Div\", \"Identity\", \"Kronecker\"]\n\n        self.protected_funcs_subs = self._construct_protected_functions()\n        self.protected_operators_subs = self._construct_implict_operators()\n\n        # Configuring the device to be used during the fitting process\n        if device == \"gpu\":\n            if not torch.cuda.is_available():\n                print(\"Warning: There is no GPU available, using CPU instead.\")\n                device = \"cpu\"\n            else:\n                device = \"cuda\"\n                print(\"Using GPU.\")\n        elif device == \"cpu\":\n            print(\"Using CPU.\")\n        else:\n            raise Exception(f\"The device must be cpu or gpu, but received: {device}\")\n\n        self.device = device\n\n        self.expressions = [self._parse_expression(expr=expr) for expr in expressions]\n\n        if isinstance(auxiliary_expressions, dict):\n            self.auxiliary_expressions = {\n                key: self._parse_expression(expr=expr)\n                for key, expr in auxiliary_expressions.items()\n            }\n        else:\n            self.auxiliary_expressions = auxiliary_expressions\n\n        self.input_vars = [self._parse_variable(var=var) for var in input_vars]\n        self.output_vars = [self._parse_variable(var=var) for var in output_vars]\n\n        self.input_names = [var.name for var in self.input_vars]\n        self.output_names = [var.name for var in self.output_vars]\n        self.keys = keys\n\n        if inputs_key != None:\n            self.inputs_key = self._parse_inputs_key(inputs_key=inputs_key)\n        else:\n            self.inputs_key = inputs_key\n\n        self.all_vars = self.input_vars + self.output_vars\n\n        if self.inputs_key is not None:\n            self.forward = self._forward_dict\n        else:\n            self.forward = self._forward_tensor\n\n        self.function = function\n        self.diff_symbol = D\n\n        self.output = None\n\n        self.f_expressions = list()\n        self.g_expressions = dict()\n\n        self.feed_vars = None\n\n        for name in self.output_names:\n            setattr(self, name, None)\n\n        # Defining functions for returning each variable of the regression\n        # function\n        for index, name in enumerate(self.output_names):\n            setattr(\n                self,\n                name,\n                lambda data: self.function.forward(input_data=data)[..., index][\n                    ..., None\n                ],\n            )\n\n        # If no external gradient is provided, use the core gradient evaluator\n        if gradient is None:\n            gradient_function = self.gradient\n        else:\n            gradient_function = gradient\n\n        subs = {self.diff_symbol.name: gradient_function}\n        subs.update(self.external_functions)\n        subs.update(self.protected_funcs_subs)\n\n        for expr in self.expressions:\n            if not callable(expr):\n                f_expr = sympy.lambdify(self.all_vars, expr, subs)\n            else:\n                f_expr = expr\n\n            self.f_expressions.append(f_expr)\n\n        if self.auxiliary_expressions is not None:\n            for key, expr in self.auxiliary_expressions.items():\n                if not callable(expr):\n                    g_expr = sympy.lambdify(self.all_vars, expr, subs)\n                else:\n                    g_expr = expr\n\n                self.g_expressions[key] = g_expr\n\n        # Method for executing the expressions evaluation\n        if self.processing == \"serial\":\n            self.process_expression = self._process_expression_serial\n        else:\n            raise Exception(f\"Processing case {self.processing} not supported.\")\n\n    def _construct_protected_functions(self):\n        \"\"\"\n        This function creates a dictionary of protected functions from the engine object attribute.\n\n        Args:\n            self (object): The instance of the class that calls this function.\n\n        Returns:\n            dict: A dictionary of function names and their corresponding function objects.\n        \"\"\"\n        protected_funcs = {\n            func: getattr(self.engine, func) for func in self.protected_funcs\n        }\n\n        return protected_funcs\n\n    def _construct_implict_operators(self):\n        \"\"\"\n        This function creates a dictionary of protected operators from the operators engine module.\n\n        Args:\n            self (object): The instance of the class that calls this function.\n\n        Returns:\n            dict: A dictionary of operator names and their corresponding function objects.\n        \"\"\"\n        operators_engine = importlib.import_module(\"simulai.tokens\")\n\n        protected_operators = {\n            func: getattr(operators_engine, func) for func in self.protected_operators\n        }\n\n        return protected_operators\n\n    def _parse_key_interval(self, intv:str) -&gt; List:\n\n        begin, end = intv.split(\",\")\n\n        end = int(end[:-1])\n        begin = int(begin)\n        end = int(end + 1)\n\n        return np.arange(begin, end).astype(int).tolist()\n\n    def _parse_inputs_key(self, inputs_key: str = None) -&gt; dict:\n        # Sentences separator: '|'\n        sep = \"|\"\n        # Index identifier: ':'\n        inx = \":\"\n        # Interval identifier\n        intv = \"[\"\n\n        # Removing possible spaces in the inputs_key string\n        inputs_key = inputs_key.replace(\" \", \"\")\n\n        try:\n            split_components = inputs_key.split(sep)\n        except ValueError:\n            split_components = inputs_key\n\n        keys_dict = dict()\n        for s in split_components:\n            try:\n                if len(s.split(inx)) &gt; 1:\n                    key, index = s.split(inx)\n\n                    if not key in keys_dict:\n                        keys_dict[key] = list()\n                        keys_dict[key].append(int(index))\n\n                    else:\n                        keys_dict[key].append(int(index))\n\n                elif len(s.split(intv)) &gt; 1:\n\n                    key, interval_str = s.split(intv)\n                    interval = self._parse_key_interval(interval_str)\n                    keys_dict[key] = interval\n\n                else:\n                    raise ValueError\n\n            except ValueError:\n                keys_dict[s] = -1\n\n        return keys_dict\n\n    def _collect_data_from_inputs_list(self, inputs_list: dict = None) -&gt; list:\n        data = list()\n        for k, v in self.inputs_key.items():\n            if v == -1:\n                if inputs_list[k].shape[1] == 1:\n                    data_ = [inputs_list[k]]\n                else:\n                    data_ = list(torch.split(inputs_list[k], 1, dim=1))\n            else:\n                data_ = [inputs_list[k][:, i : i + 1] for i in v]\n\n            data += data_\n\n        return data\n\n    def _parse_expression(self, expr=Union[sympy.Expr, str]) -&gt; sympy.Expr:\n        \"\"\"\n        Parses the input expression and returns a SymPy expression.\n\n        Parameters\n        ----------\n        expr : Union[sympy.Expr, str], optional\n            The expression to parse, by default None. It can either be a SymPy expression or a string.\n\n        Returns\n        -------\n        sympy.Expr\n            The parsed SymPy expression.\n\n        Raises\n        ------\n        Exception\n            If the `constants` attribute is not defined, and the input expression is a string.\n        \"\"\"\n        if isinstance(expr, str):\n            try:\n                expr_ = sympify(\n                    expr, locals=self.protected_operators_subs, evaluate=False\n                )\n\n                if self.constants is not None:\n                    expr_ = expr_.subs(self.constants)\n                if self.trainable_parameters is not None:\n                    expr_ = expr_.subs(self.trainable_parameters)\n            except ValueError:\n                if self.constants is not None:\n                    _expr = expr\n                    for key, value in self.constants.items():\n                        _expr = _expr.replace(key, str(value))\n\n                    expr_ = parse_expr(_expr, evaluate=0)\n                else:\n                    raise Exception(\"It is necessary to define a constants dict.\")\n        elif callable(expr):\n            expr_ = expr\n        else:\n            if self.constants is not None:\n                expr_ = expr.subs(self.constants)\n            else:\n                expr_ = expr\n\n        return expr_\n\n    def _parse_variable(self, var=Union[sympy.Symbol, str]) -&gt; sympy.Symbol:\n        \"\"\"\n        Parse the input variable and return a SymPy Symbol.\n\n        Parameters\n        ----------\n        var : Union[sympy.Symbol, str], optional\n            The input variable, either a SymPy Symbol or a string.\n\n        Returns\n        -------\n        sympy.Symbol\n            A SymPy Symbol representing the input variable.\n        \"\"\"\n        if isinstance(var, str):\n            return sympy.Symbol(var)\n        else:\n            return var\n\n    def _forward_tensor(self, input_data: torch.Tensor = None) -&gt; torch.Tensor:\n        \"\"\"\n        Forward the input tensor through the function.\n\n        Parameters\n        ----------\n        input_data : torch.Tensor, optional\n            The input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            The output tensor after forward pass.\n        \"\"\"\n        return self.function.forward(input_data=input_data)\n\n    def _forward_dict(self, input_data: dict = None) -&gt; torch.Tensor:\n        \"\"\"\n        Forward the input dictionary through the function.\n\n        Parameters\n        ----------\n        input_data : dict, optional\n            The input dictionary.\n\n        Returns\n        -------\n        torch.Tensor\n            The output tensor after forward pass.\n        \"\"\"\n        return self.function.forward(**input_data)\n\n    def _process_expression_serial(self, feed_vars: dict = None) -&gt; List[torch.Tensor]:\n        \"\"\"\n        Process the expression list serially using the given feed variables.\n\n        Parameters\n        ----------\n        feed_vars : dict, optional\n            The feed variables.\n\n        Returns\n        -------\n        List[torch.Tensor]\n            A list of tensors after evaluating the expressions serially.\n        \"\"\"\n        return [f(**feed_vars).to(self.device) for f in self.f_expressions]\n\n    def _process_expression_individual(\n        self, index: int = None, feed_vars: dict = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates a single expression specified by index from the f_expressions list with given feed variables.\n\n        Parameters\n        ----------\n        index : int, optional\n            Index of the expression to be evaluated, by default None\n        feed_vars : dict, optional\n            Dictionary of feed variables, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            Result of evaluating the specified expression with given feed variables\n        \"\"\"\n        return self.f_expressions[index](**feed_vars).to(self.device)\n\n    def __call__(\n        self, inputs_data: Union[np.ndarray, dict] = None\n    ) -&gt; List[torch.Tensor]:\n        \"\"\"\n        Evaluate the symbolic expression.\n\n        This function takes either a numpy array or a dictionary of numpy arrays as input.\n\n        Parameters:\n        inputs_data (Union[np.ndarray, dict]): Input data to evaluate the expression. If inputs_data is a numpy array,\n        it must have the same length as the input_names attribute. If inputs_data is a dictionary, it must have\n        key-value pairs where the keys correspond to the names in the input_names attribute.\n\n        Returns:\n        List[torch.Tensor]: A list of tensors containing the evaluated expressions.\n\n        Raises:\n        Exception: If inputs_data is not a numpy array or a dictionary, or if the inputs_data is a dictionary and the key\n        does not match with the inputs_key attribute.\n        \"\"\"\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        inputs_list = constructor(input_data=inputs_data, device=self.device)\n\n        output = self.forward(input_data=inputs_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs = {key: value for key, value in zip(self.output_names, outputs_list)}\n\n        if type(inputs_list) is list:\n            inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n\n        elif type(inputs_list) is dict:\n            assert (\n                self.inputs_key is not None\n            ), \"If inputs_list is dict, \\\n                it is necessary to provide\\\n                a key.\"\n\n            inputs_list = self._collect_data_from_inputs_list(inputs_list=inputs_list)\n\n            inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n        else:\n            raise Exception(\n                f\"Format {type(inputs_list)} not supported \\\n                            for inputs_list\"\n            )\n\n        feed_vars = {**outputs, **inputs}\n\n        # It returns a list of tensors containing the expressions\n        # evaluated over a domain\n        return self.process_expression(feed_vars=feed_vars)\n\n    def eval_expression(self, key, inputs_list):\n        \"\"\"\n        This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.\n\n        Parameters\n        ----------\n        key : str\n            the key used to retrieve the expression from the 'g_expressions' attribute\n        inputs_list: list\n            either a list of arrays, an np.ndarray, or a dict containing the inputs to the function\n\n        Raises:\n        -------\n        Exception: if the expression does not exist in 'g_expressions'\n        Exception: if the input format is not supported\n        Exception: if the inputs_list is a dict but the 'inputs_key' attribute is not provided\n\n        Returns\n        -------\n        result (float/torch tensor): the result of evaluating the expression using the inputs.\n        \"\"\"\n\n        try:\n            g = self.g_expressions.get(key)\n        except:\n            raise Exception(f\"The expression {key} does not exist.\")\n\n        # Periodic boundary conditions\n        if self.periodic_bc_protected_key in key:\n            assert isinstance(inputs_list, list), (\n                \"When a periodic boundary expression is used,\"\n                \" the input must be a list of arrays.\"\n            )\n\n            # Lower bound\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            tensors_list = constructor(input_data=inputs_list[0], device=self.device)\n\n            inputs_L = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n            output = self.function.forward(input_data=tensors_list)\n\n            output = output.to(self.device)  # TODO Check if it is necessary\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs_L = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            feed_vars_L = {**inputs_L, **outputs_L}\n\n            # Upper bound\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            tensors_list = constructor(input_data=inputs_list[-1], device=self.device)\n\n            inputs_U = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n            output = self.function.forward(input_data=tensors_list)\n\n            output = output.to(self.device)  # TODO Check if it is necessary\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs_U = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            feed_vars_U = {**inputs_U, **outputs_U}\n\n            # Evaluating the boundaries equality\n            return g(**feed_vars_L) - g(**feed_vars_U)\n\n        # The non-periodic cases\n        else:\n            constructor = MakeTensor(\n                input_names=self.input_names, output_names=self.output_names\n            )\n\n            inputs_list = constructor(input_data=inputs_list, device=self.device)\n\n            output = self.function.forward(input_data=inputs_list)\n\n            outputs_list = torch.split(output, 1, dim=-1)\n\n            outputs = {\n                key: value for key, value in zip(self.output_names, outputs_list)\n            }\n\n            if type(inputs_list) is list:\n                inputs = {\n                    key: value for key, value in zip(self.input_names, inputs_list)\n                }\n\n            elif type(inputs_list) is np.ndarray:\n                arrays_list = np.split(inputs_list, inputs_list.shape[1], axis=1)\n                tensors_list = [torch.from_numpy(arr) for arr in arrays_list]\n\n                for t in tensors_list:\n                    t.requires_grad = True\n\n                inputs = {\n                    key: value for key, value in zip(self.input_names, tensors_list)\n                }\n\n            elif type(inputs_list) is dict:\n                assert (\n                    self.inputs_key is not None\n                ), \"If inputs_list is dict, \\\n                                                     it is necessary to provide\\\n                                                     a key.\"\n\n                inputs = {\n                    key: value\n                    for key, value in zip(\n                        self.input_names, inputs_list[self.inputs_key]\n                    )\n                }\n\n            else:\n                raise Exception(\n                    f\"Format {type(inputs_list)} not supported \\\n                                for inputs_list\"\n                )\n\n            feed_vars = {**inputs, **outputs}\n\n            return g(**feed_vars)\n\n    @staticmethod\n    def gradient(feature, param):\n        \"\"\"\n        Calculates the gradient of the given feature with respect to the given parameter.\n\n        Parameters\n        ----------\n        feature : torch.Tensor\n            Tensor with the input feature.\n        param : torch.Tensor\n            Tensor with the parameter to calculate the gradient with respect to.\n\n        Returns\n        -------\n        grad_ : torch.Tensor\n            Tensor with the gradient of the feature with respect to the given parameter.\n\n        Examples\n        --------\n        &gt;&gt;&gt; feature = torch.tensor([1, 2, 3], dtype=torch.float32)\n        &gt;&gt;&gt; param = torch.tensor([2, 3, 4], dtype=torch.float32)\n        &gt;&gt;&gt; gradient(feature, param)\n        tensor([1., 1., 1.], grad_fn=&lt;AddBackward0&gt;)\n        \"\"\"\n        grad_ = grad(\n            feature,\n            param,\n            grad_outputs=torch.ones_like(feature),\n            create_graph=True,\n            allow_unused=True,\n            retain_graph=True,\n        )\n\n        return grad_[0]\n\n    def jac(self, inputs):\n        \"\"\"\n        Calculates the Jacobian of the forward function of the model with respect to its inputs.\n\n        Parameters\n        ----------\n        inputs : torch.Tensor\n            Tensor with the input data to the forward function.\n\n        Returns\n        -------\n        jac_ : torch.Tensor\n            Tensor with the Jacobian of the forward function with respect to its inputs.\n\n        Examples\n        --------\n        &gt;&gt;&gt; inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32)\n        &gt;&gt;&gt; jac(inputs)\n        tensor([[1., 1., 1.],\n                [1., 1., 1.]], grad_fn=&lt;MulBackward0&gt;)\n        \"\"\"\n\n        def inner(inputs):\n            return self.forward(input_data=inputs)\n\n        return jacobian(inner, inputs)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.__call__","title":"<code>__call__(inputs_data=None)</code>","text":"<p>Evaluate the symbolic expression.</p> <p>This function takes either a numpy array or a dictionary of numpy arrays as input.</p> <p>Parameters: inputs_data (Union[np.ndarray, dict]): Input data to evaluate the expression. If inputs_data is a numpy array, it must have the same length as the input_names attribute. If inputs_data is a dictionary, it must have key-value pairs where the keys correspond to the names in the input_names attribute.</p> <p>Returns: List[torch.Tensor]: A list of tensors containing the evaluated expressions.</p> <p>Raises: Exception: If inputs_data is not a numpy array or a dictionary, or if the inputs_data is a dictionary and the key does not match with the inputs_key attribute.</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def __call__(\n    self, inputs_data: Union[np.ndarray, dict] = None\n) -&gt; List[torch.Tensor]:\n    \"\"\"\n    Evaluate the symbolic expression.\n\n    This function takes either a numpy array or a dictionary of numpy arrays as input.\n\n    Parameters:\n    inputs_data (Union[np.ndarray, dict]): Input data to evaluate the expression. If inputs_data is a numpy array,\n    it must have the same length as the input_names attribute. If inputs_data is a dictionary, it must have\n    key-value pairs where the keys correspond to the names in the input_names attribute.\n\n    Returns:\n    List[torch.Tensor]: A list of tensors containing the evaluated expressions.\n\n    Raises:\n    Exception: If inputs_data is not a numpy array or a dictionary, or if the inputs_data is a dictionary and the key\n    does not match with the inputs_key attribute.\n    \"\"\"\n    constructor = MakeTensor(\n        input_names=self.input_names, output_names=self.output_names\n    )\n\n    inputs_list = constructor(input_data=inputs_data, device=self.device)\n\n    output = self.forward(input_data=inputs_list)\n\n    output = output.to(self.device)  # TODO Check if it is necessary\n\n    outputs_list = torch.split(output, 1, dim=-1)\n\n    outputs = {key: value for key, value in zip(self.output_names, outputs_list)}\n\n    if type(inputs_list) is list:\n        inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n\n    elif type(inputs_list) is dict:\n        assert (\n            self.inputs_key is not None\n        ), \"If inputs_list is dict, \\\n            it is necessary to provide\\\n            a key.\"\n\n        inputs_list = self._collect_data_from_inputs_list(inputs_list=inputs_list)\n\n        inputs = {key: value for key, value in zip(self.input_names, inputs_list)}\n    else:\n        raise Exception(\n            f\"Format {type(inputs_list)} not supported \\\n                        for inputs_list\"\n        )\n\n    feed_vars = {**outputs, **inputs}\n\n    # It returns a list of tensors containing the expressions\n    # evaluated over a domain\n    return self.process_expression(feed_vars=feed_vars)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.eval_expression","title":"<code>eval_expression(key, inputs_list)</code>","text":"<p>This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.eval_expression--parameters","title":"Parameters","text":"<p>key : str     the key used to retrieve the expression from the 'g_expressions' attribute inputs_list: list     either a list of arrays, an np.ndarray, or a dict containing the inputs to the function</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.eval_expression--raises","title":"Raises:","text":"<p>Exception: if the expression does not exist in 'g_expressions' Exception: if the input format is not supported Exception: if the inputs_list is a dict but the 'inputs_key' attribute is not provided</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.eval_expression--returns","title":"Returns","text":"<p>result (float/torch tensor): the result of evaluating the expression using the inputs.</p> Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def eval_expression(self, key, inputs_list):\n    \"\"\"\n    This function evaluates an expression stored in the class attribute 'g_expressions' using the inputs in 'inputs_list'. If the expression has a periodic boundary condition, the function evaluates the expression at the lower and upper boundaries and returns the difference. If the inputs are provided as a list, they are split into individual tensors and stored in a dictionary with the keys as the input names. If the inputs are provided as an np.ndarray, they are converted to tensors and split along the second axis. If the inputs are provided as a dict, they are extracted using the 'inputs_key' attribute. The inputs, along with the outputs obtained from running the function, are then passed as arguments to the expression using the 'g(**feed_vars)' syntax.\n\n    Parameters\n    ----------\n    key : str\n        the key used to retrieve the expression from the 'g_expressions' attribute\n    inputs_list: list\n        either a list of arrays, an np.ndarray, or a dict containing the inputs to the function\n\n    Raises:\n    -------\n    Exception: if the expression does not exist in 'g_expressions'\n    Exception: if the input format is not supported\n    Exception: if the inputs_list is a dict but the 'inputs_key' attribute is not provided\n\n    Returns\n    -------\n    result (float/torch tensor): the result of evaluating the expression using the inputs.\n    \"\"\"\n\n    try:\n        g = self.g_expressions.get(key)\n    except:\n        raise Exception(f\"The expression {key} does not exist.\")\n\n    # Periodic boundary conditions\n    if self.periodic_bc_protected_key in key:\n        assert isinstance(inputs_list, list), (\n            \"When a periodic boundary expression is used,\"\n            \" the input must be a list of arrays.\"\n        )\n\n        # Lower bound\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        tensors_list = constructor(input_data=inputs_list[0], device=self.device)\n\n        inputs_L = {\n            key: value for key, value in zip(self.input_names, tensors_list)\n        }\n\n        output = self.function.forward(input_data=tensors_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs_L = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        feed_vars_L = {**inputs_L, **outputs_L}\n\n        # Upper bound\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        tensors_list = constructor(input_data=inputs_list[-1], device=self.device)\n\n        inputs_U = {\n            key: value for key, value in zip(self.input_names, tensors_list)\n        }\n\n        output = self.function.forward(input_data=tensors_list)\n\n        output = output.to(self.device)  # TODO Check if it is necessary\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs_U = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        feed_vars_U = {**inputs_U, **outputs_U}\n\n        # Evaluating the boundaries equality\n        return g(**feed_vars_L) - g(**feed_vars_U)\n\n    # The non-periodic cases\n    else:\n        constructor = MakeTensor(\n            input_names=self.input_names, output_names=self.output_names\n        )\n\n        inputs_list = constructor(input_data=inputs_list, device=self.device)\n\n        output = self.function.forward(input_data=inputs_list)\n\n        outputs_list = torch.split(output, 1, dim=-1)\n\n        outputs = {\n            key: value for key, value in zip(self.output_names, outputs_list)\n        }\n\n        if type(inputs_list) is list:\n            inputs = {\n                key: value for key, value in zip(self.input_names, inputs_list)\n            }\n\n        elif type(inputs_list) is np.ndarray:\n            arrays_list = np.split(inputs_list, inputs_list.shape[1], axis=1)\n            tensors_list = [torch.from_numpy(arr) for arr in arrays_list]\n\n            for t in tensors_list:\n                t.requires_grad = True\n\n            inputs = {\n                key: value for key, value in zip(self.input_names, tensors_list)\n            }\n\n        elif type(inputs_list) is dict:\n            assert (\n                self.inputs_key is not None\n            ), \"If inputs_list is dict, \\\n                                                 it is necessary to provide\\\n                                                 a key.\"\n\n            inputs = {\n                key: value\n                for key, value in zip(\n                    self.input_names, inputs_list[self.inputs_key]\n                )\n            }\n\n        else:\n            raise Exception(\n                f\"Format {type(inputs_list)} not supported \\\n                            for inputs_list\"\n            )\n\n        feed_vars = {**inputs, **outputs}\n\n        return g(**feed_vars)\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.gradient","title":"<code>gradient(feature, param)</code>  <code>staticmethod</code>","text":"<p>Calculates the gradient of the given feature with respect to the given parameter.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.gradient--parameters","title":"Parameters","text":"<p>feature : torch.Tensor     Tensor with the input feature. param : torch.Tensor     Tensor with the parameter to calculate the gradient with respect to.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.gradient--returns","title":"Returns","text":"<p>grad_ : torch.Tensor     Tensor with the gradient of the feature with respect to the given parameter.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.gradient--examples","title":"Examples","text":"<p>feature = torch.tensor([1, 2, 3], dtype=torch.float32) param = torch.tensor([2, 3, 4], dtype=torch.float32) gradient(feature, param) tensor([1., 1., 1.], grad_fn=) Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>@staticmethod\ndef gradient(feature, param):\n    \"\"\"\n    Calculates the gradient of the given feature with respect to the given parameter.\n\n    Parameters\n    ----------\n    feature : torch.Tensor\n        Tensor with the input feature.\n    param : torch.Tensor\n        Tensor with the parameter to calculate the gradient with respect to.\n\n    Returns\n    -------\n    grad_ : torch.Tensor\n        Tensor with the gradient of the feature with respect to the given parameter.\n\n    Examples\n    --------\n    &gt;&gt;&gt; feature = torch.tensor([1, 2, 3], dtype=torch.float32)\n    &gt;&gt;&gt; param = torch.tensor([2, 3, 4], dtype=torch.float32)\n    &gt;&gt;&gt; gradient(feature, param)\n    tensor([1., 1., 1.], grad_fn=&lt;AddBackward0&gt;)\n    \"\"\"\n    grad_ = grad(\n        feature,\n        param,\n        grad_outputs=torch.ones_like(feature),\n        create_graph=True,\n        allow_unused=True,\n        retain_graph=True,\n    )\n\n    return grad_[0]\n</code></pre>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.jac","title":"<code>jac(inputs)</code>","text":"<p>Calculates the Jacobian of the forward function of the model with respect to its inputs.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.jac--parameters","title":"Parameters","text":"<p>inputs : torch.Tensor     Tensor with the input data to the forward function.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.jac--returns","title":"Returns","text":"<p>jac_ : torch.Tensor     Tensor with the Jacobian of the forward function with respect to its inputs.</p>"},{"location":"simulai_residuals/#simulai.residuals.SymbolicOperator.jac--examples","title":"Examples","text":"<p>inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32) jac(inputs) tensor([[1., 1., 1.],         [1., 1., 1.]], grad_fn=) Source code in <code>simulai/residuals/_pytorch_residuals.py</code> <pre><code>def jac(self, inputs):\n    \"\"\"\n    Calculates the Jacobian of the forward function of the model with respect to its inputs.\n\n    Parameters\n    ----------\n    inputs : torch.Tensor\n        Tensor with the input data to the forward function.\n\n    Returns\n    -------\n    jac_ : torch.Tensor\n        Tensor with the Jacobian of the forward function with respect to its inputs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; inputs = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float32)\n    &gt;&gt;&gt; jac(inputs)\n    tensor([[1., 1., 1.],\n            [1., 1., 1.]], grad_fn=&lt;MulBackward0&gt;)\n    \"\"\"\n\n    def inner(inputs):\n        return self.forward(input_data=inputs)\n\n    return jacobian(inner, inputs)\n</code></pre>"}]}